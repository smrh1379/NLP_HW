{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\smrh1\\.conda\\envs\\tensorflow_gpu_cnn\\lib\\site-packages (4.40.2)\n",
      "Requirement already satisfied: torch in c:\\users\\smrh1\\.conda\\envs\\tensorflow_gpu_cnn\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\smrh1\\.conda\\envs\\tensorflow_gpu_cnn\\lib\\site-packages (4.66.4)\n",
      "Requirement already satisfied: nltk in c:\\users\\smrh1\\.conda\\envs\\tensorflow_gpu_cnn\\lib\\site-packages (3.8.1)\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\smrh1\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\smrh1\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\smrh1\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\smrh1\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py egg_info did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [15 lines of output]\n",
      "  The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "  rather than 'sklearn' for pip commands.\n",
      "  \n",
      "  Here is how to fix this error in the main use cases:\n",
      "  - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "  - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "    (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "  - if the 'sklearn' package is used by one of your dependencies,\n",
      "    it would be great if you take some time to track which package uses\n",
      "    'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "  - as a last resort, set the environment variable\n",
      "    SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "  \n",
      "  More information is available at\n",
      "  https://github.com/scikit-learn/sklearn-pypi-package\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "Encountered error while generating package metadata.\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\smrh1\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\smrh1\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\smrh1\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\smrh1\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\smrh1\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\smrh1\\appdata\\roaming\\python\\python39\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch tqdm nltk sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall numpy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\smrh1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\smrh1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [9], line 164\u001B[0m\n\u001B[0;32m    161\u001B[0m val_loader \u001B[38;5;241m=\u001B[39m DataLoader(val_dataset, batch_size\u001B[38;5;241m=\u001B[39mBATCH_SIZE)\n\u001B[0;32m    163\u001B[0m \u001B[38;5;66;03m# Training arguments\u001B[39;00m\n\u001B[1;32m--> 164\u001B[0m training_args \u001B[38;5;241m=\u001B[39m \u001B[43mTrainingArguments\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    165\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m./results\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    166\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_train_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    167\u001B[0m \u001B[43m    \u001B[49m\u001B[43mper_device_train_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mBATCH_SIZE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    168\u001B[0m \u001B[43m    \u001B[49m\u001B[43mper_device_eval_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mBATCH_SIZE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    169\u001B[0m \u001B[43m    \u001B[49m\u001B[43mwarmup_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m500\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    170\u001B[0m \u001B[43m    \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    171\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlogging_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m./logs\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    172\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlogging_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    173\u001B[0m \u001B[43m    \u001B[49m\u001B[43mevaluation_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mepoch\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[0;32m    174\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m    176\u001B[0m \u001B[38;5;66;03m# Trainer\u001B[39;00m\n\u001B[0;32m    177\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[0;32m    178\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[0;32m    179\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_args,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    182\u001B[0m     tokenizer\u001B[38;5;241m=\u001B[39mtokenizer\n\u001B[0;32m    183\u001B[0m )\n",
      "File \u001B[1;32m<string>:125\u001B[0m, in \u001B[0;36m__init__\u001B[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules)\u001B[0m\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\transformers\\training_args.py:1605\u001B[0m, in \u001B[0;36mTrainingArguments.__post_init__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1599\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m version\u001B[38;5;241m.\u001B[39mparse(version\u001B[38;5;241m.\u001B[39mparse(torch\u001B[38;5;241m.\u001B[39m__version__)\u001B[38;5;241m.\u001B[39mbase_version) \u001B[38;5;241m==\u001B[39m version\u001B[38;5;241m.\u001B[39mparse(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2.0.0\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp16:\n\u001B[0;32m   1600\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1602\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   1603\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1604\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m is_torch_available()\n\u001B[1;32m-> 1605\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1606\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmlu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1607\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1608\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxpu\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1609\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (get_xla_device_type(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice) \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGPU\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCUDA\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m   1610\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp16 \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfp16_full_eval)\n\u001B[0;32m   1611\u001B[0m ):\n\u001B[0;32m   1612\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1613\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1614\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m (`--fp16_full_eval`) can only be used on CUDA or MLU devices or NPU devices or certain XPU devices (with IPEX).\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1615\u001B[0m     )\n\u001B[0;32m   1617\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   1618\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mframework \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1619\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m is_torch_available()\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1627\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbf16 \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbf16_full_eval)\n\u001B[0;32m   1628\u001B[0m ):\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\transformers\\training_args.py:2094\u001B[0m, in \u001B[0;36mTrainingArguments.device\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   2090\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   2091\u001B[0m \u001B[38;5;124;03mThe device used by this process.\u001B[39;00m\n\u001B[0;32m   2092\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   2093\u001B[0m requires_backends(\u001B[38;5;28mself\u001B[39m, [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtorch\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m-> 2094\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_setup_devices\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\transformers\\utils\\generic.py:63\u001B[0m, in \u001B[0;36mcached_property.__get__\u001B[1;34m(self, obj, objtype)\u001B[0m\n\u001B[0;32m     61\u001B[0m cached \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(obj, attr, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m     62\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m cached \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 63\u001B[0m     cached \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     64\u001B[0m     \u001B[38;5;28msetattr\u001B[39m(obj, attr, cached)\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m cached\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\transformers\\training_args.py:2000\u001B[0m, in \u001B[0;36mTrainingArguments._setup_devices\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1998\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_sagemaker_mp_enabled():\n\u001B[0;32m   1999\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_accelerate_available():\n\u001B[1;32m-> 2000\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\n\u001B[0;32m   2001\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mACCELERATE_MIN_VERSION\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m`: \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2002\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease run `pip install transformers[torch]` or `pip install accelerate -U`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2003\u001B[0m         )\n\u001B[0;32m   2004\u001B[0m     AcceleratorState\u001B[38;5;241m.\u001B[39m_reset_state(reset_partial_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m   2005\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdistributed_state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[1;31mImportError\u001B[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to read file content\n",
    "def read_file(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Function to parse annotations from .ann file\n",
    "def parse_ann(ann_content):\n",
    "    annotations = []\n",
    "    for line in ann_content.strip().split('\\n'):\n",
    "        if line.startswith('T'):\n",
    "            parts = line.split('\\t')\n",
    "            ann_id = parts[0]\n",
    "            label_info = parts[1]\n",
    "            text = parts[2]\n",
    "            label_info_parts = label_info.split()\n",
    "            label = label_info_parts[0]\n",
    "            start = int(label_info_parts[1].split(';')[0])\n",
    "            end = int(label_info_parts[2].split(';')[0])\n",
    "            annotations.append({\n",
    "                'id': ann_id,\n",
    "                'label': label,\n",
    "                'start': start,\n",
    "                'end': end,\n",
    "                'text': text\n",
    "            })\n",
    "    return annotations\n",
    "\n",
    "# Function to preprocess text and remove stop words and punctuation\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words and token not in string.punctuation]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Function to format text and annotations for BioBERT input\n",
    "def format_biobert_input(text, annotations):\n",
    "    tokens = word_tokenize(text)\n",
    "    token_annotations = ['O'] * len(tokens)\n",
    "    text_offset = 0\n",
    "\n",
    "    for ann in annotations:\n",
    "        ann_tokens = word_tokenize(ann['text'])\n",
    "        ann_label = ann['label']\n",
    "\n",
    "        while text_offset < len(tokens):\n",
    "            if tokens[text_offset] == ann_tokens[0]:\n",
    "                match = True\n",
    "                for i in range(len(ann_tokens)):\n",
    "                    if text_offset + i >= len(tokens) or tokens[text_offset + i] != ann_tokens[i]:\n",
    "                        match = False\n",
    "                        break\n",
    "                if match:\n",
    "                    for i in range(len(ann_tokens)):\n",
    "                        if i == 0:\n",
    "                            token_annotations[text_offset + i] = f'B-{ann_label}'\n",
    "                        else:\n",
    "                            token_annotations[text_offset + i] = f'I-{ann_label}'\n",
    "                    text_offset += len(ann_tokens)\n",
    "                    break\n",
    "            text_offset += 1\n",
    "\n",
    "    return tokens, token_annotations\n",
    "\n",
    "# Function to process text and annotation files\n",
    "def process_files(txt_file, ann_file):\n",
    "    text = read_file(txt_file)\n",
    "    ann_content = read_file(ann_file)\n",
    "    annotations = parse_ann(ann_content)\n",
    "    processed_text = preprocess_text(text)\n",
    "    tokens, labels = format_biobert_input(processed_text, annotations)\n",
    "    return tokens, labels\n",
    "\n",
    "# Lists to store processed tokens and labels\n",
    "tokend_text = []\n",
    "cor_labels = []\n",
    "\n",
    "# Function to process all files in a directory\n",
    "def process_all_files(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            txt_file = os.path.join(directory, filename)\n",
    "            ann_file = txt_file.replace(\".txt\", \".ann\")\n",
    "            if os.path.exists(ann_file):\n",
    "                tokens, labels = process_files(txt_file, ann_file)\n",
    "                tokend_text.append(tokens)\n",
    "                cor_labels.append(labels)\n",
    "\n",
    "# Example usage:\n",
    "directory = 'n2c2/n2c2/part2'\n",
    "process_all_files(directory)\n",
    "\n",
    "# Prepare the tokenizer and model\n",
    "tokenizer = BertTokenizerFast.from_pretrained('dmis-lab/biobert-base-cased-v1.1')\n",
    "model = BertForTokenClassification.from_pretrained('dmis-lab/biobert-base-cased-v1.1', num_labels=len(set(label for doc in cor_labels for label in doc)))\n",
    "\n",
    "# Map labels to IDs\n",
    "unique_labels = set(label for doc_labels in cor_labels for label in doc_labels)\n",
    "label_map = {label: i for i, label in enumerate(unique_labels)}\n",
    "id2label = {i: label for label, i in label_map.items()}\n",
    "\n",
    "# Convert tokens and labels to strings for tokenizer\n",
    "texts = [\" \".join(tokens) for tokens in tokend_text]\n",
    "labels = [[label_map[label] for label in doc_labels] for doc_labels in cor_labels]\n",
    "\n",
    "# Create a custom dataset class\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        word_labels = self.labels[index]\n",
    "\n",
    "        encoding = self.tokenizer(text,\n",
    "                                  padding='max_length',\n",
    "                                  truncation=True,\n",
    "                                  max_length=self.max_len,\n",
    "                                  is_split_into_words=True,\n",
    "                                  return_tensors=\"pt\")\n",
    "\n",
    "        word_ids = encoding.word_ids(batch_index=0)\n",
    "\n",
    "        # Create a mask and label array for the tokens\n",
    "        labels = [-100 if word_id is None else word_labels[word_id] for word_id in word_ids]\n",
    "\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "        return item\n",
    "\n",
    "# Prepare the datasets and dataloaders\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n",
    "\n",
    "train_dataset = NERDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
    "val_dataset = NERDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "print(\"Training completed.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\smrh1\\.conda\\envs\\tensorflow_gpu_cnn\\lib\\site-packages (0.32.1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\smrh1\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\smrh1\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\smrh1\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\smrh1\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\smrh1\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\smrh1\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\smrh1\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\smrh1\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\smrh1\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\smrh1\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\smrh1\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\smrh1\\appdata\\roaming\\python\\python39\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in c:\\users\\smrh1\\.conda\\envs\\tensorflow_gpu_cnn\\lib\\site-packages (from accelerate) (1.24.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\smrh1\\.conda\\envs\\tensorflow_gpu_cnn\\lib\\site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\smrh1\\.conda\\envs\\tensorflow_gpu_cnn\\lib\\site-packages (from accelerate) (0.23.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\smrh1\\.conda\\envs\\tensorflow_gpu_cnn\\lib\\site-packages (from accelerate) (6.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\smrh1\\.conda\\envs\\tensorflow_gpu_cnn\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\smrh1\\.conda\\envs\\tensorflow_gpu_cnn\\lib\\site-packages (from accelerate) (1.13.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\smrh1\\.conda\\envs\\tensorflow_gpu_cnn\\lib\\site-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\smrh1\\.conda\\envs\\tensorflow_gpu_cnn\\lib\\site-packages (from packaging>=20.0->accelerate) (2.4.7)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\smrh1\\.conda\\envs\\tensorflow_gpu_cnn\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\smrh1\\.conda\\envs\\tensorflow_gpu_cnn\\lib\\site-packages (from huggingface-hub->accelerate) (2.32.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\smrh1\\.conda\\envs\\tensorflow_gpu_cnn\\lib\\site-packages (from huggingface-hub->accelerate) (2024.5.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\smrh1\\.conda\\envs\\tensorflow_gpu_cnn\\lib\\site-packages (from huggingface-hub->accelerate) (4.66.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\smrh1\\.conda\\envs\\tensorflow_gpu_cnn\\lib\\site-packages (from huggingface-hub->accelerate) (3.13.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\smrh1\\.conda\\envs\\tensorflow_gpu_cnn\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\smrh1\\.conda\\envs\\tensorflow_gpu_cnn\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\smrh1\\.conda\\envs\\tensorflow_gpu_cnn\\lib\\site-packages (from requests->huggingface-hub->accelerate) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\smrh1\\appdata\\roaming\\python\\python39\\site-packages (from requests->huggingface-hub->accelerate) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\smrh1\\appdata\\roaming\\python\\python39\\site-packages (from requests->huggingface-hub->accelerate) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate -U\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smrh1\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (6016 > 512). Running this sequence through the model will result in indexing errors\n",
      "C:\\Users\\smrh1\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:585: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  np.object,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'object'.\n`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [3], line 120\u001B[0m\n\u001B[0;32m    107\u001B[0m training_args \u001B[38;5;241m=\u001B[39m TrainingArguments(\n\u001B[0;32m    108\u001B[0m     output_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./results\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m    109\u001B[0m     num_train_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    116\u001B[0m     evaluation_strategy\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepoch\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    117\u001B[0m )\n\u001B[0;32m    119\u001B[0m \u001B[38;5;66;03m# Create the Trainer instance\u001B[39;00m\n\u001B[1;32m--> 120\u001B[0m trainer \u001B[38;5;241m=\u001B[39m \u001B[43mTrainer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    121\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    122\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    123\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    124\u001B[0m \u001B[43m    \u001B[49m\u001B[43meval_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    125\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    126\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m    128\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[0;32m    129\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain()\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\transformers\\trainer.py:383\u001B[0m, in \u001B[0;36mTrainer.__init__\u001B[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001B[0m\n\u001B[0;32m    381\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs \u001B[38;5;241m=\u001B[39m args\n\u001B[0;32m    382\u001B[0m \u001B[38;5;66;03m# Seed must be set before instantiating the model when using model\u001B[39;00m\n\u001B[1;32m--> 383\u001B[0m enable_full_determinism(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mseed) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mfull_determinism \u001B[38;5;28;01melse\u001B[39;00m \u001B[43mset_seed\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    384\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhp_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    385\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdeepspeed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\transformers\\trainer_utils.py:111\u001B[0m, in \u001B[0;36mset_seed\u001B[1;34m(seed, deterministic)\u001B[0m\n\u001B[0;32m    109\u001B[0m     torch\u001B[38;5;241m.\u001B[39mxpu\u001B[38;5;241m.\u001B[39mmanual_seed_all(seed)\n\u001B[0;32m    110\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_tf_available():\n\u001B[1;32m--> 111\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[0;32m    113\u001B[0m     tf\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mset_seed(seed)\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m deterministic:\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\tensorflow\\__init__.py:41\u001B[0m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msix\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_six\u001B[39;00m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_sys\u001B[39;00m\n\u001B[1;32m---> 41\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtools\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m module_util \u001B[38;5;28;01mas\u001B[39;00m _module_util\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlazy_loader\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LazyLoader \u001B[38;5;28;01mas\u001B[39;00m _LazyLoader\n\u001B[0;32m     44\u001B[0m \u001B[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\tensorflow\\python\\__init__.py:46\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pywrap_tensorflow \u001B[38;5;28;01mas\u001B[39;00m _pywrap_tensorflow\n\u001B[0;32m     43\u001B[0m \u001B[38;5;66;03m# pylint: enable=wildcard-import\u001B[39;00m\n\u001B[0;32m     44\u001B[0m \n\u001B[0;32m     45\u001B[0m \u001B[38;5;66;03m# Bring in subpackages.\u001B[39;00m\n\u001B[1;32m---> 46\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m data\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m distribute\n\u001B[0;32m     48\u001B[0m \u001B[38;5;66;03m# from tensorflow.python import keras\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\tensorflow\\python\\data\\__init__.py:25\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m__future__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m print_function\n\u001B[0;32m     24\u001B[0m \u001B[38;5;66;03m# pylint: disable=unused-import\u001B[39;00m\n\u001B[1;32m---> 25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m experimental\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdataset_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AUTOTUNE\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdataset_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dataset\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\__init__.py:97\u001B[0m\n\u001B[0;32m     94\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m__future__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m print_function\n\u001B[0;32m     96\u001B[0m \u001B[38;5;66;03m# pylint: disable=unused-import\u001B[39;00m\n\u001B[1;32m---> 97\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m service\n\u001B[0;32m     98\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbatching\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dense_to_ragged_batch\n\u001B[0;32m     99\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbatching\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dense_to_sparse_batch\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\service\\__init__.py:353\u001B[0m\n\u001B[0;32m    350\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m__future__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m division\n\u001B[0;32m    351\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m__future__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m print_function\n\u001B[1;32m--> 353\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata_service_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m distribute\n\u001B[0;32m    354\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata_service_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m from_dataset_id\n\u001B[0;32m    355\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata_service_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m register_dataset\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\data_service_ops.py:26\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tf2\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m compat\n\u001B[1;32m---> 26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m compression_ops\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute_options\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoShardPolicy\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute_options\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ExternalStatePolicy\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\compression_ops.py:20\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m__future__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m division\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m__future__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m print_function\n\u001B[1;32m---> 20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m structure\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m gen_experimental_dataset_ops \u001B[38;5;28;01mas\u001B[39;00m ged_ops\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompress\u001B[39m(element):\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:26\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msix\u001B[39;00m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwrapt\u001B[39;00m\n\u001B[1;32m---> 26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m nest\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m composite_tensor\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ops\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\tensorflow\\python\\data\\util\\nest.py:40\u001B[0m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m__future__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m print_function\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msix\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_six\u001B[39;00m\n\u001B[1;32m---> 40\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m sparse_tensor \u001B[38;5;28;01mas\u001B[39;00m _sparse_tensor\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _pywrap_utils\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m nest\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\tensorflow\\python\\framework\\sparse_tensor.py:28\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tf2\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m composite_tensor\n\u001B[1;32m---> 28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m constant_op\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dtypes\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ops\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:29\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m types_pb2\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01meager\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m context\n\u001B[1;32m---> 29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01meager\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m execute\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dtypes\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m op_callbacks\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:27\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pywrap_tfe\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01meager\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m core\n\u001B[1;32m---> 27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dtypes\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ops\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tensor_shape\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:585\u001B[0m\n\u001B[0;32m    556\u001B[0m     _NP_TO_TF[pdt] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\n\u001B[0;32m    557\u001B[0m         _NP_TO_TF[dt] \u001B[38;5;28;01mfor\u001B[39;00m dt \u001B[38;5;129;01min\u001B[39;00m _NP_TO_TF \u001B[38;5;28;01mif\u001B[39;00m dt \u001B[38;5;241m==\u001B[39m pdt()\u001B[38;5;241m.\u001B[39mdtype)  \u001B[38;5;66;03m# pylint: disable=no-value-for-parameter\u001B[39;00m\n\u001B[0;32m    559\u001B[0m TF_VALUE_DTYPES \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m(_NP_TO_TF\u001B[38;5;241m.\u001B[39mvalues())\n\u001B[0;32m    561\u001B[0m _TF_TO_NP \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    562\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_HALF:\n\u001B[0;32m    563\u001B[0m         np\u001B[38;5;241m.\u001B[39mfloat16,\n\u001B[0;32m    564\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_FLOAT:\n\u001B[0;32m    565\u001B[0m         np\u001B[38;5;241m.\u001B[39mfloat32,\n\u001B[0;32m    566\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_DOUBLE:\n\u001B[0;32m    567\u001B[0m         np\u001B[38;5;241m.\u001B[39mfloat64,\n\u001B[0;32m    568\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_INT32:\n\u001B[0;32m    569\u001B[0m         np\u001B[38;5;241m.\u001B[39mint32,\n\u001B[0;32m    570\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_UINT8:\n\u001B[0;32m    571\u001B[0m         np\u001B[38;5;241m.\u001B[39muint8,\n\u001B[0;32m    572\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_UINT16:\n\u001B[0;32m    573\u001B[0m         np\u001B[38;5;241m.\u001B[39muint16,\n\u001B[0;32m    574\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_UINT32:\n\u001B[0;32m    575\u001B[0m         np\u001B[38;5;241m.\u001B[39muint32,\n\u001B[0;32m    576\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_UINT64:\n\u001B[0;32m    577\u001B[0m         np\u001B[38;5;241m.\u001B[39muint64,\n\u001B[0;32m    578\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_INT16:\n\u001B[0;32m    579\u001B[0m         np\u001B[38;5;241m.\u001B[39mint16,\n\u001B[0;32m    580\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_INT8:\n\u001B[0;32m    581\u001B[0m         np\u001B[38;5;241m.\u001B[39mint8,\n\u001B[0;32m    582\u001B[0m     \u001B[38;5;66;03m# NOTE(touts): For strings we use np.object as it supports variable length\u001B[39;00m\n\u001B[0;32m    583\u001B[0m     \u001B[38;5;66;03m# strings.\u001B[39;00m\n\u001B[0;32m    584\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_STRING:\n\u001B[1;32m--> 585\u001B[0m         \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobject\u001B[49m,\n\u001B[0;32m    586\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_COMPLEX64:\n\u001B[0;32m    587\u001B[0m         np\u001B[38;5;241m.\u001B[39mcomplex64,\n\u001B[0;32m    588\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_COMPLEX128:\n\u001B[0;32m    589\u001B[0m         np\u001B[38;5;241m.\u001B[39mcomplex128,\n\u001B[0;32m    590\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_INT64:\n\u001B[0;32m    591\u001B[0m         np\u001B[38;5;241m.\u001B[39mint64,\n\u001B[0;32m    592\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_BOOL:\n\u001B[0;32m    593\u001B[0m         np\u001B[38;5;241m.\u001B[39mbool_,\n\u001B[0;32m    594\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_QINT8:\n\u001B[0;32m    595\u001B[0m         _np_qint8,\n\u001B[0;32m    596\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_QUINT8:\n\u001B[0;32m    597\u001B[0m         _np_quint8,\n\u001B[0;32m    598\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_QINT16:\n\u001B[0;32m    599\u001B[0m         _np_qint16,\n\u001B[0;32m    600\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_QUINT16:\n\u001B[0;32m    601\u001B[0m         _np_quint16,\n\u001B[0;32m    602\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_QINT32:\n\u001B[0;32m    603\u001B[0m         _np_qint32,\n\u001B[0;32m    604\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_BFLOAT16:\n\u001B[0;32m    605\u001B[0m         _np_bfloat16,\n\u001B[0;32m    606\u001B[0m \n\u001B[0;32m    607\u001B[0m     \u001B[38;5;66;03m# Ref types\u001B[39;00m\n\u001B[0;32m    608\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_HALF_REF:\n\u001B[0;32m    609\u001B[0m         np\u001B[38;5;241m.\u001B[39mfloat16,\n\u001B[0;32m    610\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_FLOAT_REF:\n\u001B[0;32m    611\u001B[0m         np\u001B[38;5;241m.\u001B[39mfloat32,\n\u001B[0;32m    612\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_DOUBLE_REF:\n\u001B[0;32m    613\u001B[0m         np\u001B[38;5;241m.\u001B[39mfloat64,\n\u001B[0;32m    614\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_INT32_REF:\n\u001B[0;32m    615\u001B[0m         np\u001B[38;5;241m.\u001B[39mint32,\n\u001B[0;32m    616\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_UINT32_REF:\n\u001B[0;32m    617\u001B[0m         np\u001B[38;5;241m.\u001B[39muint32,\n\u001B[0;32m    618\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_UINT8_REF:\n\u001B[0;32m    619\u001B[0m         np\u001B[38;5;241m.\u001B[39muint8,\n\u001B[0;32m    620\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_UINT16_REF:\n\u001B[0;32m    621\u001B[0m         np\u001B[38;5;241m.\u001B[39muint16,\n\u001B[0;32m    622\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_INT16_REF:\n\u001B[0;32m    623\u001B[0m         np\u001B[38;5;241m.\u001B[39mint16,\n\u001B[0;32m    624\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_INT8_REF:\n\u001B[0;32m    625\u001B[0m         np\u001B[38;5;241m.\u001B[39mint8,\n\u001B[0;32m    626\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_STRING_REF:\n\u001B[0;32m    627\u001B[0m         np\u001B[38;5;241m.\u001B[39mobject,\n\u001B[0;32m    628\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_COMPLEX64_REF:\n\u001B[0;32m    629\u001B[0m         np\u001B[38;5;241m.\u001B[39mcomplex64,\n\u001B[0;32m    630\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_COMPLEX128_REF:\n\u001B[0;32m    631\u001B[0m         np\u001B[38;5;241m.\u001B[39mcomplex128,\n\u001B[0;32m    632\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_INT64_REF:\n\u001B[0;32m    633\u001B[0m         np\u001B[38;5;241m.\u001B[39mint64,\n\u001B[0;32m    634\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_UINT64_REF:\n\u001B[0;32m    635\u001B[0m         np\u001B[38;5;241m.\u001B[39muint64,\n\u001B[0;32m    636\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_BOOL_REF:\n\u001B[0;32m    637\u001B[0m         np\u001B[38;5;241m.\u001B[39mbool,\n\u001B[0;32m    638\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_QINT8_REF:\n\u001B[0;32m    639\u001B[0m         _np_qint8,\n\u001B[0;32m    640\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_QUINT8_REF:\n\u001B[0;32m    641\u001B[0m         _np_quint8,\n\u001B[0;32m    642\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_QINT16_REF:\n\u001B[0;32m    643\u001B[0m         _np_qint16,\n\u001B[0;32m    644\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_QUINT16_REF:\n\u001B[0;32m    645\u001B[0m         _np_quint16,\n\u001B[0;32m    646\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_QINT32_REF:\n\u001B[0;32m    647\u001B[0m         _np_qint32,\n\u001B[0;32m    648\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_BFLOAT16_REF:\n\u001B[0;32m    649\u001B[0m         _np_bfloat16,\n\u001B[0;32m    650\u001B[0m }\n\u001B[0;32m    652\u001B[0m _QUANTIZED_DTYPES_NO_REF \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfrozenset\u001B[39m([qint8, quint8, qint16, quint16, qint32])\n\u001B[0;32m    653\u001B[0m _QUANTIZED_DTYPES_REF \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfrozenset\u001B[39m(\n\u001B[0;32m    654\u001B[0m     [qint8_ref, quint8_ref, qint16_ref, quint16_ref, qint32_ref])\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\numpy\\__init__.py:305\u001B[0m, in \u001B[0;36m__getattr__\u001B[1;34m(attr)\u001B[0m\n\u001B[0;32m    300\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    301\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIn the future `np.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mattr\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m` will be defined as the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    302\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcorresponding NumPy scalar.\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m    304\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attr \u001B[38;5;129;01min\u001B[39;00m __former_attrs__:\n\u001B[1;32m--> 305\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(__former_attrs__[attr])\n\u001B[0;32m    307\u001B[0m \u001B[38;5;66;03m# Importing Tester requires importing all of UnitTest which is not a\u001B[39;00m\n\u001B[0;32m    308\u001B[0m \u001B[38;5;66;03m# cheap import Since it is mainly used in test suits, we lazy import it\u001B[39;00m\n\u001B[0;32m    309\u001B[0m \u001B[38;5;66;03m# here to save on the order of 10 ms of import time for most users\u001B[39;00m\n\u001B[0;32m    310\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m    311\u001B[0m \u001B[38;5;66;03m# The previous way Tester was imported also had a side effect of adding\u001B[39;00m\n\u001B[0;32m    312\u001B[0m \u001B[38;5;66;03m# the full `numpy.testing` namespace\u001B[39;00m\n\u001B[0;32m    313\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attr \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtesting\u001B[39m\u001B[38;5;124m'\u001B[39m:\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'numpy' has no attribute 'object'.\n`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "# Define the labels\n",
    "labels = [\"O\", \"B-Drug\", \"I-Drug\", \"B-Strength\", \"I-Strength\", \"B-Form\", \"I-Form\", \"B-Dosage\", \"I-Dosage\",\n",
    "          \"B-Duration\", \"I-Duration\", \"B-Frequency\", \"I-Frequency\", \"B-Route\", \"I-Route\", \"B-ADE\", \"I-ADE\",\n",
    "          \"B-Reason\", \"I-Reason\"]\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "id2label = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "# Load the BioBERT tokenizer and model\n",
    "tokenizer = BertTokenizerFast.from_pretrained('dmis-lab/biobert-v1.1')\n",
    "model = BertForTokenClassification.from_pretrained('dmis-lab/biobert-v1.1', num_labels=len(labels))\n",
    "\n",
    "# Function to read data and create datasets\n",
    "def read_data(text_file, ann_file):\n",
    "    with open(text_file, 'r') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    entities = []\n",
    "    with open(ann_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('T'):\n",
    "                parts = line.strip().split('\\t')\n",
    "                entity_info = parts[1].split()\n",
    "                label = entity_info[0]\n",
    "                start = int(entity_info[1].split(';')[0])  # Handle potential non-integer values\n",
    "                end = int(entity_info[2].split(';')[0])  # Handle potential non-integer values\n",
    "                entities.append((start, end, label))\n",
    "\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_offsets = tokenizer(text, return_offsets_mapping=True).offset_mapping\n",
    "    token_labels = [\"O\"] * len(tokens)\n",
    "\n",
    "    for start, end, label in entities:\n",
    "        token_start = None\n",
    "        token_end = None\n",
    "        for i, (offset_start, offset_end) in enumerate(token_offsets):\n",
    "            if offset_start == start:\n",
    "                token_start = i\n",
    "            if offset_end == end:\n",
    "                token_end = i\n",
    "            if token_start is not None and token_end is not None:\n",
    "                break\n",
    "\n",
    "        if token_start is not None and token_end is not None:\n",
    "            token_labels[token_start] = f\"B-{label}\"\n",
    "            for i in range(token_start + 1, token_end + 1):\n",
    "                token_labels[i] = f\"I-{label}\"\n",
    "\n",
    "    return tokens, token_labels\n",
    "\n",
    "# Read all data files\n",
    "def read_all_data(data_dir):\n",
    "    all_tokens = []\n",
    "    all_labels = []\n",
    "\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            base_name = filename[:-4]\n",
    "            text_file = os.path.join(data_dir, filename)\n",
    "            ann_file = os.path.join(data_dir, base_name + '.ann')\n",
    "            tokens, labels = read_data(text_file, ann_file)\n",
    "            all_tokens.append(tokens)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    return all_tokens, all_labels\n",
    "\n",
    "# Prepare data\n",
    "data_dir = 'n2c2/n2c2/part2'  # replace with your folder path\n",
    "all_tokens, all_labels = read_all_data(data_dir)\n",
    "\n",
    "max_length = 512\n",
    "\n",
    "# Function to encode the tokens and labels\n",
    "def encode_tokens_and_labels(tokens, labels, max_length):\n",
    "    encodings = tokenizer(tokens, is_split_into_words=True, truncation=True, padding='max_length', max_length=max_length)\n",
    "    encoded_labels = [label2id[label] for label in labels] + [label2id['O']] * (max_length - len(labels))\n",
    "    encodings['labels'] = encoded_labels\n",
    "    return encodings\n",
    "\n",
    "encoded_data = [encode_tokens_and_labels(tokens, labels, max_length) for tokens, labels in zip(all_tokens, all_labels)]\n",
    "\n",
    "class NERDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings[idx].items()}\n",
    "        return item\n",
    "\n",
    "dataset = NERDataset(encoded_data)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,  # reduce if you encounter memory issues\n",
    "    per_device_eval_batch_size=8,   # reduce if you encounter memory issues\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Create the Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "predictions, true_labels, _ = trainer.predict(val_dataset)\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Convert predictions and labels to the appropriate format for evaluation\n",
    "true_labels = [[id2label[label_id.item()] for label_id in labels] for labels in true_labels]\n",
    "true_predictions = [[id2label[p] for p in prediction] for prediction in predictions]\n",
    "\n",
    "# Remove padding\n",
    "true_labels = [[label for label in labels if label != 'O'] for labels in true_labels]\n",
    "true_predictions = [[pred for pred in prediction if pred != 'O'] for prediction in true_predictions]\n",
    "\n",
    "print(classification_report(true_labels, true_predictions))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (6016 > 512). Running this sequence through the model will result in indexing errors\n",
      "C:\\Users\\smrh1\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:585: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  np.object,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'object'.\n`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [5], line 120\u001B[0m\n\u001B[0;32m    107\u001B[0m training_args \u001B[38;5;241m=\u001B[39m TrainingArguments(\n\u001B[0;32m    108\u001B[0m     output_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./results\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m    109\u001B[0m     num_train_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    116\u001B[0m     evaluation_strategy\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepoch\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    117\u001B[0m )\n\u001B[0;32m    119\u001B[0m \u001B[38;5;66;03m# Create the Trainer instance\u001B[39;00m\n\u001B[1;32m--> 120\u001B[0m trainer \u001B[38;5;241m=\u001B[39m \u001B[43mTrainer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    121\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    122\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    123\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    124\u001B[0m \u001B[43m    \u001B[49m\u001B[43meval_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    125\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    126\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m    128\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[0;32m    129\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain()\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\transformers\\trainer.py:383\u001B[0m, in \u001B[0;36mTrainer.__init__\u001B[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001B[0m\n\u001B[0;32m    381\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs \u001B[38;5;241m=\u001B[39m args\n\u001B[0;32m    382\u001B[0m \u001B[38;5;66;03m# Seed must be set before instantiating the model when using model\u001B[39;00m\n\u001B[1;32m--> 383\u001B[0m enable_full_determinism(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mseed) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mfull_determinism \u001B[38;5;28;01melse\u001B[39;00m \u001B[43mset_seed\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    384\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhp_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    385\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdeepspeed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\transformers\\trainer_utils.py:111\u001B[0m, in \u001B[0;36mset_seed\u001B[1;34m(seed, deterministic)\u001B[0m\n\u001B[0;32m    109\u001B[0m     torch\u001B[38;5;241m.\u001B[39mxpu\u001B[38;5;241m.\u001B[39mmanual_seed_all(seed)\n\u001B[0;32m    110\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_tf_available():\n\u001B[1;32m--> 111\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[0;32m    113\u001B[0m     tf\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mset_seed(seed)\n\u001B[0;32m    114\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m deterministic:\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\tensorflow\\__init__.py:41\u001B[0m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msix\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_six\u001B[39;00m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_sys\u001B[39;00m\n\u001B[1;32m---> 41\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtools\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m module_util \u001B[38;5;28;01mas\u001B[39;00m _module_util\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlazy_loader\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LazyLoader \u001B[38;5;28;01mas\u001B[39;00m _LazyLoader\n\u001B[0;32m     44\u001B[0m \u001B[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\tensorflow\\python\\__init__.py:46\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pywrap_tensorflow \u001B[38;5;28;01mas\u001B[39;00m _pywrap_tensorflow\n\u001B[0;32m     43\u001B[0m \u001B[38;5;66;03m# pylint: enable=wildcard-import\u001B[39;00m\n\u001B[0;32m     44\u001B[0m \n\u001B[0;32m     45\u001B[0m \u001B[38;5;66;03m# Bring in subpackages.\u001B[39;00m\n\u001B[1;32m---> 46\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m data\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m distribute\n\u001B[0;32m     48\u001B[0m \u001B[38;5;66;03m# from tensorflow.python import keras\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\tensorflow\\python\\data\\__init__.py:25\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m__future__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m print_function\n\u001B[0;32m     24\u001B[0m \u001B[38;5;66;03m# pylint: disable=unused-import\u001B[39;00m\n\u001B[1;32m---> 25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m experimental\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdataset_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AUTOTUNE\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdataset_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dataset\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\__init__.py:97\u001B[0m\n\u001B[0;32m     94\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m__future__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m print_function\n\u001B[0;32m     96\u001B[0m \u001B[38;5;66;03m# pylint: disable=unused-import\u001B[39;00m\n\u001B[1;32m---> 97\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m service\n\u001B[0;32m     98\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbatching\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dense_to_ragged_batch\n\u001B[0;32m     99\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbatching\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dense_to_sparse_batch\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\service\\__init__.py:353\u001B[0m\n\u001B[0;32m    350\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m__future__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m division\n\u001B[0;32m    351\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m__future__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m print_function\n\u001B[1;32m--> 353\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata_service_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m distribute\n\u001B[0;32m    354\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata_service_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m from_dataset_id\n\u001B[0;32m    355\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata_service_ops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m register_dataset\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\data_service_ops.py:26\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tf2\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcompat\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m compat\n\u001B[1;32m---> 26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m compression_ops\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute_options\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AutoShardPolicy\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mexperimental\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdistribute_options\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ExternalStatePolicy\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\compression_ops.py:20\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m__future__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m division\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m__future__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m print_function\n\u001B[1;32m---> 20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m structure\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m gen_experimental_dataset_ops \u001B[38;5;28;01mas\u001B[39;00m ged_ops\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompress\u001B[39m(element):\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:26\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msix\u001B[39;00m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mwrapt\u001B[39;00m\n\u001B[1;32m---> 26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m nest\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m composite_tensor\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ops\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\tensorflow\\python\\data\\util\\nest.py:40\u001B[0m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m__future__\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m print_function\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msix\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_six\u001B[39;00m\n\u001B[1;32m---> 40\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m sparse_tensor \u001B[38;5;28;01mas\u001B[39;00m _sparse_tensor\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _pywrap_utils\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m nest\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\tensorflow\\python\\framework\\sparse_tensor.py:28\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tf2\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m composite_tensor\n\u001B[1;32m---> 28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m constant_op\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dtypes\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ops\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:29\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m types_pb2\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01meager\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m context\n\u001B[1;32m---> 29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01meager\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m execute\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dtypes\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m op_callbacks\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:27\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pywrap_tfe\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01meager\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m core\n\u001B[1;32m---> 27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dtypes\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ops\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tensor_shape\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:585\u001B[0m\n\u001B[0;32m    556\u001B[0m     _NP_TO_TF[pdt] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\n\u001B[0;32m    557\u001B[0m         _NP_TO_TF[dt] \u001B[38;5;28;01mfor\u001B[39;00m dt \u001B[38;5;129;01min\u001B[39;00m _NP_TO_TF \u001B[38;5;28;01mif\u001B[39;00m dt \u001B[38;5;241m==\u001B[39m pdt()\u001B[38;5;241m.\u001B[39mdtype)  \u001B[38;5;66;03m# pylint: disable=no-value-for-parameter\u001B[39;00m\n\u001B[0;32m    559\u001B[0m TF_VALUE_DTYPES \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m(_NP_TO_TF\u001B[38;5;241m.\u001B[39mvalues())\n\u001B[0;32m    561\u001B[0m _TF_TO_NP \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    562\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_HALF:\n\u001B[0;32m    563\u001B[0m         np\u001B[38;5;241m.\u001B[39mfloat16,\n\u001B[0;32m    564\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_FLOAT:\n\u001B[0;32m    565\u001B[0m         np\u001B[38;5;241m.\u001B[39mfloat32,\n\u001B[0;32m    566\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_DOUBLE:\n\u001B[0;32m    567\u001B[0m         np\u001B[38;5;241m.\u001B[39mfloat64,\n\u001B[0;32m    568\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_INT32:\n\u001B[0;32m    569\u001B[0m         np\u001B[38;5;241m.\u001B[39mint32,\n\u001B[0;32m    570\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_UINT8:\n\u001B[0;32m    571\u001B[0m         np\u001B[38;5;241m.\u001B[39muint8,\n\u001B[0;32m    572\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_UINT16:\n\u001B[0;32m    573\u001B[0m         np\u001B[38;5;241m.\u001B[39muint16,\n\u001B[0;32m    574\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_UINT32:\n\u001B[0;32m    575\u001B[0m         np\u001B[38;5;241m.\u001B[39muint32,\n\u001B[0;32m    576\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_UINT64:\n\u001B[0;32m    577\u001B[0m         np\u001B[38;5;241m.\u001B[39muint64,\n\u001B[0;32m    578\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_INT16:\n\u001B[0;32m    579\u001B[0m         np\u001B[38;5;241m.\u001B[39mint16,\n\u001B[0;32m    580\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_INT8:\n\u001B[0;32m    581\u001B[0m         np\u001B[38;5;241m.\u001B[39mint8,\n\u001B[0;32m    582\u001B[0m     \u001B[38;5;66;03m# NOTE(touts): For strings we use np.object as it supports variable length\u001B[39;00m\n\u001B[0;32m    583\u001B[0m     \u001B[38;5;66;03m# strings.\u001B[39;00m\n\u001B[0;32m    584\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_STRING:\n\u001B[1;32m--> 585\u001B[0m         \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mobject\u001B[49m,\n\u001B[0;32m    586\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_COMPLEX64:\n\u001B[0;32m    587\u001B[0m         np\u001B[38;5;241m.\u001B[39mcomplex64,\n\u001B[0;32m    588\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_COMPLEX128:\n\u001B[0;32m    589\u001B[0m         np\u001B[38;5;241m.\u001B[39mcomplex128,\n\u001B[0;32m    590\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_INT64:\n\u001B[0;32m    591\u001B[0m         np\u001B[38;5;241m.\u001B[39mint64,\n\u001B[0;32m    592\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_BOOL:\n\u001B[0;32m    593\u001B[0m         np\u001B[38;5;241m.\u001B[39mbool_,\n\u001B[0;32m    594\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_QINT8:\n\u001B[0;32m    595\u001B[0m         _np_qint8,\n\u001B[0;32m    596\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_QUINT8:\n\u001B[0;32m    597\u001B[0m         _np_quint8,\n\u001B[0;32m    598\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_QINT16:\n\u001B[0;32m    599\u001B[0m         _np_qint16,\n\u001B[0;32m    600\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_QUINT16:\n\u001B[0;32m    601\u001B[0m         _np_quint16,\n\u001B[0;32m    602\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_QINT32:\n\u001B[0;32m    603\u001B[0m         _np_qint32,\n\u001B[0;32m    604\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_BFLOAT16:\n\u001B[0;32m    605\u001B[0m         _np_bfloat16,\n\u001B[0;32m    606\u001B[0m \n\u001B[0;32m    607\u001B[0m     \u001B[38;5;66;03m# Ref types\u001B[39;00m\n\u001B[0;32m    608\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_HALF_REF:\n\u001B[0;32m    609\u001B[0m         np\u001B[38;5;241m.\u001B[39mfloat16,\n\u001B[0;32m    610\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_FLOAT_REF:\n\u001B[0;32m    611\u001B[0m         np\u001B[38;5;241m.\u001B[39mfloat32,\n\u001B[0;32m    612\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_DOUBLE_REF:\n\u001B[0;32m    613\u001B[0m         np\u001B[38;5;241m.\u001B[39mfloat64,\n\u001B[0;32m    614\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_INT32_REF:\n\u001B[0;32m    615\u001B[0m         np\u001B[38;5;241m.\u001B[39mint32,\n\u001B[0;32m    616\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_UINT32_REF:\n\u001B[0;32m    617\u001B[0m         np\u001B[38;5;241m.\u001B[39muint32,\n\u001B[0;32m    618\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_UINT8_REF:\n\u001B[0;32m    619\u001B[0m         np\u001B[38;5;241m.\u001B[39muint8,\n\u001B[0;32m    620\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_UINT16_REF:\n\u001B[0;32m    621\u001B[0m         np\u001B[38;5;241m.\u001B[39muint16,\n\u001B[0;32m    622\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_INT16_REF:\n\u001B[0;32m    623\u001B[0m         np\u001B[38;5;241m.\u001B[39mint16,\n\u001B[0;32m    624\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_INT8_REF:\n\u001B[0;32m    625\u001B[0m         np\u001B[38;5;241m.\u001B[39mint8,\n\u001B[0;32m    626\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_STRING_REF:\n\u001B[0;32m    627\u001B[0m         np\u001B[38;5;241m.\u001B[39mobject,\n\u001B[0;32m    628\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_COMPLEX64_REF:\n\u001B[0;32m    629\u001B[0m         np\u001B[38;5;241m.\u001B[39mcomplex64,\n\u001B[0;32m    630\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_COMPLEX128_REF:\n\u001B[0;32m    631\u001B[0m         np\u001B[38;5;241m.\u001B[39mcomplex128,\n\u001B[0;32m    632\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_INT64_REF:\n\u001B[0;32m    633\u001B[0m         np\u001B[38;5;241m.\u001B[39mint64,\n\u001B[0;32m    634\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_UINT64_REF:\n\u001B[0;32m    635\u001B[0m         np\u001B[38;5;241m.\u001B[39muint64,\n\u001B[0;32m    636\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_BOOL_REF:\n\u001B[0;32m    637\u001B[0m         np\u001B[38;5;241m.\u001B[39mbool,\n\u001B[0;32m    638\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_QINT8_REF:\n\u001B[0;32m    639\u001B[0m         _np_qint8,\n\u001B[0;32m    640\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_QUINT8_REF:\n\u001B[0;32m    641\u001B[0m         _np_quint8,\n\u001B[0;32m    642\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_QINT16_REF:\n\u001B[0;32m    643\u001B[0m         _np_qint16,\n\u001B[0;32m    644\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_QUINT16_REF:\n\u001B[0;32m    645\u001B[0m         _np_quint16,\n\u001B[0;32m    646\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_QINT32_REF:\n\u001B[0;32m    647\u001B[0m         _np_qint32,\n\u001B[0;32m    648\u001B[0m     types_pb2\u001B[38;5;241m.\u001B[39mDT_BFLOAT16_REF:\n\u001B[0;32m    649\u001B[0m         _np_bfloat16,\n\u001B[0;32m    650\u001B[0m }\n\u001B[0;32m    652\u001B[0m _QUANTIZED_DTYPES_NO_REF \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfrozenset\u001B[39m([qint8, quint8, qint16, quint16, qint32])\n\u001B[0;32m    653\u001B[0m _QUANTIZED_DTYPES_REF \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfrozenset\u001B[39m(\n\u001B[0;32m    654\u001B[0m     [qint8_ref, quint8_ref, qint16_ref, quint16_ref, qint32_ref])\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\numpy\\__init__.py:305\u001B[0m, in \u001B[0;36m__getattr__\u001B[1;34m(attr)\u001B[0m\n\u001B[0;32m    300\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    301\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIn the future `np.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mattr\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m` will be defined as the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    302\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcorresponding NumPy scalar.\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m    304\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attr \u001B[38;5;129;01min\u001B[39;00m __former_attrs__:\n\u001B[1;32m--> 305\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(__former_attrs__[attr])\n\u001B[0;32m    307\u001B[0m \u001B[38;5;66;03m# Importing Tester requires importing all of UnitTest which is not a\u001B[39;00m\n\u001B[0;32m    308\u001B[0m \u001B[38;5;66;03m# cheap import Since it is mainly used in test suits, we lazy import it\u001B[39;00m\n\u001B[0;32m    309\u001B[0m \u001B[38;5;66;03m# here to save on the order of 10 ms of import time for most users\u001B[39;00m\n\u001B[0;32m    310\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[0;32m    311\u001B[0m \u001B[38;5;66;03m# The previous way Tester was imported also had a side effect of adding\u001B[39;00m\n\u001B[0;32m    312\u001B[0m \u001B[38;5;66;03m# the full `numpy.testing` namespace\u001B[39;00m\n\u001B[0;32m    313\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attr \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtesting\u001B[39m\u001B[38;5;124m'\u001B[39m:\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'numpy' has no attribute 'object'.\n`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "# Define the labels\n",
    "labels = [\"O\", \"B-Drug\", \"I-Drug\", \"B-Strength\", \"I-Strength\", \"B-Form\", \"I-Form\", \"B-Dosage\", \"I-Dosage\",\n",
    "          \"B-Duration\", \"I-Duration\", \"B-Frequency\", \"I-Frequency\", \"B-Route\", \"I-Route\", \"B-ADE\", \"I-ADE\",\n",
    "          \"B-Reason\", \"I-Reason\"]\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "id2label = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "# Load the BioBERT tokenizer and model\n",
    "tokenizer = BertTokenizerFast.from_pretrained('dmis-lab/biobert-v1.1')\n",
    "model = BertForTokenClassification.from_pretrained('dmis-lab/biobert-v1.1', num_labels=len(labels))\n",
    "\n",
    "# Function to read data and create datasets\n",
    "def read_data(text_file, ann_file):\n",
    "    with open(text_file, 'r') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    entities = []\n",
    "    with open(ann_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('T'):\n",
    "                parts = line.strip().split('\\t')\n",
    "                entity_info = parts[1].split()\n",
    "                label = entity_info[0]\n",
    "                start = int(entity_info[1].split(';')[0])  # Handle potential non-integer values\n",
    "                end = int(entity_info[2].split(';')[0])  # Handle potential non-integer values\n",
    "                entities.append((start, end, label))\n",
    "\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_offsets = tokenizer(text, return_offsets_mapping=True).offset_mapping\n",
    "    token_labels = [\"O\"] * len(tokens)\n",
    "\n",
    "    for start, end, label in entities:\n",
    "        token_start = None\n",
    "        token_end = None\n",
    "        for i, (offset_start, offset_end) in enumerate(token_offsets):\n",
    "            if offset_start == start:\n",
    "                token_start = i\n",
    "            if offset_end == end:\n",
    "                token_end = i\n",
    "            if token_start is not None and token_end is not None:\n",
    "                break\n",
    "\n",
    "        if token_start is not None and token_end is not None:\n",
    "            token_labels[token_start] = f\"B-{label}\"\n",
    "            for i in range(token_start + 1, token_end + 1):\n",
    "                token_labels[i] = f\"I-{label}\"\n",
    "\n",
    "    return tokens, token_labels\n",
    "\n",
    "# Read all data files\n",
    "def read_all_data(data_dir):\n",
    "    all_tokens = []\n",
    "    all_labels = []\n",
    "\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            base_name = filename[:-4]\n",
    "            text_file = os.path.join(data_dir, filename)\n",
    "            ann_file = os.path.join(data_dir, base_name + '.ann')\n",
    "            tokens, labels = read_data(text_file, ann_file)\n",
    "            all_tokens.append(tokens)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    return all_tokens, all_labels\n",
    "\n",
    "# Prepare data\n",
    "data_dir = 'n2c2/n2c2/part2'  # replace with your folder path\n",
    "all_tokens, all_labels = read_all_data(data_dir)\n",
    "\n",
    "max_length = 512\n",
    "\n",
    "# Function to encode the tokens and labels\n",
    "def encode_tokens_and_labels(tokens, labels, max_length):\n",
    "    encodings = tokenizer(tokens, is_split_into_words=True, truncation=True, padding='max_length', max_length=max_length)\n",
    "    encoded_labels = [label2id[label] for label in labels] + [label2id['O']] * (max_length - len(labels))\n",
    "    encodings['labels'] = encoded_labels\n",
    "    return encodings\n",
    "\n",
    "encoded_data = [encode_tokens_and_labels(tokens, labels, max_length) for tokens, labels in zip(all_tokens, all_labels)]\n",
    "\n",
    "class NERDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val) for key, val in self.encodings[idx].items()}\n",
    "        return item\n",
    "\n",
    "dataset = NERDataset(encoded_data)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,  # reduce if you encounter memory issues\n",
    "    per_device_eval_batch_size=8,   # reduce if you encounter memory issues\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Create the Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "predictions, true_labels, _ = trainer.predict(val_dataset)\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Convert predictions and labels to the appropriate format for evaluation\n",
    "true_labels = [[id2label[label_id.item()] for label_id in labels] for labels in true_labels]\n",
    "true_predictions = [[id2label[pred] for pred in prediction] for prediction in predictions]\n",
    "\n",
    "# Remove padding from predictions and labels for evaluation\n",
    "true_labels = [[label for label in label_seq if label != 'O'] for label_seq in true_labels]\n",
    "true_predictions = [[pred for pred in pred_seq if pred != 'O'] for pred_seq in true_predictions]\n",
    "\n",
    "print(classification_report(true_labels, true_predictions))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\smrh1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\smrh1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "false INTERNAL ASSERT FAILED at \"C:\\\\cb\\\\pytorch_1000000000000\\\\work\\\\c10/cuda/CUDAGraphsC10Utils.h\":74, please report a bug to PyTorch. Unknown CUDA graph CaptureStatus1893637952",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [4], line 235\u001B[0m\n\u001B[0;32m    220\u001B[0m val_dataset \u001B[38;5;241m=\u001B[39m NERDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n\u001B[0;32m    222\u001B[0m training_args \u001B[38;5;241m=\u001B[39m TrainingArguments(\n\u001B[0;32m    223\u001B[0m     output_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m./results/fold_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfold \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m    224\u001B[0m     num_train_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    232\u001B[0m     save_strategy\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepoch\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    233\u001B[0m )\n\u001B[1;32m--> 235\u001B[0m trainer \u001B[38;5;241m=\u001B[39m \u001B[43mTrainer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    236\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    237\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    238\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    239\u001B[0m \u001B[43m    \u001B[49m\u001B[43meval_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    240\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenizer\u001B[49m\n\u001B[0;32m    241\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    243\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[0;32m    244\u001B[0m \u001B[38;5;66;03m# Save the model after training each fold\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\transformers\\trainer.py:398\u001B[0m, in \u001B[0;36mTrainer.__init__\u001B[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001B[0m\n\u001B[0;32m    396\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs \u001B[38;5;241m=\u001B[39m args\n\u001B[0;32m    397\u001B[0m \u001B[38;5;66;03m# Seed must be set before instantiating the model when using model\u001B[39;00m\n\u001B[1;32m--> 398\u001B[0m enable_full_determinism(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mseed) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mfull_determinism \u001B[38;5;28;01melse\u001B[39;00m \u001B[43mset_seed\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mseed\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    399\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhp_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    400\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdeepspeed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\transformers\\trainer_utils.py:102\u001B[0m, in \u001B[0;36mset_seed\u001B[1;34m(seed, deterministic)\u001B[0m\n\u001B[0;32m    100\u001B[0m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mseed(seed)\n\u001B[0;32m    101\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_torch_available():\n\u001B[1;32m--> 102\u001B[0m     \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmanual_seed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mseed\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    103\u001B[0m     torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mmanual_seed_all(seed)\n\u001B[0;32m    104\u001B[0m     \u001B[38;5;66;03m# ^^ safe to call this function even if cuda is not available\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\torch\\random.py:40\u001B[0m, in \u001B[0;36mmanual_seed\u001B[1;34m(seed)\u001B[0m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcuda\u001B[39;00m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39m_is_in_bad_fork():\n\u001B[1;32m---> 40\u001B[0m     \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmanual_seed_all\u001B[49m\u001B[43m(\u001B[49m\u001B[43mseed\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m default_generator\u001B[38;5;241m.\u001B[39mmanual_seed(seed)\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\torch\\cuda\\random.py:113\u001B[0m, in \u001B[0;36mmanual_seed_all\u001B[1;34m(seed)\u001B[0m\n\u001B[0;32m    110\u001B[0m         default_generator \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mdefault_generators[i]\n\u001B[0;32m    111\u001B[0m         default_generator\u001B[38;5;241m.\u001B[39mmanual_seed(seed)\n\u001B[1;32m--> 113\u001B[0m \u001B[43m_lazy_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed_all\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\torch\\cuda\\__init__.py:165\u001B[0m, in \u001B[0;36m_lazy_call\u001B[1;34m(callable, **kwargs)\u001B[0m\n\u001B[0;32m    163\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_lazy_call\u001B[39m(callable, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    164\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_initialized():\n\u001B[1;32m--> 165\u001B[0m         \u001B[43mcallable\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    166\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    167\u001B[0m         \u001B[38;5;66;03m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001B[39;00m\n\u001B[0;32m    168\u001B[0m         \u001B[38;5;66;03m# file system to get traceback info. Patch linecache or do something\u001B[39;00m\n\u001B[0;32m    169\u001B[0m         \u001B[38;5;66;03m# else here if this ends up being important.\u001B[39;00m\n\u001B[0;32m    170\u001B[0m         \u001B[38;5;28;01mglobal\u001B[39;00m _lazy_seed_tracker\n",
      "File \u001B[1;32m~\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\torch\\cuda\\random.py:111\u001B[0m, in \u001B[0;36mmanual_seed_all.<locals>.cb\u001B[1;34m()\u001B[0m\n\u001B[0;32m    109\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(device_count()):\n\u001B[0;32m    110\u001B[0m     default_generator \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mdefault_generators[i]\n\u001B[1;32m--> 111\u001B[0m     \u001B[43mdefault_generator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmanual_seed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mseed\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: false INTERNAL ASSERT FAILED at \"C:\\\\cb\\\\pytorch_1000000000000\\\\work\\\\c10/cuda/CUDAGraphsC10Utils.h\":74, please report a bug to PyTorch. Unknown CUDA graph CaptureStatus1893637952"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('dmis-lab/biobert-base-cased-v1.1')\n",
    "\n",
    "# Function to read file content\n",
    "def read_file(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Function to parse annotations from .ann file\n",
    "def parse_ann(ann_content):\n",
    "    annotations = []\n",
    "    for line in ann_content.strip().split('\\n'):\n",
    "        if line.startswith('T'):\n",
    "            parts = line.split('\\t')\n",
    "            ann_id = parts[0]\n",
    "            label_info = parts[1]\n",
    "            text = parts[2]\n",
    "            label_info_parts = label_info.split()\n",
    "            label = label_info_parts[0]\n",
    "            start = int(label_info_parts[1].split(';')[0])\n",
    "            end = int(label_info_parts[2].split(';')[0])\n",
    "            annotations.append({\n",
    "                'id': ann_id,\n",
    "                'label': label,\n",
    "                'start': start,\n",
    "                'end': end,\n",
    "                'text': text\n",
    "            })\n",
    "    return annotations\n",
    "\n",
    "# Function to preprocess text and remove stop words and punctuation\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words and token not in string.punctuation]\n",
    "    return filtered_tokens\n",
    "\n",
    "# Function to format text and annotations for BioBERT input\n",
    "def format_biobert_input(text, annotations):\n",
    "    tokens = preprocess_text(text)\n",
    "    token_annotations = ['O'] * len(tokens)\n",
    "    text_offset = 0\n",
    "\n",
    "    for ann in annotations:\n",
    "        ann_tokens = word_tokenize(ann['text'])\n",
    "        ann_label = ann['label']\n",
    "\n",
    "        while text_offset < len(tokens):\n",
    "            try:\n",
    "                if tokens[text_offset] == ann_tokens[0]:\n",
    "                    match = True\n",
    "                    for i in range(len(ann_tokens)):\n",
    "                        if text_offset + i >= len(tokens) or tokens[text_offset + i] != ann_tokens[i]:\n",
    "                            match = False\n",
    "                            break\n",
    "                    if match:\n",
    "                        for i in range(len(ann_tokens)):\n",
    "                            if i == 0:\n",
    "                                token_annotations[text_offset + i] = f'B-{ann_label}'\n",
    "                            else:\n",
    "                                token_annotations[text_offset + i] = f'I-{ann_label}'\n",
    "                        text_offset += len(ann_tokens)\n",
    "                        break\n",
    "                text_offset += 1\n",
    "            except:\n",
    "                print(ann_tokens)\n",
    "                print(tokens[text_offset])\n",
    "\n",
    "    return tokens, token_annotations\n",
    "\n",
    "# Function to split tokens and labels into chunks of up to max_length\n",
    "def split_into_chunks(tokens, labels, max_length=509):\n",
    "    chunks = []\n",
    "    chunk_labels = []\n",
    "    current_chunk = []\n",
    "    current_chunk_labels = []\n",
    "    current_length = 0\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        current_chunk.append(tokens[i])\n",
    "        current_chunk_labels.append(labels[i])\n",
    "        current_length += 1\n",
    "\n",
    "        if current_length >= max_length:\n",
    "            # Ensure that we do not split entities and the last label is 'O'\n",
    "            while i < len(tokens) and not labels[i] == 'O':\n",
    "                current_chunk.append(tokens[i])\n",
    "                current_chunk_labels.append(labels[i])\n",
    "                current_length += 1\n",
    "                i += 1\n",
    "\n",
    "            chunks.append(current_chunk)\n",
    "            chunk_labels.append(current_chunk_labels)\n",
    "            current_chunk = []\n",
    "            current_chunk_labels = []\n",
    "            current_length = 0\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "        chunk_labels.append(current_chunk_labels)\n",
    "\n",
    "    return chunks, chunk_labels\n",
    "\n",
    "# Function to process text and annotation files\n",
    "def process_files(txt_file, ann_file):\n",
    "    text = read_file(txt_file)\n",
    "    ann_content = read_file(ann_file)\n",
    "    annotations = parse_ann(ann_content)\n",
    "    tokens, labels = format_biobert_input(text, annotations)\n",
    "    token_chunks, label_chunks = split_into_chunks(tokens, labels)\n",
    "    return token_chunks, label_chunks\n",
    "\n",
    "# Lists to store processed tokens and labels\n",
    "tokend_text = []\n",
    "cor_labels = []\n",
    "\n",
    "# Function to process all files in a directory\n",
    "def process_all_files(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            txt_file = os.path.join(directory, filename)\n",
    "            ann_file = txt_file.replace(\".txt\", \".ann\")\n",
    "            if os.path.exists(ann_file):\n",
    "                token_chunks, label_chunks = process_files(txt_file, ann_file)\n",
    "                tokend_text.extend(token_chunks)\n",
    "                cor_labels.extend(label_chunks)\n",
    "\n",
    "# Example usage:\n",
    "directory = '/content/data'\n",
    "process_all_files(directory)\n",
    "\n",
    "# Prepare the tokenizer and model\n",
    "tokenizer = BertTokenizerFast.from_pretrained('dmis-lab/biobert-base-cased-v1.1')\n",
    "model = BertForTokenClassification.from_pretrained('dmis-lab/biobert-base-cased-v1.1', num_labels=len(set(label for doc in cor_labels for label in doc)))\n",
    "\n",
    "# Define labels\n",
    "labels = [\"O\", \"B-Drug\", \"I-Drug\", \"B-Strength\", \"I-Strength\", \"B-Form\", \"I-Form\", \"B-Dosage\", \"I-Dosage\",\n",
    "          \"B-Duration\", \"I-Duration\", \"B-Frequency\", \"I-Frequency\", \"B-Route\", \"I-Route\", \"B-ADE\", \"I-ADE\",\n",
    "          \"B-Reason\", \"I-Reason\"]\n",
    "\n",
    "# Map labels to IDs\n",
    "label_map = {label: i for i, label in enumerate(labels)}\n",
    "id2label = {i: label for label, i in label_map.items()}\n",
    "\n",
    "# Convert tokens and labels to strings for tokenizer\n",
    "texts = tokend_text\n",
    "labels = [[label_map[label] for label in doc_labels] for doc_labels in cor_labels]\n",
    "\n",
    "# Create a custom dataset class\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        word_labels = self.labels[index]\n",
    "\n",
    "        encoding = self.tokenizer(text,\n",
    "                                  truncation=True,\n",
    "                                  padding='max_length',  # Ensure padding\n",
    "                                  max_length=self.max_len,\n",
    "                                  is_split_into_words=True,\n",
    "                                  return_tensors='pt')\n",
    "\n",
    "        word_ids = encoding.word_ids(batch_index=0)\n",
    "\n",
    "        # Create a mask and label array for the tokens\n",
    "        labels = [-100 if word_id is None else word_labels[word_id] for word_id in word_ids]\n",
    "\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "        return item\n",
    "\n",
    "# Prepare the datasets and dataloaders\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# 10-fold cross-validation setup\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "all_metrics = {\n",
    "    'accuracy': [],\n",
    "    'precision_micro': [],\n",
    "    'recall_micro': [],\n",
    "    'f1_micro': [],\n",
    "    'precision_macro': [],\n",
    "    'recall_macro': [],\n",
    "    'f1_macro': []\n",
    "}\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(texts)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "\n",
    "    train_texts = [texts[i] for i in train_index]\n",
    "    val_texts = [texts[i] for i in val_index]\n",
    "    train_labels = [labels[i] for i in train_index]\n",
    "    val_labels = [labels[i] for i in val_index]\n",
    "\n",
    "    train_dataset = NERDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
    "    val_dataset = NERDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'./results/fold_{fold + 1}',\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f'./logs/fold_{fold + 1}',\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    # Save the model after training each fold\n",
    "    trainer.save_model(f'./model/fold_{fold + 1}')\n",
    "\n",
    "    # Evaluation\n",
    "    predictions, labels, _ = trainer.predict(val_dataset)\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        true_labels.extend(labels[i])\n",
    "        pred_labels.extend(predictions[i])\n",
    "\n",
    "    # Remove ignored index (-100)\n",
    "    true_labels = [label for label in true_labels if label != -100]\n",
    "    pred_labels = [pred for label, pred in zip(true_labels, pred_labels) if label != -100]\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, pred_labels)\n",
    "    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(true_labels, pred_labels, average='micro')\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(true_labels, pred_labels, average='macro')\n",
    "\n",
    "    all_metrics['accuracy'].append(accuracy)\n",
    "    all_metrics['precision_micro'].append(precision_micro)\n",
    "    all_metrics['recall_micro'].append(recall_micro)\n",
    "    all_metrics['f1_micro'].append(f1_micro)\n",
    "    all_metrics['precision_macro'].append(precision_macro)\n",
    "    all_metrics['recall_macro'].append(recall_macro)\n",
    "    all_metrics['f1_macro'].append(f1_macro)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Micro Precision: {precision_micro}, Micro Recall: {recall_micro}, Micro F1: {f1_micro}\")\n",
    "    print(f\"Macro Precision: {precision_macro}, Macro Recall: {recall_macro}, Macro F1: {f1_macro}\")\n",
    "\n",
    "# Calculate mean and standard deviation of metrics\n",
    "metrics_mean_std = {metric: (np.mean(all_metrics[metric]), np.std(all_metrics[metric])) for metric in all_metrics}\n",
    "\n",
    "print(\"\\nMetrics Mean and Standard Deviation:\")\n",
    "for metric, (mean, std) in metrics_mean_std.items():\n",
    "    print(f\"{metric.capitalize()} - Mean: {mean}, Std: {std}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\smrh1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\smrh1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smrh1\\.conda\\envs\\tensorflow_gpu_CNN\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [1], line 274\u001B[0m\n\u001B[0;32m    271\u001B[0m train_dataloader \u001B[38;5;241m=\u001B[39m DataLoader(train_dataset, sampler\u001B[38;5;241m=\u001B[39mRandomSampler(train_dataset), batch_size\u001B[38;5;241m=\u001B[39mBATCH_SIZE)\n\u001B[0;32m    272\u001B[0m val_dataloader \u001B[38;5;241m=\u001B[39m DataLoader(val_dataset, sampler\u001B[38;5;241m=\u001B[39mSequentialSampler(val_dataset), batch_size\u001B[38;5;241m=\u001B[39mBATCH_SIZE)\n\u001B[1;32m--> 274\u001B[0m \u001B[43mtrain_one_fold\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfold\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    276\u001B[0m accuracy, precision_micro, recall_micro, f1_micro, precision_macro, recall_macro, f1_macro \u001B[38;5;241m=\u001B[39m evaluate_one_fold(val_dataloader)\n\u001B[0;32m    278\u001B[0m all_metrics[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mappend(accuracy)\n",
      "Cell \u001B[1;32mIn [1], line 227\u001B[0m, in \u001B[0;36mtrain_one_fold\u001B[1;34m(train_dataloader, val_dataloader, fold)\u001B[0m\n\u001B[0;32m    225\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs)\n\u001B[0;32m    226\u001B[0m loss \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mloss\n\u001B[1;32m--> 227\u001B[0m total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    228\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m    229\u001B[0m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mclip_grad_norm_(model\u001B[38;5;241m.\u001B[39mparameters(), \u001B[38;5;241m1.0\u001B[39m)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('dmis-lab/biobert-base-cased-v1.1')\n",
    "\n",
    "# Function to read file content\n",
    "def read_file(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "# Function to parse annotations from .ann file\n",
    "def parse_ann(ann_content):\n",
    "    annotations = []\n",
    "    for line in ann_content.strip().split('\\n'):\n",
    "        if line.startswith('T'):\n",
    "            parts = line.split('\\t')\n",
    "            ann_id = parts[0]\n",
    "            label_info = parts[1]\n",
    "            text = parts[2]\n",
    "            label_info_parts = label_info.split()\n",
    "            label = label_info_parts[0]\n",
    "            start = int(label_info_parts[1].split(';')[0])\n",
    "            end = int(label_info_parts[2].split(';')[0])\n",
    "            annotations.append({\n",
    "                'id': ann_id,\n",
    "                'label': label,\n",
    "                'start': start,\n",
    "                'end': end,\n",
    "                'text': text\n",
    "            })\n",
    "    return annotations\n",
    "\n",
    "# Function to preprocess text and remove stop words and punctuation\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stop_words and token not in string.punctuation]\n",
    "    return filtered_tokens\n",
    "\n",
    "# Function to format text and annotations for BioBERT input\n",
    "def format_biobert_input(text, annotations):\n",
    "    tokens = preprocess_text(text)\n",
    "    token_annotations = ['O'] * len(tokens)\n",
    "    text_offset = 0\n",
    "\n",
    "    for ann in annotations:\n",
    "        ann_tokens = word_tokenize(ann['text'])\n",
    "        ann_label = ann['label']\n",
    "\n",
    "        while text_offset < len(tokens):\n",
    "            try:\n",
    "                if tokens[text_offset] == ann_tokens[0]:\n",
    "                    match = True\n",
    "                    for i in range(len(ann_tokens)):\n",
    "                        if text_offset + i >= len(tokens) or tokens[text_offset + i] != ann_tokens[i]:\n",
    "                            match = False\n",
    "                            break\n",
    "                    if match:\n",
    "                        for i in range(len(ann_tokens)):\n",
    "                            if i == 0:\n",
    "                                token_annotations[text_offset + i] = f'B-{ann_label}'\n",
    "                            else:\n",
    "                                token_annotations[text_offset + i] = f'I-{ann_label}'\n",
    "                        text_offset += len(ann_tokens)\n",
    "                        break\n",
    "                text_offset += 1\n",
    "            except:\n",
    "                print(ann_tokens)\n",
    "                print(tokens[text_offset])\n",
    "\n",
    "    return tokens, token_annotations\n",
    "\n",
    "# Function to split tokens and labels into chunks of up to max_length\n",
    "def split_into_chunks(tokens, labels, max_length=509):\n",
    "    chunks = []\n",
    "    chunk_labels = []\n",
    "    current_chunk = []\n",
    "    current_chunk_labels = []\n",
    "    current_length = 0\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        current_chunk.append(tokens[i])\n",
    "        current_chunk_labels.append(labels[i])\n",
    "        current_length += 1\n",
    "\n",
    "        if current_length >= max_length:\n",
    "            # Ensure that we do not split entities and the last label is 'O'\n",
    "            while i < len(tokens) and not labels[i] == 'O':\n",
    "                current_chunk.append(tokens[i])\n",
    "                current_chunk_labels.append(labels[i])\n",
    "                current_length += 1\n",
    "                i += 1\n",
    "\n",
    "            chunks.append(current_chunk)\n",
    "            chunk_labels.append(current_chunk_labels)\n",
    "            current_chunk = []\n",
    "            current_chunk_labels = []\n",
    "            current_length = 0\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "        chunk_labels.append(current_chunk_labels)\n",
    "\n",
    "    return chunks, chunk_labels\n",
    "\n",
    "# Function to process text and annotation files\n",
    "def process_files(txt_file, ann_file):\n",
    "    text = read_file(txt_file)\n",
    "    ann_content = read_file(ann_file)\n",
    "    annotations = parse_ann(ann_content)\n",
    "    tokens, labels = format_biobert_input(text, annotations)\n",
    "    token_chunks, label_chunks = split_into_chunks(tokens, labels)\n",
    "    return token_chunks, label_chunks\n",
    "\n",
    "# Lists to store processed tokens and labels\n",
    "tokend_text = []\n",
    "cor_labels = []\n",
    "\n",
    "# Function to process all files in a directory\n",
    "def process_all_files(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            txt_file = os.path.join(directory, filename)\n",
    "            ann_file = txt_file.replace(\".txt\", \".ann\")\n",
    "            if os.path.exists(ann_file):\n",
    "                token_chunks, label_chunks = process_files(txt_file, ann_file)\n",
    "                tokend_text.extend(token_chunks)\n",
    "                cor_labels.extend(label_chunks)\n",
    "\n",
    "# Example usage:\n",
    "directory = 'n2c2/n2c2/part2'\n",
    "process_all_files(directory)\n",
    "\n",
    "# Prepare the tokenizer and model\n",
    "tokenizer = BertTokenizerFast.from_pretrained('dmis-lab/biobert-base-cased-v1.1')\n",
    "model = BertForTokenClassification.from_pretrained('dmis-lab/biobert-base-cased-v1.1', num_labels=len(set(label for doc in cor_labels for label in doc)))\n",
    "\n",
    "# Define labels\n",
    "labels = [\"O\", \"B-Drug\", \"I-Drug\", \"B-Strength\", \"I-Strength\", \"B-Form\", \"I-Form\", \"B-Dosage\", \"I-Dosage\",\n",
    "          \"B-Duration\", \"I-Duration\", \"B-Frequency\", \"I-Frequency\", \"B-Route\", \"I-Route\", \"B-ADE\", \"I-ADE\",\n",
    "          \"B-Reason\", \"I-Reason\"]\n",
    "\n",
    "# Map labels to IDs\n",
    "label_map = {label: i for i, label in enumerate(labels)}\n",
    "id2label = {i: label for label, i in label_map.items()}\n",
    "\n",
    "# Convert tokens and labels to strings for tokenizer\n",
    "texts = tokend_text\n",
    "labels = [[label_map[label] for label in doc_labels] for doc_labels in cor_labels]\n",
    "\n",
    "# Create a custom dataset class\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.texts[index]\n",
    "        word_labels = self.labels[index]\n",
    "\n",
    "        encoding = self.tokenizer(text,\n",
    "                                  truncation=True,\n",
    "                                  padding='max_length',  # Ensure padding\n",
    "                                  max_length=self.max_len,\n",
    "                                  is_split_into_words=True,\n",
    "                                  return_tensors='pt')\n",
    "\n",
    "        word_ids = encoding.word_ids(batch_index=0)\n",
    "\n",
    "        # Create a mask and label array for the tokens\n",
    "        labels = [-100 if word_id is None else word_labels[word_id] for word_id in word_ids]\n",
    "\n",
    "        item = {key: val.squeeze().to(device) for key, val in encoding.items()}  # Move to GPU\n",
    "        item['labels'] = torch.tensor(labels, dtype=torch.long).to(device)  # Move to GPU\n",
    "\n",
    "        return item\n",
    "\n",
    "# Prepare the datasets and dataloaders\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Custom training loop with 10-fold cross-validation\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "all_metrics = {\n",
    "    'accuracy': [],\n",
    "    'precision_micro': [],\n",
    "    'recall_micro': [],\n",
    "    'f1_micro': [],\n",
    "    'precision_macro': [],\n",
    "    'recall_macro': [],\n",
    "    'f1_macro': []\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def train_one_fold(train_dataloader, val_dataloader, fold):\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "    total_steps = len(train_dataloader) * 3\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(3):\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            model.zero_grad()\n",
    "            inputs = {key: val.to(device) for key, val in batch.items()}\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        print(f\"Fold {fold}, Epoch {epoch + 1}, Loss: {total_loss / len(train_dataloader)}\")\n",
    "\n",
    "def evaluate_one_fold(val_dataloader):\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "\n",
    "    for batch in val_dataloader:\n",
    "        inputs = {key: val.to(device) for key, val in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        labels = inputs['labels']\n",
    "\n",
    "        for i in range(len(labels)):\n",
    "            true_labels.extend(labels[i].cpu().numpy())\n",
    "            pred_labels.extend(predictions[i].cpu().numpy())\n",
    "\n",
    "    true_labels = [label for label in true_labels if label != -100]\n",
    "    pred_labels = [pred for label, pred in zip(true_labels, pred_labels) if label != -100]\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, pred_labels)\n",
    "    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(true_labels, pred_labels, average='micro')\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(true_labels, pred_labels, average='macro')\n",
    "\n",
    "    return accuracy, precision_micro, recall_micro, f1_micro, precision_macro, recall_macro, f1_macro\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(texts)):\n",
    "    print(f\"Fold {fold + 1}\")\n",
    "\n",
    "    train_texts = [texts[i] for i in train_index]\n",
    "    val_texts = [texts[i] for i in val_index]\n",
    "    train_labels = [labels[i] for i in train_index]\n",
    "    val_labels = [labels[i] for i in val_index]\n",
    "\n",
    "    train_dataset = NERDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
    "    val_dataset = NERDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=BATCH_SIZE)\n",
    "    val_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=BATCH_SIZE)\n",
    "\n",
    "    train_one_fold(train_dataloader, val_dataloader, fold + 1)\n",
    "\n",
    "    accuracy, precision_micro, recall_micro, f1_micro, precision_macro, recall_macro, f1_macro = evaluate_one_fold(val_dataloader)\n",
    "\n",
    "    all_metrics['accuracy'].append(accuracy)\n",
    "    all_metrics['precision_micro'].append(precision_micro)\n",
    "    all_metrics['recall_micro'].append(recall_micro)\n",
    "    all_metrics['f1_micro'].append(f1_micro)\n",
    "    all_metrics['precision_macro'].append(precision_macro)\n",
    "    all_metrics['recall_macro'].append(recall_macro)\n",
    "    all_metrics['f1_macro'].append(f1_macro)\n",
    "\n",
    "    # Save the model after training each fold\n",
    "    model_save_path = f'./model/fold_{fold + 1}'\n",
    "    os.makedirs(model_save_path, exist_ok=True)\n",
    "    model.save_pretrained(model_save_path)\n",
    "    tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Micro Precision: {precision_micro}, Micro Recall: {recall_micro}, Micro F1: {f1_micro}\")\n",
    "    print(f\"Macro Precision: {precision_macro}, Macro Recall: {recall_macro}, Macro F1: {f1_macro}\")\n",
    "\n",
    "# Calculate mean and standard deviation of metrics\n",
    "metrics_mean_std = {metric: (np.mean(all_metrics[metric]), np.std(all_metrics[metric])) for metric in all_metrics}\n",
    "\n",
    "print(\"\\nMetrics Mean and Standard Deviation:\")\n",
    "for metric, (mean, std) in metrics_mean_std.items():\n",
    "    print(f\"{metric.capitalize()} - Mean: {mean}, Std: {std}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
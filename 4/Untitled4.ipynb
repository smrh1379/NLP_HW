{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d9d0bbd0d47c40ee870c63149c370cc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ddcbe4ebd6a2429a9a8e639427a26422",
              "IPY_MODEL_54c663b0153240f785640a600f622184",
              "IPY_MODEL_ae31059fee924a1e893a5e0fce917ad0"
            ],
            "layout": "IPY_MODEL_2c0cc9c1965948e4bb69b6ef7b80fd3a"
          }
        },
        "ddcbe4ebd6a2429a9a8e639427a26422": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00e823f4a55c401197527d5dd4f3d63e",
            "placeholder": "​",
            "style": "IPY_MODEL_8894e93c86dc4c50b8ef26a22ffbb9e5",
            "value": "Downloading builder script: "
          }
        },
        "54c663b0153240f785640a600f622184": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_014a1c7899ac44039b05f5fca49f3488",
            "max": 2169,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_08a649a8c6b84a05b4ba22bfa600a51d",
            "value": 2169
          }
        },
        "ae31059fee924a1e893a5e0fce917ad0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d266d5f00984dcea4e65984abdbf676",
            "placeholder": "​",
            "style": "IPY_MODEL_a8ae58809405430f87fb288bd43c5abd",
            "value": " 5.65k/? [00:00&lt;00:00, 363kB/s]"
          }
        },
        "2c0cc9c1965948e4bb69b6ef7b80fd3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00e823f4a55c401197527d5dd4f3d63e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8894e93c86dc4c50b8ef26a22ffbb9e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "014a1c7899ac44039b05f5fca49f3488": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08a649a8c6b84a05b4ba22bfa600a51d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5d266d5f00984dcea4e65984abdbf676": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8ae58809405430f87fb288bd43c5abd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "daf8dccb3e8c40bc9b121ee5e702e17f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_44eb11bdfb9243cc821cc71872ed2cea",
              "IPY_MODEL_e9a397f814eb4bf6b39780136368c698",
              "IPY_MODEL_9f64a2a8573548bea7a04be3884f9052"
            ],
            "layout": "IPY_MODEL_abe04e1bf04c44459e9430da3ff2b69f"
          }
        },
        "44eb11bdfb9243cc821cc71872ed2cea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1869f4515c6b41648196440a899ae0a0",
            "placeholder": "​",
            "style": "IPY_MODEL_e08b36b4cd694883897ff2f1385c1ce2",
            "value": "Downloading builder script: "
          }
        },
        "e9a397f814eb4bf6b39780136368c698": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_954ea6b704944706a8bde293107fc47f",
            "max": 2198,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5545f2074a974437a77272bdb50fe8f2",
            "value": 2198
          }
        },
        "9f64a2a8573548bea7a04be3884f9052": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c95d927825d4dd08212301349eec341",
            "placeholder": "​",
            "style": "IPY_MODEL_e3a980ca131e44d0b07391b51580d937",
            "value": " 5.34k/? [00:00&lt;00:00, 403kB/s]"
          }
        },
        "abe04e1bf04c44459e9430da3ff2b69f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1869f4515c6b41648196440a899ae0a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e08b36b4cd694883897ff2f1385c1ce2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "954ea6b704944706a8bde293107fc47f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5545f2074a974437a77272bdb50fe8f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8c95d927825d4dd08212301349eec341": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3a980ca131e44d0b07391b51580d937": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR5ioImHnyIT",
        "outputId": "0ec55c1f-ee9f-414d-9f73-91bd718ab3f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp drive/MyDrive/HW4_Train/train.json ."
      ],
      "metadata": {
        "id": "3s8NzigSn1Li"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# فایل اصلی که شامل داده‌های اولیه است\n",
        "input_file = 'train.json'\n",
        "# فایل خروجی که داده‌های تبدیل شده را ذخیره می‌کند\n",
        "output_file = 'evaluation_data.json'\n",
        "\n",
        "def convert_data(input_file, output_file, start_line=10000, end_line=10050):\n",
        "    data = []\n",
        "\n",
        "    # خواندن داده‌ها از فایل اصلی\n",
        "    with open(input_file, 'r', encoding='utf-8') as file:\n",
        "        for i, line in enumerate(file):\n",
        "            if i < start_line:\n",
        "                continue\n",
        "            if i > end_line:\n",
        "                break\n",
        "            try:\n",
        "                item = json.loads(line.strip())\n",
        "                # استخراج فیلدهای مورد نظر\n",
        "                new_item = {\n",
        "                    \"question\": item[\"question\"],\n",
        "                    \"exp\": item[\"exp\"]\n",
        "                }\n",
        "                data.append(new_item)\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Error decoding JSON at line {i}: {e}\")\n",
        "\n",
        "    # ذخیره داده‌های تبدیل شده در فایل جدید به فرمت JSON\n",
        "    with open(output_file, 'w', encoding='utf-8') as file:\n",
        "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "# اجرای تابع تبدیل داده‌ها\n",
        "convert_data(input_file, output_file)\n",
        "print(f\"Data converted and saved to {output_file}\")\n",
        "output_file = 'evaluation_data_train.json'\n",
        "convert_data(input_file, output_file,start_line=100, end_line=150)\n",
        "print(f\"Data converted and saved to {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gAWqHRVr3KR",
        "outputId": "6706c14f-4246-4aa0-8006-85636628bd0c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data converted and saved to evaluation_data.json\n",
            "Data converted and saved to evaluation_data_train.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install transformers\n",
        "%pip install datasets\n",
        "!pip install transformers datasets sacrebleu rouge-score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEw-xUwWwFlV",
        "outputId": "99716a05-432b-4cc9-c772-9842623518e2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.1)\n",
            "Collecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.8.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Downloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=8e5bd52ab034d7953eeb9fbdfccc7311bb9660e0ba365aa717e03f623cba16a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: portalocker, colorama, sacrebleu, rouge-score\n",
            "Successfully installed colorama-0.4.6 portalocker-2.10.1 rouge-score-0.1.2 sacrebleu-2.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Load and preprocess fine-tuning data\n",
        "def load_finetune_data(json_file,limit = 2000):\n",
        "    with open(json_file, 'r', encoding='utf-8') as file:\n",
        "        data = [json.loads(line) for line in file][:limit]\n",
        "    return data\n",
        "\n",
        "def preprocess_finetune_data(data):\n",
        "    inputs = [f\"پرسش: {item['question']}\" for item in data]\n",
        "    targets = [item['exp'] for item in data]\n",
        "    return inputs, targets\n",
        "\n",
        "# Step 2: Tokenize inputs and targets\n",
        "def tokenize_data(tokenizer, inputs, targets, max_length=512):\n",
        "    encodings = tokenizer(list(map(str, inputs)), truncation=True, padding=True, max_length=max_length)\n",
        "    labels = tokenizer(list(map(str, targets)), truncation=True, padding=True, max_length=max_length).input_ids\n",
        "\n",
        "    # Replace padding token id's in the labels by -100 to ignore padding in the loss\n",
        "    labels = [[(label if label != tokenizer.pad_token_id else -100) for label in labels_example] for labels_example in labels]\n",
        "\n",
        "    encodings['labels'] = labels\n",
        "    return encodings\n",
        "\n",
        "# Step 3: Fine-tune the T5 model\n",
        "def finetune_model(model, tokenizer, train_dataset, eval_dataset, output_dir=\"./results\"):\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=6,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        warmup_steps=1000,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=10,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "# Execute steps for fine-tuning\n",
        "json_file = 'train.json'\n",
        "finetune_data = load_finetune_data(json_file,limit=3000)\n",
        "inputs, targets = preprocess_finetune_data(finetune_data)\n",
        "\n",
        "# Split data into training and evaluation sets\n",
        "train_inputs, eval_inputs, train_targets, eval_targets = train_test_split(inputs, targets, test_size=0.1)\n",
        "\n",
        "# Load T5 model and tokenizer\n",
        "model_name = \"t5-small\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize inputs and targets\n",
        "train_encodings = tokenize_data(tokenizer, train_inputs, train_targets)\n",
        "eval_encodings = tokenize_data(tokenizer, eval_inputs, eval_targets)\n",
        "\n",
        "# Create datasets using the `datasets` library\n",
        "train_dataset = Dataset.from_dict(train_encodings)\n",
        "eval_dataset = Dataset.from_dict(eval_encodings)\n",
        "\n",
        "# Fine-tune the model\n",
        "finetune_model(model, tokenizer, train_dataset, eval_dataset)\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"./finetuned_model_6e\")\n",
        "tokenizer.save_pretrained(\"./finetuned_model_6e\")\n",
        "model.save_pretrained(\"./drive/MyDrive/finetuned_model_6e\")\n",
        "tokenizer.save_pretrained(\"./drive/MyDrive/finetuned_model_6e\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "riKUxDEkyKzU",
        "outputId": "d7d075c3-6b82-4177-f366-6c07f51bbd92"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2028' max='2028' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2028/2028 17:13, Epoch 6/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>4.701400</td>\n",
              "      <td>4.509769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>4.704900</td>\n",
              "      <td>4.277831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>4.388300</td>\n",
              "      <td>4.167476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>4.424300</td>\n",
              "      <td>4.105363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>4.394600</td>\n",
              "      <td>4.077777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>4.090000</td>\n",
              "      <td>4.070305</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./drive/MyDrive/finetuned_model_6e/tokenizer_config.json',\n",
              " './drive/MyDrive/finetuned_model_6e/special_tokens_map.json',\n",
              " './drive/MyDrive/finetuned_model_6e/spiece.model',\n",
              " './drive/MyDrive/finetuned_model_6e/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from datasets import load_metric\n",
        "\n",
        "# Step 1: Load data from JSON file\n",
        "def load_data(json_file, limit=None):\n",
        "    with open(json_file, 'r', encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "    if limit:\n",
        "        data = data[:limit]\n",
        "    return data\n",
        "\n",
        "# Step 2: Preprocess texts (filter out None values and combine fields)\n",
        "def preprocess_texts(data):\n",
        "    filtered_data = [item for item in data if item['exp'] is not None and item['question'] is not None]\n",
        "    texts = [f\"{item['question']} {item['exp']}\" for item in filtered_data]\n",
        "    return filtered_data, texts\n",
        "\n",
        "# Step 3: Compute TF-IDF for texts and user query\n",
        "def compute_tfidf(texts, query):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(texts + [query])\n",
        "    return tfidf_matrix\n",
        "\n",
        "# Step 4: Compute Cosine Similarity\n",
        "def find_similar_texts(tfidf_matrix):\n",
        "    query_vector = tfidf_matrix[-1]\n",
        "    similarities = cosine_similarity(query_vector, tfidf_matrix[:-1])\n",
        "    return similarities.flatten()\n",
        "\n",
        "# Step 5: Display related texts\n",
        "def get_related_texts(filtered_data, similarities, top_n=3):\n",
        "    sorted_indices = np.argsort(similarities)[-top_n:][::-1]\n",
        "    related_texts = [filtered_data[idx]['exp'] for idx in sorted_indices]\n",
        "    return related_texts\n",
        "\n",
        "# Step 6: Combine question and related texts to create input for the model\n",
        "def create_input(query, related_texts):\n",
        "    input_text = f\"پرسش: {query} زمینه: {' '.join(related_texts)}\"\n",
        "    return input_text\n",
        "\n",
        "# Step 7: Generate answer using the model\n",
        "def generate_answer(model, tokenizer, input_text, max_length=150, num_beams=5, early_stopping=True, temperature=1.0, top_k=None, top_p=None, repetition_penalty=1.0, no_repeat_ngram_size=2, length_penalty=2.0, do_sample=True):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        num_beams=num_beams,\n",
        "        early_stopping=early_stopping,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "        length_penalty=length_penalty,\n",
        "        do_sample=do_sample\n",
        "    )\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return answer\n",
        "\n",
        "# Load evaluation data (limited to 50 items)\n",
        "eval_file = 'evaluation_data.json'\n",
        "eval_data = load_data(eval_file, limit=50)\n",
        "\n",
        "# Initialize BLEU, ROUGE, and METEOR metrics\n",
        "bleu = load_metric(\"sacrebleu\")\n",
        "# rouge = load_metric(\"rouge\")\n",
        "# meteor = load_metric(\"meteor\")\n",
        "\n",
        "# Generate answers with the base model\n",
        "base_model_path = \"t5-small\"\n",
        "base_model = T5ForConditionalGeneration.from_pretrained(base_model_path)\n",
        "base_tokenizer = T5Tokenizer.from_pretrained(base_model_path)\n",
        "\n",
        "# Generate answers with the fine-tuned model\n",
        "finetuned_model_path = \"./finetuned_model_6e/\"\n",
        "finetuned_model = T5ForConditionalGeneration.from_pretrained(finetuned_model_path, local_files_only=True)\n",
        "finetuned_tokenizer = T5Tokenizer.from_pretrained(finetuned_model_path, local_files_only=True)\n",
        "\n",
        "# Function to evaluate model\n",
        "def evaluate_model(model, tokenizer, eval_data):\n",
        "    references = []\n",
        "    predictions = []\n",
        "    for idx, item in enumerate(eval_data):\n",
        "        query = item['question']\n",
        "        reference = item['exp']\n",
        "        _, texts = preprocess_texts([item])\n",
        "        if not texts:\n",
        "            print(f\"Skipping item {idx} with empty texts: {item}\")\n",
        "            continue\n",
        "        tfidf_matrix = compute_tfidf(texts, query)\n",
        "        if tfidf_matrix.shape[0] == 0:\n",
        "            print(f\"Empty TF-IDF matrix for query {idx}: {query}\")\n",
        "            continue\n",
        "        similarities = find_similar_texts(tfidf_matrix)\n",
        "        if len(similarities) == 0:\n",
        "            print(f\"No similarities found for query {idx}: {query}\")\n",
        "            continue\n",
        "        related_texts = get_related_texts([item], similarities)\n",
        "        input_text = create_input(query, related_texts)\n",
        "        prediction = generate_answer(model, tokenizer, input_text)\n",
        "        references.append(reference)\n",
        "        predictions.append(prediction)\n",
        "        print(f\"Processed item {idx}\")\n",
        "    return references, predictions\n",
        "\n",
        "# Evaluate base model\n",
        "print(\"Evaluating base model...\")\n",
        "base_references, base_predictions = evaluate_model(base_model, base_tokenizer, eval_data)\n",
        "\n",
        "# Evaluate fine-tuned model\n",
        "print(\"Evaluating fine-tuned model...\")\n",
        "finetuned_references, finetuned_predictions = evaluate_model(finetuned_model, finetuned_tokenizer, eval_data)\n",
        "\n",
        "# Calculate BLEU, ROUGE, and METEOR scores\n",
        "base_bleu_score = bleu.compute(predictions=base_predictions, references=[[ref] for ref in base_references])\n",
        "# base_rouge_score = rouge.compute(predictions=base_predictions, references=base_references)\n",
        "# base_meteor_score = meteor.compute(predictions=base_predictions, references=base_references)\n",
        "\n",
        "finetuned_bleu_score = bleu.compute(predictions=finetuned_predictions, references=[[ref] for ref in finetuned_references])\n",
        "# finetuned_rouge_score = rouge.compute(predictions=finetuned_predictions, references=finetuned_references)\n",
        "# finetuned_meteor_score = meteor.compute(predictions=finetuned_predictions, references=finetuned_references)\n",
        "\n",
        "print(\"Base Model BLEU Score:\", base_bleu_score)\n",
        "# print(\"Base Model ROUGE Score:\", base_rouge_score)\n",
        "# print(\"Base Model METEOR Score:\", base_meteor_score)\n",
        "\n",
        "print(\"Fine-tuned Model BLEU Score:\", finetuned_bleu_score)\n",
        "# print(\"Fine-tuned Model ROUGE Score:\", finetuned_rouge_score)\n",
        "# print(\"Fine-tuned Model METEOR Score:\", finetuned_meteor_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbZhAo2Nn5pf",
        "outputId": "b1b86c83-c52c-48da-e75b-75fe1ce15927"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating base model...\n",
            "Skipping item 0 with empty texts: {'question': 'Retinoscopy in 5 year old is best done with:', 'exp': None}\n",
            "Processed item 1\n",
            "Processed item 2\n",
            "Processed item 3\n",
            "Processed item 4\n",
            "Processed item 5\n",
            "Skipping item 6 with empty texts: {'question': 'True about streptococcus:', 'exp': None}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (590 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed item 7\n",
            "Processed item 8\n",
            "Processed item 9\n",
            "Processed item 10\n",
            "Processed item 11\n",
            "Skipping item 12 with empty texts: {'question': 'Patient diagnosed to have malaria, smear shows all stages of schizonts 14-20 merozoites, yellowish - brown pigment. The type of malaria is -', 'exp': None}\n",
            "Processed item 13\n",
            "Processed item 14\n",
            "Processed item 15\n",
            "Skipping item 16 with empty texts: {'question': 'Crude birth rate denominator is -', 'exp': None}\n",
            "Processed item 17\n",
            "Processed item 18\n",
            "Processed item 19\n",
            "Skipping item 20 with empty texts: {'question': 'Intrauterine exposure of diethylstilboestrol is associated with -', 'exp': None}\n",
            "Processed item 21\n",
            "Processed item 22\n",
            "Processed item 23\n",
            "Processed item 24\n",
            "Processed item 25\n",
            "Processed item 26\n",
            "Skipping item 27 with empty texts: {'question': 'Which of the following Antiepileptic acts by opening Potassium channel?', 'exp': None}\n",
            "Processed item 28\n",
            "Processed item 29\n",
            "Processed item 30\n",
            "Processed item 31\n",
            "Processed item 32\n",
            "Processed item 33\n",
            "Processed item 34\n",
            "Processed item 35\n",
            "Processed item 36\n",
            "Processed item 37\n",
            "Processed item 38\n",
            "Processed item 39\n",
            "Processed item 40\n",
            "Processed item 41\n",
            "Processed item 42\n",
            "Skipping item 43 with empty texts: {'question': 'Negative BMR is observed with:', 'exp': None}\n",
            "Processed item 44\n",
            "Skipping item 45 with empty texts: {'question': 'Asepsis means -', 'exp': None}\n",
            "Processed item 46\n",
            "Processed item 47\n",
            "Processed item 48\n",
            "Processed item 49\n",
            "Evaluating fine-tuned model...\n",
            "Skipping item 0 with empty texts: {'question': 'Retinoscopy in 5 year old is best done with:', 'exp': None}\n",
            "Processed item 1\n",
            "Processed item 2\n",
            "Processed item 3\n",
            "Processed item 4\n",
            "Processed item 5\n",
            "Skipping item 6 with empty texts: {'question': 'True about streptococcus:', 'exp': None}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (590 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed item 7\n",
            "Processed item 8\n",
            "Processed item 9\n",
            "Processed item 10\n",
            "Processed item 11\n",
            "Skipping item 12 with empty texts: {'question': 'Patient diagnosed to have malaria, smear shows all stages of schizonts 14-20 merozoites, yellowish - brown pigment. The type of malaria is -', 'exp': None}\n",
            "Processed item 13\n",
            "Processed item 14\n",
            "Processed item 15\n",
            "Skipping item 16 with empty texts: {'question': 'Crude birth rate denominator is -', 'exp': None}\n",
            "Processed item 17\n",
            "Processed item 18\n",
            "Processed item 19\n",
            "Skipping item 20 with empty texts: {'question': 'Intrauterine exposure of diethylstilboestrol is associated with -', 'exp': None}\n",
            "Processed item 21\n",
            "Processed item 22\n",
            "Processed item 23\n",
            "Processed item 24\n",
            "Processed item 25\n",
            "Processed item 26\n",
            "Skipping item 27 with empty texts: {'question': 'Which of the following Antiepileptic acts by opening Potassium channel?', 'exp': None}\n",
            "Processed item 28\n",
            "Processed item 29\n",
            "Processed item 30\n",
            "Processed item 31\n",
            "Processed item 32\n",
            "Processed item 33\n",
            "Processed item 34\n",
            "Processed item 35\n",
            "Processed item 36\n",
            "Processed item 37\n",
            "Processed item 38\n",
            "Processed item 39\n",
            "Processed item 40\n",
            "Processed item 41\n",
            "Processed item 42\n",
            "Skipping item 43 with empty texts: {'question': 'Negative BMR is observed with:', 'exp': None}\n",
            "Processed item 44\n",
            "Skipping item 45 with empty texts: {'question': 'Asepsis means -', 'exp': None}\n",
            "Processed item 46\n",
            "Processed item 47\n",
            "Processed item 48\n",
            "Processed item 49\n",
            "Base Model BLEU Score: {'score': 19.82599571130802, 'counts': [1537, 1278, 1105, 965], 'totals': [2026, 1984, 1942, 1900], 'precisions': [75.86377097729516, 64.41532258064517, 56.90010298661174, 50.78947368421053], 'bp': 0.32341295578259766, 'sys_len': 2026, 'ref_len': 4313}\n",
            "Fine-tuned Model BLEU Score: {'score': 22.273935464913475, 'counts': [1603, 1092, 819, 670], 'totals': [3193, 3151, 3109, 3067], 'precisions': [50.203570310053244, 34.65566486829578, 26.342875522676103, 21.845451581349852], 'bp': 0.7041475874888652, 'sys_len': 3193, 'ref_len': 4313}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from datasets import load_metric\n",
        "\n",
        "# Step 1: Load data from JSON file\n",
        "def load_data(json_file, limit=None):\n",
        "    with open(json_file, 'r', encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "    if limit:\n",
        "        data = data[:limit]\n",
        "    return data\n",
        "\n",
        "# Step 2: Preprocess texts (filter out None values and combine fields)\n",
        "def preprocess_texts(data):\n",
        "    filtered_data = [item for item in data if item['exp'] is not None and item['question'] is not None]\n",
        "    texts = [f\"{item['question']} {item['exp']}\" for item in filtered_data]\n",
        "    return filtered_data, texts\n",
        "\n",
        "# Step 3: Compute TF-IDF for texts and user query\n",
        "def compute_tfidf(texts, query):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(texts + [query])\n",
        "    return tfidf_matrix\n",
        "\n",
        "# Step 4: Compute Cosine Similarity\n",
        "def find_similar_texts(tfidf_matrix):\n",
        "    query_vector = tfidf_matrix[-1]\n",
        "    similarities = cosine_similarity(query_vector, tfidf_matrix[:-1])\n",
        "    return similarities.flatten()\n",
        "\n",
        "# Step 5: Display related texts\n",
        "def get_related_texts(filtered_data, similarities, top_n=3):\n",
        "    sorted_indices = np.argsort(similarities)[-top_n:][::-1]\n",
        "    related_texts = [filtered_data[idx]['exp'] for idx in sorted_indices]\n",
        "    return related_texts\n",
        "\n",
        "# Step 6: Combine question and related texts to create input for the model\n",
        "def create_input(query, related_texts):\n",
        "    input_text = f\"پرسش: {query} زمینه: {' '.join(related_texts)}\"\n",
        "    return input_text\n",
        "\n",
        "# Step 7: Generate answer using the model\n",
        "def generate_answer(model, tokenizer, input_text, max_length=150, num_beams=5, early_stopping=True, temperature=1.0, top_k=None, top_p=None, repetition_penalty=1.0, no_repeat_ngram_size=2, length_penalty=2.0, do_sample=True):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        num_beams=num_beams,\n",
        "        early_stopping=early_stopping,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "        length_penalty=length_penalty,\n",
        "        do_sample=do_sample\n",
        "    )\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return answer\n",
        "\n",
        "# Load evaluation data (limited to 50 items)\n",
        "eval_file = 'evaluation_data.json'\n",
        "eval_data = load_data(eval_file, limit=50)\n",
        "\n",
        "# Initialize BLEU, ROUGE, and METEOR metrics\n",
        "bleu = load_metric(\"sacrebleu\")\n",
        "rouge = load_metric(\"rouge\")\n",
        "meteor = load_metric(\"meteor\")\n",
        "\n",
        "# Generate answers with the base model\n",
        "base_model_path = \"t5-small\"\n",
        "base_model = T5ForConditionalGeneration.from_pretrained(base_model_path)\n",
        "base_tokenizer = T5Tokenizer.from_pretrained(base_model_path)\n",
        "\n",
        "# Generate answers with the fine-tuned model\n",
        "finetuned_model_path = \"./finetuned_model_6e/\"\n",
        "finetuned_model = T5ForConditionalGeneration.from_pretrained(finetuned_model_path, local_files_only=True)\n",
        "finetuned_tokenizer = T5Tokenizer.from_pretrained(finetuned_model_path, local_files_only=True)\n",
        "\n",
        "# Function to evaluate model\n",
        "def evaluate_model(model, tokenizer, eval_data):\n",
        "    references = []\n",
        "    predictions = []\n",
        "    for idx, item in enumerate(eval_data):\n",
        "        query = item['question']\n",
        "        reference = item['exp']\n",
        "        _, texts = preprocess_texts([item])\n",
        "        if not texts:\n",
        "            print(f\"Skipping item {idx} with empty texts: {item}\")\n",
        "            continue\n",
        "        tfidf_matrix = compute_tfidf(texts, query)\n",
        "        if tfidf_matrix.shape[0] == 0:\n",
        "            print(f\"Empty TF-IDF matrix for query {idx}: {query}\")\n",
        "            continue\n",
        "        similarities = find_similar_texts(tfidf_matrix)\n",
        "        if len(similarities) == 0:\n",
        "            print(f\"No similarities found for query {idx}: {query}\")\n",
        "            continue\n",
        "        related_texts = get_related_texts([item], similarities)\n",
        "        input_text = create_input(query, related_texts)\n",
        "        prediction = generate_answer(model, tokenizer, input_text)\n",
        "        references.append(reference)\n",
        "        predictions.append(prediction)\n",
        "        print(f\"Processed item {idx}\")\n",
        "    return references, predictions\n",
        "\n",
        "# Evaluate base model\n",
        "print(\"Evaluating base model...\")\n",
        "base_references, base_predictions = evaluate_model(base_model, base_tokenizer, eval_data)\n",
        "\n",
        "# Evaluate fine-tuned model\n",
        "print(\"Evaluating fine-tuned model...\")\n",
        "finetuned_references, finetuned_predictions = evaluate_model(finetuned_model, finetuned_tokenizer, eval_data)\n",
        "\n",
        "# Calculate BLEU, ROUGE, and METEOR scores\n",
        "base_bleu_score = bleu.compute(predictions=base_predictions, references=[[ref] for ref in base_references])\n",
        "base_rouge_score = rouge.compute(predictions=base_predictions, references=base_references)\n",
        "base_meteor_score = meteor.compute(predictions=base_predictions, references=base_references)\n",
        "\n",
        "finetuned_bleu_score = bleu.compute(predictions=finetuned_predictions, references=[[ref] for ref in finetuned_references])\n",
        "finetuned_rouge_score = rouge.compute(predictions=finetuned_predictions, references=finetuned_references)\n",
        "finetuned_meteor_score = meteor.compute(predictions=finetuned_predictions, references=finetuned_references)\n",
        "\n",
        "print(\"Base Model BLEU Score:\", base_bleu_score)\n",
        "print(\"Base Model ROUGE Score:\", base_rouge_score)\n",
        "print(\"Base Model METEOR Score:\", base_meteor_score)\n",
        "\n",
        "print(\"Fine-tuned Model BLEU Score:\", finetuned_bleu_score)\n",
        "print(\"Fine-tuned Model ROUGE Score:\", finetuned_rouge_score)\n",
        "print(\"Fine-tuned Model METEOR Score:\", finetuned_meteor_score)\n",
        "\n",
        "# Calculate precision, recall, and accuracy\n",
        "def calculate_precision_recall_accuracy(predictions, references):\n",
        "    precision = np.mean([1 if pred in ref else 0 for pred, ref in zip(predictions, references)])\n",
        "    recall = np.mean([1 if ref in pred else 0 for pred, ref in zip(predictions, references)])\n",
        "    accuracy = np.mean([1 if pred == ref else 0 for pred, ref in zip(predictions, references)])\n",
        "    return precision, recall, accuracy\n",
        "\n",
        "# Base model precision, recall, accuracy\n",
        "base_precision, base_recall, base_accuracy = calculate_precision_recall_accuracy(base_predictions, base_references)\n",
        "print(\"Base Model Precision:\", base_precision)\n",
        "print(\"Base Model Recall:\", base_recall)\n",
        "print(\"Base Model Accuracy:\", base_accuracy)\n",
        "\n",
        "# Fine-tuned model precision, recall, accuracy\n",
        "finetuned_precision, finetuned_recall, finetuned_accuracy = calculate_precision_recall_accuracy(finetuned_predictions, finetuned_references)\n",
        "print(\"Fine-tuned Model Precision:\", finetuned_precision)\n",
        "print(\"Fine-tuned Model Recall:\", finetuned_recall)\n",
        "print(\"Fine-tuned Model Accuracy:\", finetuned_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d9d0bbd0d47c40ee870c63149c370cc2",
            "ddcbe4ebd6a2429a9a8e639427a26422",
            "54c663b0153240f785640a600f622184",
            "ae31059fee924a1e893a5e0fce917ad0",
            "2c0cc9c1965948e4bb69b6ef7b80fd3a",
            "00e823f4a55c401197527d5dd4f3d63e",
            "8894e93c86dc4c50b8ef26a22ffbb9e5",
            "014a1c7899ac44039b05f5fca49f3488",
            "08a649a8c6b84a05b4ba22bfa600a51d",
            "5d266d5f00984dcea4e65984abdbf676",
            "a8ae58809405430f87fb288bd43c5abd",
            "daf8dccb3e8c40bc9b121ee5e702e17f",
            "44eb11bdfb9243cc821cc71872ed2cea",
            "e9a397f814eb4bf6b39780136368c698",
            "9f64a2a8573548bea7a04be3884f9052",
            "abe04e1bf04c44459e9430da3ff2b69f",
            "1869f4515c6b41648196440a899ae0a0",
            "e08b36b4cd694883897ff2f1385c1ce2",
            "954ea6b704944706a8bde293107fc47f",
            "5545f2074a974437a77272bdb50fe8f2",
            "8c95d927825d4dd08212301349eec341",
            "e3a980ca131e44d0b07391b51580d937"
          ]
        },
        "id": "xyAI3a8cn5m5",
        "outputId": "6d58c9f3-5089-4fed-855e-c8490c7ca934"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9d0bbd0d47c40ee870c63149c370cc2"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The repository for rouge contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/rouge.\n",
            "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
            "\n",
            "Do you wish to run the custom code? [y/N] y\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "daf8dccb3e8c40bc9b121ee5e702e17f"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The repository for meteor contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/meteor.\n",
            "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
            "\n",
            "Do you wish to run the custom code? [y/N] y\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating base model...\n",
            "Skipping item 0 with empty texts: {'question': 'Retinoscopy in 5 year old is best done with:', 'exp': None}\n",
            "Processed item 1\n",
            "Processed item 2\n",
            "Processed item 3\n",
            "Processed item 4\n",
            "Processed item 5\n",
            "Skipping item 6 with empty texts: {'question': 'True about streptococcus:', 'exp': None}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (590 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed item 7\n",
            "Processed item 8\n",
            "Processed item 9\n",
            "Processed item 10\n",
            "Processed item 11\n",
            "Skipping item 12 with empty texts: {'question': 'Patient diagnosed to have malaria, smear shows all stages of schizonts 14-20 merozoites, yellowish - brown pigment. The type of malaria is -', 'exp': None}\n",
            "Processed item 13\n",
            "Processed item 14\n",
            "Processed item 15\n",
            "Skipping item 16 with empty texts: {'question': 'Crude birth rate denominator is -', 'exp': None}\n",
            "Processed item 17\n",
            "Processed item 18\n",
            "Processed item 19\n",
            "Skipping item 20 with empty texts: {'question': 'Intrauterine exposure of diethylstilboestrol is associated with -', 'exp': None}\n",
            "Processed item 21\n",
            "Processed item 22\n",
            "Processed item 23\n",
            "Processed item 24\n",
            "Processed item 25\n",
            "Processed item 26\n",
            "Skipping item 27 with empty texts: {'question': 'Which of the following Antiepileptic acts by opening Potassium channel?', 'exp': None}\n",
            "Processed item 28\n",
            "Processed item 29\n",
            "Processed item 30\n",
            "Processed item 31\n",
            "Processed item 32\n",
            "Processed item 33\n",
            "Processed item 34\n",
            "Processed item 35\n",
            "Processed item 36\n",
            "Processed item 37\n",
            "Processed item 38\n",
            "Processed item 39\n",
            "Processed item 40\n",
            "Processed item 41\n",
            "Processed item 42\n",
            "Skipping item 43 with empty texts: {'question': 'Negative BMR is observed with:', 'exp': None}\n",
            "Processed item 44\n",
            "Skipping item 45 with empty texts: {'question': 'Asepsis means -', 'exp': None}\n",
            "Processed item 46\n",
            "Processed item 47\n",
            "Processed item 48\n",
            "Processed item 49\n",
            "Evaluating fine-tuned model...\n",
            "Skipping item 0 with empty texts: {'question': 'Retinoscopy in 5 year old is best done with:', 'exp': None}\n",
            "Processed item 1\n",
            "Processed item 2\n",
            "Processed item 3\n",
            "Processed item 4\n",
            "Processed item 5\n",
            "Skipping item 6 with empty texts: {'question': 'True about streptococcus:', 'exp': None}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (590 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed item 7\n",
            "Processed item 8\n",
            "Processed item 9\n",
            "Processed item 10\n",
            "Processed item 11\n",
            "Skipping item 12 with empty texts: {'question': 'Patient diagnosed to have malaria, smear shows all stages of schizonts 14-20 merozoites, yellowish - brown pigment. The type of malaria is -', 'exp': None}\n",
            "Processed item 13\n",
            "Processed item 14\n",
            "Processed item 15\n",
            "Skipping item 16 with empty texts: {'question': 'Crude birth rate denominator is -', 'exp': None}\n",
            "Processed item 17\n",
            "Processed item 18\n",
            "Processed item 19\n",
            "Skipping item 20 with empty texts: {'question': 'Intrauterine exposure of diethylstilboestrol is associated with -', 'exp': None}\n",
            "Processed item 21\n",
            "Processed item 22\n",
            "Processed item 23\n",
            "Processed item 24\n",
            "Processed item 25\n",
            "Processed item 26\n",
            "Skipping item 27 with empty texts: {'question': 'Which of the following Antiepileptic acts by opening Potassium channel?', 'exp': None}\n",
            "Processed item 28\n",
            "Processed item 29\n",
            "Processed item 30\n",
            "Processed item 31\n",
            "Processed item 32\n",
            "Processed item 33\n",
            "Processed item 34\n",
            "Processed item 35\n",
            "Processed item 36\n",
            "Processed item 37\n",
            "Processed item 38\n",
            "Processed item 39\n",
            "Processed item 40\n",
            "Processed item 41\n",
            "Processed item 42\n",
            "Skipping item 43 with empty texts: {'question': 'Negative BMR is observed with:', 'exp': None}\n",
            "Processed item 44\n",
            "Skipping item 45 with empty texts: {'question': 'Asepsis means -', 'exp': None}\n",
            "Processed item 46\n",
            "Processed item 47\n",
            "Processed item 48\n",
            "Processed item 49\n",
            "Base Model BLEU Score: {'score': 17.629505379361227, 'counts': [1444, 1194, 1045, 922], 'totals': [1924, 1882, 1840, 1798], 'precisions': [75.05197505197505, 63.443145589798085, 56.79347826086956, 51.279199110122356], 'bp': 0.2888973074142663, 'sys_len': 1924, 'ref_len': 4313}\n",
            "Base Model ROUGE Score: {'rouge1': AggregateScore(low=Score(precision=0.6584495807825766, recall=0.4644801900567144, fmeasure=0.4953541960164611), mid=Score(precision=0.7389642138212178, recall=0.5554905945834413, fmeasure=0.5694716586596447), high=Score(precision=0.8151307507976887, recall=0.6457309938687599, fmeasure=0.6399972319082969)), 'rouge2': AggregateScore(low=Score(precision=0.5452242770456928, recall=0.36550011949895894, fmeasure=0.40126482478481196), mid=Score(precision=0.6306887233856762, recall=0.4630973585976128, fmeasure=0.4859809939435884), high=Score(precision=0.7134133465505179, recall=0.5682145521853904, fmeasure=0.5710265201701509)), 'rougeL': AggregateScore(low=Score(precision=0.5838192871994886, recall=0.4115643946926448, fmeasure=0.43585860423252193), mid=Score(precision=0.6639170173877045, recall=0.5029000985730347, fmeasure=0.5163101110923074), high=Score(precision=0.7420560939512663, recall=0.5928675834398709, fmeasure=0.5937233189324366)), 'rougeLsum': AggregateScore(low=Score(precision=0.5800662912268134, recall=0.4158107513761582, fmeasure=0.44062169650641175), mid=Score(precision=0.665996487088848, recall=0.5043160182192348, fmeasure=0.5159612802063024), high=Score(precision=0.7426687534034362, recall=0.593497308135768, fmeasure=0.5876284601891054))}\n",
            "Base Model METEOR Score: {'meteor': 0.4999402446590186}\n",
            "Fine-tuned Model BLEU Score: {'score': 18.91291897774872, 'counts': [1478, 962, 723, 568], 'totals': [2894, 2852, 2810, 2768], 'precisions': [51.07118175535591, 33.73071528751753, 25.729537366548044, 20.520231213872833], 'bp': 0.6124274393479621, 'sys_len': 2894, 'ref_len': 4313}\n",
            "Fine-tuned Model ROUGE Score: {'rouge1': AggregateScore(low=Score(precision=0.4594775631015364, recall=0.4378280755172668, fmeasure=0.3854562880634302), mid=Score(precision=0.5566060495765959, recall=0.5105998964396314, fmeasure=0.4429117354711508), high=Score(precision=0.6542208563391078, recall=0.5862908301556711, fmeasure=0.5045608857578968)), 'rouge2': AggregateScore(low=Score(precision=0.32584070837432805, recall=0.28336064831616453, fmeasure=0.2626053936252341), mid=Score(precision=0.41274941504984636, recall=0.3488868053326081, fmeasure=0.32258342272553076), high=Score(precision=0.5147320291164528, recall=0.42512589771845627, fmeasure=0.388462785733369)), 'rougeL': AggregateScore(low=Score(precision=0.3437370440226461, recall=0.32992711743482894, fmeasure=0.279191860144125), mid=Score(precision=0.43267205524162583, recall=0.39221486276181894, fmeasure=0.33580727902618546), high=Score(precision=0.521256249822079, recall=0.46764538370978204, fmeasure=0.39268628487437257)), 'rougeLsum': AggregateScore(low=Score(precision=0.3466371373318507, recall=0.3306778880521213, fmeasure=0.28298369458492506), mid=Score(precision=0.4359221408390711, recall=0.39607910957985837, fmeasure=0.3375677312679419), high=Score(precision=0.5193489760706634, recall=0.46500938597061847, fmeasure=0.39563534453041727))}\n",
            "Fine-tuned Model METEOR Score: {'meteor': 0.38879519825876346}\n",
            "Base Model Precision: 0.07142857142857142\n",
            "Base Model Recall: 0.023809523809523808\n",
            "Base Model Accuracy: 0.0\n",
            "Fine-tuned Model Precision: 0.047619047619047616\n",
            "Fine-tuned Model Recall: 0.023809523809523808\n",
            "Fine-tuned Model Accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from datasets import load_metric\n",
        "\n",
        "# Step 1: Load data from JSON file\n",
        "def load_data(json_file, limit=None):\n",
        "    with open(json_file, 'r', encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "    if limit:\n",
        "        data = data[:limit]\n",
        "    return data\n",
        "\n",
        "# Step 2: Preprocess texts (filter out None values and combine fields)\n",
        "def preprocess_texts(data):\n",
        "    filtered_data = [item for item in data if item['exp'] is not None and item['question'] is not None]\n",
        "    texts = [f\"{item['question']} {item['exp']}\" for item in filtered_data]\n",
        "    return filtered_data, texts\n",
        "\n",
        "# Step 3: Compute TF-IDF for texts and user query\n",
        "def compute_tfidf(texts, query):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(texts + [query])\n",
        "    return tfidf_matrix\n",
        "\n",
        "# Step 4: Compute Cosine Similarity\n",
        "def find_similar_texts(tfidf_matrix):\n",
        "    query_vector = tfidf_matrix[-1]\n",
        "    similarities = cosine_similarity(query_vector, tfidf_matrix[:-1])\n",
        "    return similarities.flatten()\n",
        "\n",
        "# Step 5: Display related texts\n",
        "def get_related_texts(filtered_data, similarities, top_n=3):\n",
        "    sorted_indices = np.argsort(similarities)[-top_n:][::-1]\n",
        "    related_texts = [filtered_data[idx]['exp'] for idx in sorted_indices]\n",
        "    return related_texts\n",
        "\n",
        "# Step 6: Combine question and related texts to create input for the model\n",
        "def create_input(query, related_texts):\n",
        "    input_text = f\"پرسش: {query} زمینه: {' '.join(related_texts)}\"\n",
        "    return input_text\n",
        "\n",
        "# Step 7: Generate answer using the model\n",
        "def generate_answer(model, tokenizer, input_text, max_length=150, num_beams=5, early_stopping=True, temperature=1.0, top_k=None, top_p=None, repetition_penalty=1.0, no_repeat_ngram_size=2, length_penalty=2.0, do_sample=True):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        num_beams=num_beams,\n",
        "        early_stopping=early_stopping,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "        length_penalty=length_penalty,\n",
        "        do_sample=do_sample\n",
        "    )\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return answer\n",
        "\n",
        "# Load evaluation data (limited to 50 items)\n",
        "eval_file = 'evaluation_data_train.json'\n",
        "eval_data = load_data(eval_file, limit=50)\n",
        "\n",
        "# Initialize BLEU, ROUGE, and METEOR metrics\n",
        "bleu = load_metric(\"sacrebleu\")\n",
        "rouge = load_metric(\"rouge\")\n",
        "meteor = load_metric(\"meteor\")\n",
        "\n",
        "# Generate answers with the base model\n",
        "base_model_path = \"t5-small\"\n",
        "base_model = T5ForConditionalGeneration.from_pretrained(base_model_path)\n",
        "base_tokenizer = T5Tokenizer.from_pretrained(base_model_path)\n",
        "\n",
        "# Generate answers with the fine-tuned model\n",
        "finetuned_model_path = \"./finetuned_model_6e/\"\n",
        "finetuned_model = T5ForConditionalGeneration.from_pretrained(finetuned_model_path, local_files_only=True)\n",
        "finetuned_tokenizer = T5Tokenizer.from_pretrained(finetuned_model_path, local_files_only=True)\n",
        "\n",
        "# Function to evaluate model\n",
        "def evaluate_model(model, tokenizer, eval_data):\n",
        "    references = []\n",
        "    predictions = []\n",
        "    for idx, item in enumerate(eval_data):\n",
        "        query = item['question']\n",
        "        reference = item['exp']\n",
        "        _, texts = preprocess_texts([item])\n",
        "        if not texts:\n",
        "            print(f\"Skipping item {idx} with empty texts: {item}\")\n",
        "            continue\n",
        "        tfidf_matrix = compute_tfidf(texts, query)\n",
        "        if tfidf_matrix.shape[0] == 0:\n",
        "            print(f\"Empty TF-IDF matrix for query {idx}: {query}\")\n",
        "            continue\n",
        "        similarities = find_similar_texts(tfidf_matrix)\n",
        "        if len(similarities) == 0:\n",
        "            print(f\"No similarities found for query {idx}: {query}\")\n",
        "            continue\n",
        "        related_texts = get_related_texts([item], similarities)\n",
        "        input_text = create_input(query, related_texts)\n",
        "        prediction = generate_answer(model, tokenizer, input_text)\n",
        "        references.append(reference)\n",
        "        predictions.append(prediction)\n",
        "        print(f\"Processed item {idx}\")\n",
        "    return references, predictions\n",
        "\n",
        "# Evaluate base model\n",
        "print(\"Evaluating base model...\")\n",
        "base_references, base_predictions = evaluate_model(base_model, base_tokenizer, eval_data)\n",
        "\n",
        "# Evaluate fine-tuned model\n",
        "print(\"Evaluating fine-tuned model...\")\n",
        "finetuned_references, finetuned_predictions = evaluate_model(finetuned_model, finetuned_tokenizer, eval_data)\n",
        "\n",
        "# Calculate BLEU, ROUGE, and METEOR scores\n",
        "base_bleu_score = bleu.compute(predictions=base_predictions, references=[[ref] for ref in base_references])\n",
        "base_rouge_score = rouge.compute(predictions=base_predictions, references=base_references)\n",
        "base_meteor_score = meteor.compute(predictions=base_predictions, references=base_references)\n",
        "\n",
        "finetuned_bleu_score = bleu.compute(predictions=finetuned_predictions, references=[[ref] for ref in finetuned_references])\n",
        "finetuned_rouge_score = rouge.compute(predictions=finetuned_predictions, references=finetuned_references)\n",
        "finetuned_meteor_score = meteor.compute(predictions=finetuned_predictions, references=finetuned_references)\n",
        "\n",
        "print(\"Base Model BLEU Score:\", base_bleu_score)\n",
        "print(\"Base Model ROUGE Score:\", base_rouge_score)\n",
        "print(\"Base Model METEOR Score:\", base_meteor_score)\n",
        "\n",
        "print(\"Fine-tuned Model BLEU Score:\", finetuned_bleu_score)\n",
        "print(\"Fine-tuned Model ROUGE Score:\", finetuned_rouge_score)\n",
        "print(\"Fine-tuned Model METEOR Score:\", finetuned_meteor_score)\n",
        "\n",
        "# Calculate precision, recall, and accuracy\n",
        "def calculate_precision_recall_accuracy(predictions, references):\n",
        "    precision = np.mean([1 if pred in ref else 0 for pred, ref in zip(predictions, references)])\n",
        "    recall = np.mean([1 if ref in pred else 0 for pred, ref in zip(predictions, references)])\n",
        "    accuracy = np.mean([1 if pred == ref else 0 for pred, ref in zip(predictions, references)])\n",
        "    return precision, recall, accuracy\n",
        "\n",
        "# Base model precision, recall, accuracy\n",
        "base_precision, base_recall, base_accuracy = calculate_precision_recall_accuracy(base_predictions, base_references)\n",
        "print(\"Base Model Precision:\", base_precision)\n",
        "print(\"Base Model Recall:\", base_recall)\n",
        "print(\"Base Model Accuracy:\", base_accuracy)\n",
        "\n",
        "# Fine-tuned model precision, recall, accuracy\n",
        "finetuned_precision, finetuned_recall, finetuned_accuracy = calculate_precision_recall_accuracy(finetuned_predictions, finetuned_references)\n",
        "print(\"Fine-tuned Model Precision:\", finetuned_precision)\n",
        "print(\"Fine-tuned Model Recall:\", finetuned_recall)\n",
        "print(\"Fine-tuned Model Accuracy:\", finetuned_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqANqrkHYZQ7",
        "outputId": "2ef17307-aa3f-4b80-cfa3-821c404b8188"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating base model...\n",
            "Processed item 0\n",
            "Processed item 1\n",
            "Processed item 2\n",
            "Processed item 3\n",
            "Processed item 4\n",
            "Processed item 5\n",
            "Processed item 6\n",
            "Processed item 7\n",
            "Processed item 8\n",
            "Processed item 9\n",
            "Processed item 10\n",
            "Skipping item 11 with empty texts: {'question': 'Relining of complete denture is not indicated when', 'exp': None}\n",
            "Processed item 12\n",
            "Processed item 13\n",
            "Processed item 14\n",
            "Processed item 15\n",
            "Processed item 16\n",
            "Processed item 17\n",
            "Processed item 18\n",
            "Processed item 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (540 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed item 20\n",
            "Skipping item 21 with empty texts: {'question': 'Secondary retention for a removable partial denture is provided by', 'exp': None}\n",
            "Processed item 22\n",
            "Processed item 23\n",
            "Skipping item 24 with empty texts: {'question': 'Egg shell calcification is seen in all except –', 'exp': None}\n",
            "Processed item 25\n",
            "Processed item 26\n",
            "Processed item 27\n",
            "Processed item 28\n",
            "Processed item 29\n",
            "Processed item 30\n",
            "Skipping item 31 with empty texts: {'question': 'A labourer involved with repair-work of sewers was admitted with fever, jaundice and renal failure. The most appropriate test to diagnose the infection in this patient is -', 'exp': None}\n",
            "Processed item 32\n",
            "Processed item 33\n",
            "Processed item 34\n",
            "Processed item 35\n",
            "Skipping item 36 with empty texts: {'question': 'A 40-year old diabetic patient presents with proptosis of one eye and black eschar over palate. The likely organism is :', 'exp': None}\n",
            "Processed item 37\n",
            "Skipping item 38 with empty texts: {'question': 'Investigations in a clinically suspected case of tuberculosis -', 'exp': None}\n",
            "Processed item 39\n",
            "Processed item 40\n",
            "Processed item 41\n",
            "Processed item 42\n",
            "Processed item 43\n",
            "Processed item 44\n",
            "Skipping item 45 with empty texts: {'question': 'Dimension stability of hydrocollids may be achieved by', 'exp': None}\n",
            "Processed item 46\n",
            "Processed item 47\n",
            "Processed item 48\n",
            "Processed item 49\n",
            "Evaluating fine-tuned model...\n",
            "Processed item 0\n",
            "Processed item 1\n",
            "Processed item 2\n",
            "Processed item 3\n",
            "Processed item 4\n",
            "Processed item 5\n",
            "Processed item 6\n",
            "Processed item 7\n",
            "Processed item 8\n",
            "Processed item 9\n",
            "Processed item 10\n",
            "Skipping item 11 with empty texts: {'question': 'Relining of complete denture is not indicated when', 'exp': None}\n",
            "Processed item 12\n",
            "Processed item 13\n",
            "Processed item 14\n",
            "Processed item 15\n",
            "Processed item 16\n",
            "Processed item 17\n",
            "Processed item 18\n",
            "Processed item 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (540 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed item 20\n",
            "Skipping item 21 with empty texts: {'question': 'Secondary retention for a removable partial denture is provided by', 'exp': None}\n",
            "Processed item 22\n",
            "Processed item 23\n",
            "Skipping item 24 with empty texts: {'question': 'Egg shell calcification is seen in all except –', 'exp': None}\n",
            "Processed item 25\n",
            "Processed item 26\n",
            "Processed item 27\n",
            "Processed item 28\n",
            "Processed item 29\n",
            "Processed item 30\n",
            "Skipping item 31 with empty texts: {'question': 'A labourer involved with repair-work of sewers was admitted with fever, jaundice and renal failure. The most appropriate test to diagnose the infection in this patient is -', 'exp': None}\n",
            "Processed item 32\n",
            "Processed item 33\n",
            "Processed item 34\n",
            "Processed item 35\n",
            "Skipping item 36 with empty texts: {'question': 'A 40-year old diabetic patient presents with proptosis of one eye and black eschar over palate. The likely organism is :', 'exp': None}\n",
            "Processed item 37\n",
            "Skipping item 38 with empty texts: {'question': 'Investigations in a clinically suspected case of tuberculosis -', 'exp': None}\n",
            "Processed item 39\n",
            "Processed item 40\n",
            "Processed item 41\n",
            "Processed item 42\n",
            "Processed item 43\n",
            "Processed item 44\n",
            "Skipping item 45 with empty texts: {'question': 'Dimension stability of hydrocollids may be achieved by', 'exp': None}\n",
            "Processed item 46\n",
            "Processed item 47\n",
            "Processed item 48\n",
            "Processed item 49\n",
            "Base Model BLEU Score: {'score': 16.414538629029156, 'counts': [1405, 1136, 965, 835], 'totals': [1872, 1829, 1786, 1743], 'precisions': [75.05341880341881, 62.11044286495353, 54.03135498320269, 47.905909351692486], 'bp': 0.2785070332728969, 'sys_len': 1872, 'ref_len': 4265}\n",
            "Base Model ROUGE Score: {'rouge1': AggregateScore(low=Score(precision=0.6174701137848696, recall=0.49425858947030915, fmeasure=0.47771548515492146), mid=Score(precision=0.6958337335508455, recall=0.5816958641593541, fmeasure=0.5415503309991446), high=Score(precision=0.7633205924509744, recall=0.6773031886371028, fmeasure=0.6066171192981327)), 'rouge2': AggregateScore(low=Score(precision=0.4969633396084659, recall=0.40294398675814763, fmeasure=0.3803014506467947), mid=Score(precision=0.5794032222903731, recall=0.5018272274378672, fmeasure=0.45404642480351765), high=Score(precision=0.6504696083862463, recall=0.598665969488075, fmeasure=0.5178392494254624)), 'rougeL': AggregateScore(low=Score(precision=0.5426480019579197, recall=0.4479630321487723, fmeasure=0.43011750819008054), mid=Score(precision=0.6153851818624625, recall=0.5449066978863475, fmeasure=0.49532456828786897), high=Score(precision=0.6860674179163179, recall=0.6515055708983575, fmeasure=0.5573882436617719)), 'rougeLsum': AggregateScore(low=Score(precision=0.545873089428845, recall=0.44289058471145776, fmeasure=0.4280478995442058), mid=Score(precision=0.6180942120061125, recall=0.5463434526936657, fmeasure=0.49663783104106807), high=Score(precision=0.6849146663198826, recall=0.6488826673816793, fmeasure=0.5622922954062215))}\n",
            "Base Model METEOR Score: {'meteor': 0.5013003114427612}\n",
            "Fine-tuned Model BLEU Score: {'score': 15.503477460617649, 'counts': [1321, 739, 529, 400], 'totals': [3261, 3218, 3175, 3132], 'precisions': [40.509046304814476, 22.964574269732754, 16.661417322834644, 12.77139208173691], 'bp': 0.7350027648375679, 'sys_len': 3261, 'ref_len': 4265}\n",
            "Fine-tuned Model ROUGE Score: {'rouge1': AggregateScore(low=Score(precision=0.3226320687191391, recall=0.45987974109651825, fmeasure=0.29463213197341304), mid=Score(precision=0.4006700508440416, recall=0.5449881205404803, fmeasure=0.3450711294429102), high=Score(precision=0.4785676093331119, recall=0.619358187357869, fmeasure=0.3988831109341237)), 'rouge2': AggregateScore(low=Score(precision=0.1865908269464008, recall=0.2647716578507837, fmeasure=0.17482995756754655), mid=Score(precision=0.24202335703014738, recall=0.347109836098933, fmeasure=0.2153049282781186), high=Score(precision=0.29907754009063725, recall=0.4410539232209264, fmeasure=0.257928049609369)), 'rougeL': AggregateScore(low=Score(precision=0.22581347073205177, recall=0.3520720798004508, fmeasure=0.2145814528383853), mid=Score(precision=0.28698158688131625, recall=0.4488308510080661, fmeasure=0.2547132258694915), high=Score(precision=0.3518914752655384, recall=0.5433098174217582, fmeasure=0.30054212841480327)), 'rougeLsum': AggregateScore(low=Score(precision=0.22894385464079114, recall=0.35743201636972644, fmeasure=0.2133761278207679), mid=Score(precision=0.289740954518665, recall=0.4507699726709835, fmeasure=0.2577788643930452), high=Score(precision=0.35936552426032403, recall=0.5553657476303167, fmeasure=0.3017202972658442))}\n",
            "Fine-tuned Model METEOR Score: {'meteor': 0.3302520341477321}\n",
            "Base Model Precision: 0.046511627906976744\n",
            "Base Model Recall: 0.20930232558139536\n",
            "Base Model Accuracy: 0.0\n",
            "Fine-tuned Model Precision: 0.0\n",
            "Fine-tuned Model Recall: 0.06976744186046512\n",
            "Fine-tuned Model Accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from datasets import Dataset, load_metric\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Load and preprocess fine-tuning data\n",
        "def load_finetune_data(json_file, limit=2000):\n",
        "    with open(json_file, 'r', encoding='utf-8') as file:\n",
        "        data = [json.loads(line) for line in file][:limit]\n",
        "    return data\n",
        "\n",
        "def preprocess_finetune_data(data):\n",
        "    inputs = [f\"پرسش: {item['question']}\" for item in data]\n",
        "    targets = [item['exp'] for item in data]\n",
        "    return inputs, targets\n",
        "\n",
        "# Step 2: Tokenize inputs and targets\n",
        "def tokenize_data(tokenizer, inputs, targets, max_length=512):\n",
        "    encodings = tokenizer(list(map(str, inputs)), truncation=True, padding=True, max_length=max_length)\n",
        "    labels = tokenizer(list(map(str, targets)), truncation=True, padding=True, max_length=max_length).input_ids\n",
        "\n",
        "    # Replace padding token id's in the labels by -100 to ignore padding in the loss\n",
        "    labels = [[(label if label != tokenizer.pad_token_id else -100) for label in labels_example] for labels_example in labels]\n",
        "\n",
        "    encodings['labels'] = labels\n",
        "    return encodings\n",
        "\n",
        "# Step 3: Fine-tune the T5 model\n",
        "def finetune_model(model, tokenizer, train_dataset, eval_dataset, output_dir=\"./results\"):\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=10,  # Increase the number of epochs\n",
        "        per_device_train_batch_size=16,  # Increase batch size if GPU memory allows\n",
        "        per_device_eval_batch_size=16,\n",
        "        warmup_steps=500,  # Reduced warmup steps\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=10,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"loss\",  # Track loss for best model\n",
        "        greater_is_better=False,\n",
        "        fp16=True,  # Enable mixed precision training for faster performance on GPUs\n",
        "        learning_rate=3e-4,  # Increase learning rate slightly\n",
        "        lr_scheduler_type=\"cosine\",  # Use a cosine scheduler for learning rate\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "# Execute steps for fine-tuning\n",
        "json_file = 'train.json'\n",
        "finetune_data = load_finetune_data(json_file, limit=5000)  # Increase data limit for fine-tuning\n",
        "inputs, targets = preprocess_finetune_data(finetune_data)\n",
        "\n",
        "# Split data into training and evaluation sets\n",
        "train_inputs, eval_inputs, train_targets, eval_targets = train_test_split(inputs, targets, test_size=0.1, random_state=42)\n",
        "\n",
        "# Load T5 model and tokenizer\n",
        "model_name = \"t5-small\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize inputs and targets\n",
        "train_encodings = tokenize_data(tokenizer, train_inputs, train_targets)\n",
        "eval_encodings = tokenize_data(tokenizer, eval_inputs, eval_targets)\n",
        "\n",
        "# Create datasets using the datasets library\n",
        "train_dataset = Dataset.from_dict(train_encodings)\n",
        "eval_dataset = Dataset.from_dict(eval_encodings)\n",
        "\n",
        "# Fine-tune the model\n",
        "finetune_model(model, tokenizer, train_dataset, eval_dataset)\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"./finetuned_model_10e\")\n",
        "tokenizer.save_pretrained(\"./finetuned_model_10e\")\n",
        "model.save_pretrained(\"./drive/MyDrive/finetuned_model_10e\")\n",
        "tokenizer.save_pretrained(\"./drive/MyDrive/finetuned_model_10e\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "umogrnTWn5kc",
        "outputId": "60d7ce03-4e17-4453-8ae0-2f5844ab0f24"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2820' max='2820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2820/2820 25:03, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>4.416200</td>\n",
              "      <td>4.113675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>4.221900</td>\n",
              "      <td>3.916138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>4.022600</td>\n",
              "      <td>3.819242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.896900</td>\n",
              "      <td>3.763644</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.829600</td>\n",
              "      <td>3.718250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.820800</td>\n",
              "      <td>3.691356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>3.709300</td>\n",
              "      <td>3.678784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>3.677600</td>\n",
              "      <td>3.669274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>3.680800</td>\n",
              "      <td>3.668258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.696500</td>\n",
              "      <td>3.668129</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./drive/MyDrive/finetuned_model_10e/tokenizer_config.json',\n",
              " './drive/MyDrive/finetuned_model_10e/special_tokens_map.json',\n",
              " './drive/MyDrive/finetuned_model_10e/spiece.model',\n",
              " './drive/MyDrive/finetuned_model_10e/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from datasets import load_metric\n",
        "\n",
        "# Step 1: Load data from JSON file\n",
        "def load_data(json_file, limit=None):\n",
        "    with open(json_file, 'r', encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "    if limit:\n",
        "        data = data[:limit]\n",
        "    return data\n",
        "\n",
        "# Step 2: Preprocess texts (filter out None values and combine fields)\n",
        "def preprocess_texts(data):\n",
        "    filtered_data = [item for item in data if item['exp'] is not None and item['question'] is not None]\n",
        "    texts = [f\"{item['question']} {item['exp']}\" for item in filtered_data]\n",
        "    return filtered_data, texts\n",
        "\n",
        "# Step 3: Compute TF-IDF for texts and user query\n",
        "def compute_tfidf(texts, query):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(texts + [query])\n",
        "    return tfidf_matrix\n",
        "\n",
        "# Step 4: Compute Cosine Similarity\n",
        "def find_similar_texts(tfidf_matrix):\n",
        "    query_vector = tfidf_matrix[-1]\n",
        "    similarities = cosine_similarity(query_vector, tfidf_matrix[:-1])\n",
        "    return similarities.flatten()\n",
        "\n",
        "# Step 5: Display related texts\n",
        "def get_related_texts(filtered_data, similarities, top_n=3):\n",
        "    sorted_indices = np.argsort(similarities)[-top_n:][::-1]\n",
        "    related_texts = [filtered_data[idx]['exp'] for idx in sorted_indices]\n",
        "    return related_texts\n",
        "\n",
        "# Step 6: Combine question and related texts to create input for the model\n",
        "def create_input(query, related_texts):\n",
        "    input_text = f\"پرسش: {query} زمینه: {' '.join(related_texts)}\"\n",
        "    return input_text\n",
        "\n",
        "# Step 7: Generate answer using the model\n",
        "def generate_answer(model, tokenizer, input_text, max_length=150, num_beams=5, early_stopping=True, temperature=1.0, top_k=None, top_p=None, repetition_penalty=1.0, no_repeat_ngram_size=2, length_penalty=2.0, do_sample=True):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        num_beams=num_beams,\n",
        "        early_stopping=early_stopping,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "        length_penalty=length_penalty,\n",
        "        do_sample=do_sample\n",
        "    )\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return answer\n",
        "\n",
        "# Load evaluation data (limited to 50 items)\n",
        "eval_file = 'evaluation_data.json'\n",
        "eval_data = load_data(eval_file, limit=50)\n",
        "\n",
        "# Initialize BLEU, ROUGE, and METEOR metrics\n",
        "bleu = load_metric(\"sacrebleu\")\n",
        "rouge = load_metric(\"rouge\")\n",
        "meteor = load_metric(\"meteor\")\n",
        "\n",
        "# Generate answers with the base model\n",
        "base_model_path = \"t5-small\"\n",
        "base_model = T5ForConditionalGeneration.from_pretrained(base_model_path)\n",
        "base_tokenizer = T5Tokenizer.from_pretrained(base_model_path)\n",
        "\n",
        "# Generate answers with the fine-tuned model\n",
        "finetuned_model_path = \"./finetuned_model_10e/\"\n",
        "finetuned_model = T5ForConditionalGeneration.from_pretrained(finetuned_model_path, local_files_only=True)\n",
        "finetuned_tokenizer = T5Tokenizer.from_pretrained(finetuned_model_path, local_files_only=True)\n",
        "\n",
        "# Function to evaluate model\n",
        "def evaluate_model(model, tokenizer, eval_data):\n",
        "    references = []\n",
        "    predictions = []\n",
        "    for idx, item in enumerate(eval_data):\n",
        "        query = item['question']\n",
        "        reference = item['exp']\n",
        "        _, texts = preprocess_texts([item])\n",
        "        if not texts:\n",
        "            print(f\"Skipping item {idx} with empty texts: {item}\")\n",
        "            continue\n",
        "        tfidf_matrix = compute_tfidf(texts, query)\n",
        "        if tfidf_matrix.shape[0] == 0:\n",
        "            print(f\"Empty TF-IDF matrix for query {idx}: {query}\")\n",
        "            continue\n",
        "        similarities = find_similar_texts(tfidf_matrix)\n",
        "        if len(similarities) == 0:\n",
        "            print(f\"No similarities found for query {idx}: {query}\")\n",
        "            continue\n",
        "        related_texts = get_related_texts([item], similarities)\n",
        "        input_text = create_input(query, related_texts)\n",
        "        prediction = generate_answer(model, tokenizer, input_text)\n",
        "        references.append(reference)\n",
        "        predictions.append(prediction)\n",
        "        print(f\"Processed item {idx}\")\n",
        "    return references, predictions\n",
        "\n",
        "# Evaluate base model\n",
        "print(\"Evaluating base model...\")\n",
        "base_references, base_predictions = evaluate_model(base_model, base_tokenizer, eval_data)\n",
        "\n",
        "# Evaluate fine-tuned model\n",
        "print(\"Evaluating fine-tuned model...\")\n",
        "finetuned_references, finetuned_predictions = evaluate_model(finetuned_model, finetuned_tokenizer, eval_data)\n",
        "\n",
        "# Calculate BLEU, ROUGE, and METEOR scores\n",
        "base_bleu_score = bleu.compute(predictions=base_predictions, references=[[ref] for ref in base_references])\n",
        "base_rouge_score = rouge.compute(predictions=base_predictions, references=base_references)\n",
        "base_meteor_score = meteor.compute(predictions=base_predictions, references=base_references)\n",
        "\n",
        "finetuned_bleu_score = bleu.compute(predictions=finetuned_predictions, references=[[ref] for ref in finetuned_references])\n",
        "finetuned_rouge_score = rouge.compute(predictions=finetuned_predictions, references=finetuned_references)\n",
        "finetuned_meteor_score = meteor.compute(predictions=finetuned_predictions, references=finetuned_references)\n",
        "\n",
        "print(\"Base Model BLEU Score:\", base_bleu_score)\n",
        "print(\"Base Model ROUGE Score:\", base_rouge_score)\n",
        "print(\"Base Model METEOR Score:\", base_meteor_score)\n",
        "\n",
        "print(\"Fine-tuned Model BLEU Score:\", finetuned_bleu_score)\n",
        "print(\"Fine-tuned Model ROUGE Score:\", finetuned_rouge_score)\n",
        "print(\"Fine-tuned Model METEOR Score:\", finetuned_meteor_score)\n",
        "\n",
        "# Calculate precision, recall, and accuracy\n",
        "def calculate_precision_recall_accuracy(predictions, references):\n",
        "    precision = np.mean([1 if pred in ref else 0 for pred, ref in zip(predictions, references)])\n",
        "    recall = np.mean([1 if ref in pred else 0 for pred, ref in zip(predictions, references)])\n",
        "    accuracy = np.mean([1 if pred == ref else 0 for pred, ref in zip(predictions, references)])\n",
        "    return precision, recall, accuracy\n",
        "\n",
        "# Base model precision, recall, accuracy\n",
        "base_precision, base_recall, base_accuracy = calculate_precision_recall_accuracy(base_predictions, base_references)\n",
        "print(\"Base Model Precision:\", base_precision)\n",
        "print(\"Base Model Recall:\", base_recall)\n",
        "print(\"Base Model Accuracy:\", base_accuracy)\n",
        "\n",
        "# Fine-tuned model precision, recall, accuracy\n",
        "finetuned_precision, finetuned_recall, finetuned_accuracy = calculate_precision_recall_accuracy(finetuned_predictions, finetuned_references)\n",
        "print(\"Fine-tuned Model Precision:\", finetuned_precision)\n",
        "print(\"Fine-tuned Model Recall:\", finetuned_recall)\n",
        "print(\"Fine-tuned Model Accuracy:\", finetuned_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTyKigc9n5hv",
        "outputId": "1474d612-8ad2-40bb-d524-568027ee9b51"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating base model...\n",
            "Skipping item 0 with empty texts: {'question': 'Retinoscopy in 5 year old is best done with:', 'exp': None}\n",
            "Processed item 1\n",
            "Processed item 2\n",
            "Processed item 3\n",
            "Processed item 4\n",
            "Processed item 5\n",
            "Skipping item 6 with empty texts: {'question': 'True about streptococcus:', 'exp': None}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (590 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed item 7\n",
            "Processed item 8\n",
            "Processed item 9\n",
            "Processed item 10\n",
            "Processed item 11\n",
            "Skipping item 12 with empty texts: {'question': 'Patient diagnosed to have malaria, smear shows all stages of schizonts 14-20 merozoites, yellowish - brown pigment. The type of malaria is -', 'exp': None}\n",
            "Processed item 13\n",
            "Processed item 14\n",
            "Processed item 15\n",
            "Skipping item 16 with empty texts: {'question': 'Crude birth rate denominator is -', 'exp': None}\n",
            "Processed item 17\n",
            "Processed item 18\n",
            "Processed item 19\n",
            "Skipping item 20 with empty texts: {'question': 'Intrauterine exposure of diethylstilboestrol is associated with -', 'exp': None}\n",
            "Processed item 21\n",
            "Processed item 22\n",
            "Processed item 23\n",
            "Processed item 24\n",
            "Processed item 25\n",
            "Processed item 26\n",
            "Skipping item 27 with empty texts: {'question': 'Which of the following Antiepileptic acts by opening Potassium channel?', 'exp': None}\n",
            "Processed item 28\n",
            "Processed item 29\n",
            "Processed item 30\n",
            "Processed item 31\n",
            "Processed item 32\n",
            "Processed item 33\n",
            "Processed item 34\n",
            "Processed item 35\n",
            "Processed item 36\n",
            "Processed item 37\n",
            "Processed item 38\n",
            "Processed item 39\n",
            "Processed item 40\n",
            "Processed item 41\n",
            "Processed item 42\n",
            "Skipping item 43 with empty texts: {'question': 'Negative BMR is observed with:', 'exp': None}\n",
            "Processed item 44\n",
            "Skipping item 45 with empty texts: {'question': 'Asepsis means -', 'exp': None}\n",
            "Processed item 46\n",
            "Processed item 47\n",
            "Processed item 48\n",
            "Processed item 49\n",
            "Evaluating fine-tuned model...\n",
            "Skipping item 0 with empty texts: {'question': 'Retinoscopy in 5 year old is best done with:', 'exp': None}\n",
            "Processed item 1\n",
            "Processed item 2\n",
            "Processed item 3\n",
            "Processed item 4\n",
            "Processed item 5\n",
            "Skipping item 6 with empty texts: {'question': 'True about streptococcus:', 'exp': None}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (590 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed item 7\n",
            "Processed item 8\n",
            "Processed item 9\n",
            "Processed item 10\n",
            "Processed item 11\n",
            "Skipping item 12 with empty texts: {'question': 'Patient diagnosed to have malaria, smear shows all stages of schizonts 14-20 merozoites, yellowish - brown pigment. The type of malaria is -', 'exp': None}\n",
            "Processed item 13\n",
            "Processed item 14\n",
            "Processed item 15\n",
            "Skipping item 16 with empty texts: {'question': 'Crude birth rate denominator is -', 'exp': None}\n",
            "Processed item 17\n",
            "Processed item 18\n",
            "Processed item 19\n",
            "Skipping item 20 with empty texts: {'question': 'Intrauterine exposure of diethylstilboestrol is associated with -', 'exp': None}\n",
            "Processed item 21\n",
            "Processed item 22\n",
            "Processed item 23\n",
            "Processed item 24\n",
            "Processed item 25\n",
            "Processed item 26\n",
            "Skipping item 27 with empty texts: {'question': 'Which of the following Antiepileptic acts by opening Potassium channel?', 'exp': None}\n",
            "Processed item 28\n",
            "Processed item 29\n",
            "Processed item 30\n",
            "Processed item 31\n",
            "Processed item 32\n",
            "Processed item 33\n",
            "Processed item 34\n",
            "Processed item 35\n",
            "Processed item 36\n",
            "Processed item 37\n",
            "Processed item 38\n",
            "Processed item 39\n",
            "Processed item 40\n",
            "Processed item 41\n",
            "Processed item 42\n",
            "Skipping item 43 with empty texts: {'question': 'Negative BMR is observed with:', 'exp': None}\n",
            "Processed item 44\n",
            "Skipping item 45 with empty texts: {'question': 'Asepsis means -', 'exp': None}\n",
            "Processed item 46\n",
            "Processed item 47\n",
            "Processed item 48\n",
            "Processed item 49\n",
            "Base Model BLEU Score: {'score': 19.67316506107492, 'counts': [1536, 1307, 1159, 1031], 'totals': [1953, 1911, 1869, 1827], 'precisions': [78.64823348694317, 68.39351125065411, 62.01177100053505, 56.43130815544609], 'bp': 0.2986755722168241, 'sys_len': 1953, 'ref_len': 4313}\n",
            "Base Model ROUGE Score: {'rouge1': AggregateScore(low=Score(precision=0.6884174621302949, recall=0.4832937496976874, fmeasure=0.5209093334907381), mid=Score(precision=0.769314563502743, recall=0.5724275409887942, fmeasure=0.589935546565017), high=Score(precision=0.8439576756009964, recall=0.6562067528379457, fmeasure=0.6585053315776505)), 'rouge2': AggregateScore(low=Score(precision=0.5915591955145468, recall=0.39436790881661515, fmeasure=0.4407618122255393), mid=Score(precision=0.6816757282336232, recall=0.48625148158853426, fmeasure=0.5174721929347824), high=Score(precision=0.7540153790493888, recall=0.5843010890285363, fmeasure=0.5938660967477165)), 'rougeL': AggregateScore(low=Score(precision=0.620493035479412, recall=0.43528867274920063, fmeasure=0.4634637724666325), mid=Score(precision=0.7071284650828196, recall=0.5303110596158009, fmeasure=0.5439374402333056), high=Score(precision=0.7792302298616313, recall=0.6214213243956772, fmeasure=0.6170168147573473)), 'rougeLsum': AggregateScore(low=Score(precision=0.6223593421568832, recall=0.44149880192313923, fmeasure=0.4692798688238784), mid=Score(precision=0.7099378815399695, recall=0.5303079687467912, fmeasure=0.5454808577994215), high=Score(precision=0.7864400211126464, recall=0.6232294083880339, fmeasure=0.6191522107554879))}\n",
            "Base Model METEOR Score: {'meteor': 0.5207160868888759}\n",
            "Fine-tuned Model BLEU Score: {'score': 11.975295219640746, 'counts': [1279, 838, 615, 471], 'totals': [1978, 1936, 1894, 1852], 'precisions': [64.66127401415571, 43.28512396694215, 32.47096092925026, 25.43196544276458], 'bp': 0.30712964051408054, 'sys_len': 1978, 'ref_len': 4313}\n",
            "Fine-tuned Model ROUGE Score: {'rouge1': AggregateScore(low=Score(precision=0.5957655865918976, recall=0.3719458162205621, fmeasure=0.4133342541575794), mid=Score(precision=0.6699583359523383, recall=0.4407274347492083, fmeasure=0.4724769134850507), high=Score(precision=0.7391497669958453, recall=0.5097645635517533, fmeasure=0.536391165750354)), 'rouge2': AggregateScore(low=Score(precision=0.41191412896360713, recall=0.2562935503881229, fmeasure=0.28554666024107245), mid=Score(precision=0.4874925021717774, recall=0.3209755399627916, fmeasure=0.3443790656766085), high=Score(precision=0.5643405928644495, recall=0.3909711318291924, fmeasure=0.41052723074548975)), 'rougeL': AggregateScore(low=Score(precision=0.47811952873871094, recall=0.30442507471208213, fmeasure=0.3347308801616648), mid=Score(precision=0.5561954923532035, recall=0.3704668650973877, fmeasure=0.39494878301578357), high=Score(precision=0.6348700007670304, recall=0.4454927804853364, fmeasure=0.46091946061257894)), 'rougeLsum': AggregateScore(low=Score(precision=0.4807350867216468, recall=0.3070700887440897, fmeasure=0.34009337468216694), mid=Score(precision=0.5626350789563602, recall=0.37356610084762004, fmeasure=0.39824157086170797), high=Score(precision=0.640717611199751, recall=0.44209222072426235, fmeasure=0.4600116939068654))}\n",
            "Fine-tuned Model METEOR Score: {'meteor': 0.3934702146382086}\n",
            "Base Model Precision: 0.0\n",
            "Base Model Recall: 0.047619047619047616\n",
            "Base Model Accuracy: 0.0\n",
            "Fine-tuned Model Precision: 0.0\n",
            "Fine-tuned Model Recall: 0.0\n",
            "Fine-tuned Model Accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from datasets import load_metric\n",
        "\n",
        "# Step 1: Load data from JSON file\n",
        "def load_data(json_file, limit=None):\n",
        "    with open(json_file, 'r', encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "    if limit:\n",
        "        data = data[:limit]\n",
        "    return data\n",
        "\n",
        "# Step 2: Preprocess texts (filter out None values and combine fields)\n",
        "def preprocess_texts(data):\n",
        "    filtered_data = [item for item in data if item['exp'] is not None and item['question'] is not None]\n",
        "    texts = [f\"{item['question']} {item['exp']}\" for item in filtered_data]\n",
        "    return filtered_data, texts\n",
        "\n",
        "# Step 3: Compute TF-IDF for texts and user query\n",
        "def compute_tfidf(texts, query):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(texts + [query])\n",
        "    return tfidf_matrix\n",
        "\n",
        "# Step 4: Compute Cosine Similarity\n",
        "def find_similar_texts(tfidf_matrix):\n",
        "    query_vector = tfidf_matrix[-1]\n",
        "    similarities = cosine_similarity(query_vector, tfidf_matrix[:-1])\n",
        "    return similarities.flatten()\n",
        "\n",
        "# Step 5: Display related texts\n",
        "def get_related_texts(filtered_data, similarities, top_n=3):\n",
        "    sorted_indices = np.argsort(similarities)[-top_n:][::-1]\n",
        "    related_texts = [filtered_data[idx]['exp'] for idx in sorted_indices]\n",
        "    return related_texts\n",
        "\n",
        "# Step 6: Combine question and related texts to create input for the model\n",
        "def create_input(query, related_texts):\n",
        "    input_text = f\"پرسش: {query} زمینه: {' '.join(related_texts)}\"\n",
        "    return input_text\n",
        "\n",
        "# Step 7: Generate answer using the model\n",
        "def generate_answer(model, tokenizer, input_text, max_length=150, num_beams=5, early_stopping=True, temperature=1.0, top_k=None, top_p=None, repetition_penalty=1.0, no_repeat_ngram_size=2, length_penalty=2.0, do_sample=True):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        num_beams=num_beams,\n",
        "        early_stopping=early_stopping,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "        length_penalty=length_penalty,\n",
        "        do_sample=do_sample\n",
        "    )\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return answer\n",
        "\n",
        "# Load evaluation data (limited to 50 items)\n",
        "eval_file = 'evaluation_data_train.json'\n",
        "eval_data = load_data(eval_file, limit=50)\n",
        "\n",
        "# Initialize BLEU, ROUGE, and METEOR metrics\n",
        "bleu = load_metric(\"sacrebleu\")\n",
        "rouge = load_metric(\"rouge\")\n",
        "meteor = load_metric(\"meteor\")\n",
        "\n",
        "# Generate answers with the base model\n",
        "base_model_path = \"t5-small\"\n",
        "base_model = T5ForConditionalGeneration.from_pretrained(base_model_path)\n",
        "base_tokenizer = T5Tokenizer.from_pretrained(base_model_path)\n",
        "\n",
        "# Generate answers with the fine-tuned model\n",
        "finetuned_model_path = \"./finetuned_model_10e/\"\n",
        "finetuned_model = T5ForConditionalGeneration.from_pretrained(finetuned_model_path, local_files_only=True)\n",
        "finetuned_tokenizer = T5Tokenizer.from_pretrained(finetuned_model_path, local_files_only=True)\n",
        "\n",
        "# Function to evaluate model\n",
        "def evaluate_model(model, tokenizer, eval_data):\n",
        "    references = []\n",
        "    predictions = []\n",
        "    for idx, item in enumerate(eval_data):\n",
        "        query = item['question']\n",
        "        reference = item['exp']\n",
        "        _, texts = preprocess_texts([item])\n",
        "        if not texts:\n",
        "            print(f\"Skipping item {idx} with empty texts: {item}\")\n",
        "            continue\n",
        "        tfidf_matrix = compute_tfidf(texts, query)\n",
        "        if tfidf_matrix.shape[0] == 0:\n",
        "            print(f\"Empty TF-IDF matrix for query {idx}: {query}\")\n",
        "            continue\n",
        "        similarities = find_similar_texts(tfidf_matrix)\n",
        "        if len(similarities) == 0:\n",
        "            print(f\"No similarities found for query {idx}: {query}\")\n",
        "            continue\n",
        "        related_texts = get_related_texts([item], similarities)\n",
        "        input_text = create_input(query, related_texts)\n",
        "        prediction = generate_answer(model, tokenizer, input_text)\n",
        "        references.append(reference)\n",
        "        predictions.append(prediction)\n",
        "        print(f\"Processed item {idx}\")\n",
        "    return references, predictions\n",
        "\n",
        "# Evaluate base model\n",
        "print(\"Evaluating base model...\")\n",
        "base_references, base_predictions = evaluate_model(base_model, base_tokenizer, eval_data)\n",
        "\n",
        "# Evaluate fine-tuned model\n",
        "print(\"Evaluating fine-tuned model...\")\n",
        "finetuned_references, finetuned_predictions = evaluate_model(finetuned_model, finetuned_tokenizer, eval_data)\n",
        "\n",
        "# Calculate BLEU, ROUGE, and METEOR scores\n",
        "base_bleu_score = bleu.compute(predictions=base_predictions, references=[[ref] for ref in base_references])\n",
        "base_rouge_score = rouge.compute(predictions=base_predictions, references=base_references)\n",
        "base_meteor_score = meteor.compute(predictions=base_predictions, references=base_references)\n",
        "\n",
        "finetuned_bleu_score = bleu.compute(predictions=finetuned_predictions, references=[[ref] for ref in finetuned_references])\n",
        "finetuned_rouge_score = rouge.compute(predictions=finetuned_predictions, references=finetuned_references)\n",
        "finetuned_meteor_score = meteor.compute(predictions=finetuned_predictions, references=finetuned_references)\n",
        "\n",
        "print(\"Base Model BLEU Score:\", base_bleu_score)\n",
        "print(\"Base Model ROUGE Score:\", base_rouge_score)\n",
        "print(\"Base Model METEOR Score:\", base_meteor_score)\n",
        "\n",
        "print(\"Fine-tuned Model BLEU Score:\", finetuned_bleu_score)\n",
        "print(\"Fine-tuned Model ROUGE Score:\", finetuned_rouge_score)\n",
        "print(\"Fine-tuned Model METEOR Score:\", finetuned_meteor_score)\n",
        "\n",
        "# Calculate precision, recall, and accuracy\n",
        "def calculate_precision_recall_accuracy(predictions, references):\n",
        "    precision = np.mean([1 if pred in ref else 0 for pred, ref in zip(predictions, references)])\n",
        "    recall = np.mean([1 if ref in pred else 0 for pred, ref in zip(predictions, references)])\n",
        "    accuracy = np.mean([1 if pred == ref else 0 for pred, ref in zip(predictions, references)])\n",
        "    return precision, recall, accuracy\n",
        "\n",
        "# Base model precision, recall, accuracy\n",
        "base_precision, base_recall, base_accuracy = calculate_precision_recall_accuracy(base_predictions, base_references)\n",
        "print(\"Base Model Precision:\", base_precision)\n",
        "print(\"Base Model Recall:\", base_recall)\n",
        "print(\"Base Model Accuracy:\", base_accuracy)\n",
        "\n",
        "# Fine-tuned model precision, recall, accuracy\n",
        "finetuned_precision, finetuned_recall, finetuned_accuracy = calculate_precision_recall_accuracy(finetuned_predictions, finetuned_references)\n",
        "print(\"Fine-tuned Model Precision:\", finetuned_precision)\n",
        "print(\"Fine-tuned Model Recall:\", finetuned_recall)\n",
        "print(\"Fine-tuned Model Accuracy:\", finetuned_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9G5Cq_tn5fL",
        "outputId": "23e2c533-11f4-4162-9722-e9c40e19a3a4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating base model...\n",
            "Processed item 0\n",
            "Processed item 1\n",
            "Processed item 2\n",
            "Processed item 3\n",
            "Processed item 4\n",
            "Processed item 5\n",
            "Processed item 6\n",
            "Processed item 7\n",
            "Processed item 8\n",
            "Processed item 9\n",
            "Processed item 10\n",
            "Skipping item 11 with empty texts: {'question': 'Relining of complete denture is not indicated when', 'exp': None}\n",
            "Processed item 12\n",
            "Processed item 13\n",
            "Processed item 14\n",
            "Processed item 15\n",
            "Processed item 16\n",
            "Processed item 17\n",
            "Processed item 18\n",
            "Processed item 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (540 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed item 20\n",
            "Skipping item 21 with empty texts: {'question': 'Secondary retention for a removable partial denture is provided by', 'exp': None}\n",
            "Processed item 22\n",
            "Processed item 23\n",
            "Skipping item 24 with empty texts: {'question': 'Egg shell calcification is seen in all except –', 'exp': None}\n",
            "Processed item 25\n",
            "Processed item 26\n",
            "Processed item 27\n",
            "Processed item 28\n",
            "Processed item 29\n",
            "Processed item 30\n",
            "Skipping item 31 with empty texts: {'question': 'A labourer involved with repair-work of sewers was admitted with fever, jaundice and renal failure. The most appropriate test to diagnose the infection in this patient is -', 'exp': None}\n",
            "Processed item 32\n",
            "Processed item 33\n",
            "Processed item 34\n",
            "Processed item 35\n",
            "Skipping item 36 with empty texts: {'question': 'A 40-year old diabetic patient presents with proptosis of one eye and black eschar over palate. The likely organism is :', 'exp': None}\n",
            "Processed item 37\n",
            "Skipping item 38 with empty texts: {'question': 'Investigations in a clinically suspected case of tuberculosis -', 'exp': None}\n",
            "Processed item 39\n",
            "Processed item 40\n",
            "Processed item 41\n",
            "Processed item 42\n",
            "Processed item 43\n",
            "Processed item 44\n",
            "Skipping item 45 with empty texts: {'question': 'Dimension stability of hydrocollids may be achieved by', 'exp': None}\n",
            "Processed item 46\n",
            "Processed item 47\n",
            "Processed item 48\n",
            "Processed item 49\n",
            "Evaluating fine-tuned model...\n",
            "Processed item 0\n",
            "Processed item 1\n",
            "Processed item 2\n",
            "Processed item 3\n",
            "Processed item 4\n",
            "Processed item 5\n",
            "Processed item 6\n",
            "Processed item 7\n",
            "Processed item 8\n",
            "Processed item 9\n",
            "Processed item 10\n",
            "Skipping item 11 with empty texts: {'question': 'Relining of complete denture is not indicated when', 'exp': None}\n",
            "Processed item 12\n",
            "Processed item 13\n",
            "Processed item 14\n",
            "Processed item 15\n",
            "Processed item 16\n",
            "Processed item 17\n",
            "Processed item 18\n",
            "Processed item 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (540 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed item 20\n",
            "Skipping item 21 with empty texts: {'question': 'Secondary retention for a removable partial denture is provided by', 'exp': None}\n",
            "Processed item 22\n",
            "Processed item 23\n",
            "Skipping item 24 with empty texts: {'question': 'Egg shell calcification is seen in all except –', 'exp': None}\n",
            "Processed item 25\n",
            "Processed item 26\n",
            "Processed item 27\n",
            "Processed item 28\n",
            "Processed item 29\n",
            "Processed item 30\n",
            "Skipping item 31 with empty texts: {'question': 'A labourer involved with repair-work of sewers was admitted with fever, jaundice and renal failure. The most appropriate test to diagnose the infection in this patient is -', 'exp': None}\n",
            "Processed item 32\n",
            "Processed item 33\n",
            "Processed item 34\n",
            "Processed item 35\n",
            "Skipping item 36 with empty texts: {'question': 'A 40-year old diabetic patient presents with proptosis of one eye and black eschar over palate. The likely organism is :', 'exp': None}\n",
            "Processed item 37\n",
            "Skipping item 38 with empty texts: {'question': 'Investigations in a clinically suspected case of tuberculosis -', 'exp': None}\n",
            "Processed item 39\n",
            "Processed item 40\n",
            "Processed item 41\n",
            "Processed item 42\n",
            "Processed item 43\n",
            "Processed item 44\n",
            "Skipping item 45 with empty texts: {'question': 'Dimension stability of hydrocollids may be achieved by', 'exp': None}\n",
            "Processed item 46\n",
            "Processed item 47\n",
            "Processed item 48\n",
            "Processed item 49\n",
            "Base Model BLEU Score: {'score': 18.041645510658373, 'counts': [1460, 1192, 1018, 886], 'totals': [1943, 1900, 1857, 1814], 'precisions': [75.14153371075656, 62.73684210526316, 54.819601507808294, 48.84233737596472], 'bp': 0.30268603862651944, 'sys_len': 1943, 'ref_len': 4265}\n",
            "Base Model ROUGE Score: {'rouge1': AggregateScore(low=Score(precision=0.6102999263659855, recall=0.5063287630003319, fmeasure=0.49489080765335175), mid=Score(precision=0.6912967862731919, recall=0.5945316761607486, fmeasure=0.5542675290356572), high=Score(precision=0.7614070507017688, recall=0.6832376525335415, fmeasure=0.6141800440296528)), 'rouge2': AggregateScore(low=Score(precision=0.4971760460625569, recall=0.4183579827802821, fmeasure=0.3924015782333363), mid=Score(precision=0.5745192655229928, recall=0.5056636846106106, fmeasure=0.4584871931904092), high=Score(precision=0.6515257391575227, recall=0.5964664404490306, fmeasure=0.5208358895875381)), 'rougeL': AggregateScore(low=Score(precision=0.5380136617140723, recall=0.4594575869807003, fmeasure=0.4395542080113542), mid=Score(precision=0.6122658032542512, recall=0.5530895885119351, fmeasure=0.5025082062704302), high=Score(precision=0.6835661892146714, recall=0.6462150946071206, fmeasure=0.5627179320764099)), 'rougeLsum': AggregateScore(low=Score(precision=0.5405115496357102, recall=0.4577105259271623, fmeasure=0.4401465349429725), mid=Score(precision=0.6170447419258924, recall=0.5541268716783361, fmeasure=0.5066137859497601), high=Score(precision=0.6835490333893552, recall=0.6544808549282171, fmeasure=0.5678799824320208))}\n",
            "Base Model METEOR Score: {'meteor': 0.510447641068035}\n",
            "Fine-tuned Model BLEU Score: {'score': 8.298648726592784, 'counts': [1009, 588, 412, 307], 'totals': [1916, 1873, 1830, 1787], 'precisions': [52.66179540709812, 31.393486385477843, 22.51366120218579, 17.179630665920538], 'bp': 0.2934665401029222, 'sys_len': 1916, 'ref_len': 4265}\n",
            "Fine-tuned Model ROUGE Score: {'rouge1': AggregateScore(low=Score(precision=0.43996075120488504, recall=0.32846689379224964, fmeasure=0.29420978734942066), mid=Score(precision=0.5233909255823004, recall=0.4223006433220414, fmeasure=0.3665336840159292), high=Score(precision=0.5902729135977359, recall=0.5186310385956356, fmeasure=0.4354830920910693)), 'rouge2': AggregateScore(low=Score(precision=0.258247678447376, recall=0.1941609990024406, fmeasure=0.1826559394127934), mid=Score(precision=0.33597935168307513, recall=0.2698653684331583, fmeasure=0.25119282411827615), high=Score(precision=0.4206727502817329, recall=0.3579054929047994, fmeasure=0.32901094708547957)), 'rougeL': AggregateScore(low=Score(precision=0.32570715906624337, recall=0.25575043197512204, fmeasure=0.22213340477054958), mid=Score(precision=0.3918529138900261, recall=0.3483824518456593, fmeasure=0.28451274205148347), high=Score(precision=0.46133212173410043, recall=0.4465378912842208, fmeasure=0.35437229578338)), 'rougeLsum': AggregateScore(low=Score(precision=0.3269735210128833, recall=0.25941709837343374, fmeasure=0.22340703190722822), mid=Score(precision=0.3921125188235652, recall=0.3483474170736708, fmeasure=0.2855663743207426), high=Score(precision=0.46274225122902957, recall=0.44958848452765166, fmeasure=0.35927697861794344))}\n",
            "Fine-tuned Model METEOR Score: {'meteor': 0.2995093170695101}\n",
            "Base Model Precision: 0.023255813953488372\n",
            "Base Model Recall: 0.16279069767441862\n",
            "Base Model Accuracy: 0.0\n",
            "Fine-tuned Model Precision: 0.046511627906976744\n",
            "Fine-tuned Model Recall: 0.06976744186046512\n",
            "Fine-tuned Model Accuracy: 0.046511627906976744\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Load and preprocess fine-tuning data\n",
        "def load_finetune_data(json_file, limit=5000):  # Increase data limit\n",
        "    with open(json_file, 'r', encoding='utf-8') as file:\n",
        "        data = [json.loads(line) for line in file][:limit]\n",
        "    return data\n",
        "\n",
        "def preprocess_finetune_data(data):\n",
        "    inputs = [f\"پرسش: {item['question']}\" for item in data]\n",
        "    targets = [item['exp'] for item in data]\n",
        "    return inputs, targets\n",
        "\n",
        "# Step 2: Tokenize inputs and targets\n",
        "def tokenize_data(tokenizer, inputs, targets, max_length=512):\n",
        "    encodings = tokenizer(list(map(str, inputs)), truncation=True, padding=True, max_length=max_length)\n",
        "    labels = tokenizer(list(map(str, targets)), truncation=True, padding=True, max_length=max_length).input_ids\n",
        "\n",
        "    # Replace padding token id's in the labels by -100 to ignore padding in the loss\n",
        "    labels = [[(label if label != tokenizer.pad_token_id else -100) for label in labels_example] for labels_example in labels]\n",
        "\n",
        "    encodings['labels'] = labels\n",
        "    return encodings\n",
        "\n",
        "# Step 3: Fine-tune the T5 model\n",
        "def finetune_model(model, tokenizer, train_dataset, eval_dataset, output_dir=\"./results\"):\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=10,  # Increase the number of epochs\n",
        "        per_device_train_batch_size=8,  # Keep batch size manageable\n",
        "        per_device_eval_batch_size=8,\n",
        "        warmup_steps=200,  # Increase warmup steps\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=10,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",  # Track validation loss for best model\n",
        "        greater_is_better=False,\n",
        "        fp16=True,  # Enable mixed precision training\n",
        "        learning_rate=5e-5,  # Experiment with lower learning rates\n",
        "        lr_scheduler_type=\"cosine\",  # Use a cosine scheduler\n",
        "        save_total_limit=3,  # Save only the last 3 checkpoints\n",
        "        seed=42  # Ensure reproducibility\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "# Execute steps for fine-tuning\n",
        "json_file = 'train.json'\n",
        "finetune_data = load_finetune_data(json_file, limit=5000)  # Increased data limit\n",
        "inputs, targets = preprocess_finetune_data(finetune_data)\n",
        "\n",
        "# Split data into training and evaluation sets\n",
        "train_inputs, eval_inputs, train_targets, eval_targets = train_test_split(inputs, targets, test_size=0.1, random_state=42)\n",
        "\n",
        "# Load T5 model and tokenizer\n",
        "model_name = \"t5-small\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize inputs and targets\n",
        "train_encodings = tokenize_data(tokenizer, train_inputs, train_targets)\n",
        "eval_encodings = tokenize_data(tokenizer, eval_inputs, eval_targets)\n",
        "\n",
        "# Create datasets using the datasets library\n",
        "train_dataset = Dataset.from_dict(train_encodings)\n",
        "eval_dataset = Dataset.from_dict(eval_encodings)\n",
        "\n",
        "# Fine-tune the model\n",
        "finetune_model(model, tokenizer, train_dataset, eval_dataset)\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"./finetuned_model_11e\")\n",
        "tokenizer.save_pretrained(\"./finetuned_model_11e\")\n",
        "model.save_pretrained(\"./drive/MyDrive/finetuned_model_11e\")\n",
        "tokenizer.save_pretrained(\"./drive/MyDrive/finetuned_model_11e\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "i_l4c37Un5ck",
        "outputId": "4a4bc498-b4e3-4740-8b90-5a990cbdbde0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5630' max='5630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5630/5630 27:01, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>4.393800</td>\n",
              "      <td>4.140195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>4.302400</td>\n",
              "      <td>4.024040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>4.160300</td>\n",
              "      <td>3.965619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>4.292700</td>\n",
              "      <td>3.933632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>4.034300</td>\n",
              "      <td>3.904641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>4.253400</td>\n",
              "      <td>3.886363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>4.084500</td>\n",
              "      <td>3.879431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>3.979000</td>\n",
              "      <td>3.873139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>4.072300</td>\n",
              "      <td>3.871222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>4.150400</td>\n",
              "      <td>3.870848</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./drive/MyDrive/finetuned_model_11e/tokenizer_config.json',\n",
              " './drive/MyDrive/finetuned_model_11e/special_tokens_map.json',\n",
              " './drive/MyDrive/finetuned_model_11e/spiece.model',\n",
              " './drive/MyDrive/finetuned_model_11e/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from datasets import load_metric\n",
        "\n",
        "# Step 1: Load data from JSON file\n",
        "def load_data(json_file, limit=None):\n",
        "    with open(json_file, 'r', encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "    if limit:\n",
        "        data = data[:limit]\n",
        "    return data\n",
        "\n",
        "# Step 2: Preprocess texts (filter out None values and combine fields)\n",
        "def preprocess_texts(data):\n",
        "    filtered_data = [item for item in data if item['exp'] is not None and item['question'] is not None]\n",
        "    texts = [f\"{item['question']} {item['exp']}\" for item in filtered_data]\n",
        "    return filtered_data, texts\n",
        "\n",
        "# Step 3: Compute TF-IDF for texts and user query\n",
        "def compute_tfidf(texts, query):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(texts + [query])\n",
        "    return tfidf_matrix\n",
        "\n",
        "# Step 4: Compute Cosine Similarity\n",
        "def find_similar_texts(tfidf_matrix):\n",
        "    query_vector = tfidf_matrix[-1]\n",
        "    similarities = cosine_similarity(query_vector, tfidf_matrix[:-1])\n",
        "    return similarities.flatten()\n",
        "\n",
        "# Step 5: Display related texts\n",
        "def get_related_texts(filtered_data, similarities, top_n=3):\n",
        "    sorted_indices = np.argsort(similarities)[-top_n:][::-1]\n",
        "    related_texts = [filtered_data[idx]['exp'] for idx in sorted_indices]\n",
        "    return related_texts\n",
        "\n",
        "# Step 6: Combine question and related texts to create input for the model\n",
        "def create_input(query, related_texts):\n",
        "    input_text = f\"پرسش: {query} زمینه: {' '.join(related_texts)}\"\n",
        "    return input_text\n",
        "\n",
        "# Step 7: Generate answer using the model\n",
        "def generate_answer(model, tokenizer, input_text, max_length=150, num_beams=5, early_stopping=True, temperature=1.0, top_k=None, top_p=None, repetition_penalty=1.0, no_repeat_ngram_size=2, length_penalty=2.0, do_sample=True):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        num_beams=num_beams,\n",
        "        early_stopping=early_stopping,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "        length_penalty=length_penalty,\n",
        "        do_sample=do_sample\n",
        "    )\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return answer\n",
        "\n",
        "# Load evaluation data (limited to 50 items)\n",
        "eval_file = 'evaluation_data_train.json'\n",
        "eval_data = load_data(eval_file, limit=50)\n",
        "\n",
        "# Initialize BLEU, ROUGE, and METEOR metrics\n",
        "bleu = load_metric(\"sacrebleu\")\n",
        "rouge = load_metric(\"rouge\")\n",
        "meteor = load_metric(\"meteor\")\n",
        "\n",
        "# Generate answers with the base model\n",
        "base_model_path = \"t5-small\"\n",
        "base_model = T5ForConditionalGeneration.from_pretrained(base_model_path)\n",
        "base_tokenizer = T5Tokenizer.from_pretrained(base_model_path)\n",
        "\n",
        "# Generate answers with the fine-tuned model\n",
        "finetuned_model_path = \"./finetuned_model_11e/\"\n",
        "finetuned_model = T5ForConditionalGeneration.from_pretrained(finetuned_model_path, local_files_only=True)\n",
        "finetuned_tokenizer = T5Tokenizer.from_pretrained(finetuned_model_path, local_files_only=True)\n",
        "\n",
        "# Function to evaluate model\n",
        "def evaluate_model(model, tokenizer, eval_data):\n",
        "    references = []\n",
        "    predictions = []\n",
        "    for idx, item in enumerate(eval_data):\n",
        "        query = item['question']\n",
        "        reference = item['exp']\n",
        "        _, texts = preprocess_texts([item])\n",
        "        if not texts:\n",
        "            print(f\"Skipping item {idx} with empty texts: {item}\")\n",
        "            continue\n",
        "        tfidf_matrix = compute_tfidf(texts, query)\n",
        "        if tfidf_matrix.shape[0] == 0:\n",
        "            print(f\"Empty TF-IDF matrix for query {idx}: {query}\")\n",
        "            continue\n",
        "        similarities = find_similar_texts(tfidf_matrix)\n",
        "        if len(similarities) == 0:\n",
        "            print(f\"No similarities found for query {idx}: {query}\")\n",
        "            continue\n",
        "        related_texts = get_related_texts([item], similarities)\n",
        "        input_text = create_input(query, related_texts)\n",
        "        prediction = generate_answer(model, tokenizer, input_text)\n",
        "        references.append(reference)\n",
        "        predictions.append(prediction)\n",
        "        print(f\"Processed item {idx}\")\n",
        "    return references, predictions\n",
        "\n",
        "# Evaluate base model\n",
        "print(\"Evaluating base model...\")\n",
        "base_references, base_predictions = evaluate_model(base_model, base_tokenizer, eval_data)\n",
        "\n",
        "# Evaluate fine-tuned model\n",
        "print(\"Evaluating fine-tuned model...\")\n",
        "finetuned_references, finetuned_predictions = evaluate_model(finetuned_model, finetuned_tokenizer, eval_data)\n",
        "\n",
        "# Calculate BLEU, ROUGE, and METEOR scores\n",
        "base_bleu_score = bleu.compute(predictions=base_predictions, references=[[ref] for ref in base_references])\n",
        "base_rouge_score = rouge.compute(predictions=base_predictions, references=base_references)\n",
        "base_meteor_score = meteor.compute(predictions=base_predictions, references=base_references)\n",
        "\n",
        "finetuned_bleu_score = bleu.compute(predictions=finetuned_predictions, references=[[ref] for ref in finetuned_references])\n",
        "finetuned_rouge_score = rouge.compute(predictions=finetuned_predictions, references=finetuned_references)\n",
        "finetuned_meteor_score = meteor.compute(predictions=finetuned_predictions, references=finetuned_references)\n",
        "\n",
        "print(\"Base Model BLEU Score:\", base_bleu_score)\n",
        "print(\"Base Model ROUGE Score:\", base_rouge_score)\n",
        "print(\"Base Model METEOR Score:\", base_meteor_score)\n",
        "\n",
        "print(\"Fine-tuned Model BLEU Score:\", finetuned_bleu_score)\n",
        "print(\"Fine-tuned Model ROUGE Score:\", finetuned_rouge_score)\n",
        "print(\"Fine-tuned Model METEOR Score:\", finetuned_meteor_score)\n",
        "\n",
        "# Calculate precision, recall, and accuracy\n",
        "def calculate_precision_recall_accuracy(predictions, references):\n",
        "    precision = np.mean([1 if pred in ref else 0 for pred, ref in zip(predictions, references)])\n",
        "    recall = np.mean([1 if ref in pred else 0 for pred, ref in zip(predictions, references)])\n",
        "    accuracy = np.mean([1 if pred == ref else 0 for pred, ref in zip(predictions, references)])\n",
        "    return precision, recall, accuracy\n",
        "\n",
        "# Base model precision, recall, accuracy\n",
        "base_precision, base_recall, base_accuracy = calculate_precision_recall_accuracy(base_predictions, base_references)\n",
        "print(\"Base Model Precision:\", base_precision)\n",
        "print(\"Base Model Recall:\", base_recall)\n",
        "print(\"Base Model Accuracy:\", base_accuracy)\n",
        "\n",
        "# Fine-tuned model precision, recall, accuracy\n",
        "finetuned_precision, finetuned_recall, finetuned_accuracy = calculate_precision_recall_accuracy(finetuned_predictions, finetuned_references)\n",
        "print(\"Fine-tuned Model Precision:\", finetuned_precision)\n",
        "print(\"Fine-tuned Model Recall:\", finetuned_recall)\n",
        "print(\"Fine-tuned Model Accuracy:\", finetuned_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRhoGLf4n5Z3",
        "outputId": "2feb5241-5805-4860-c2b0-17adc6c5fdc2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating base model...\n",
            "Processed item 0\n",
            "Processed item 1\n",
            "Processed item 2\n",
            "Processed item 3\n",
            "Processed item 4\n",
            "Processed item 5\n",
            "Processed item 6\n",
            "Processed item 7\n",
            "Processed item 8\n",
            "Processed item 9\n",
            "Processed item 10\n",
            "Skipping item 11 with empty texts: {'question': 'Relining of complete denture is not indicated when', 'exp': None}\n",
            "Processed item 12\n",
            "Processed item 13\n",
            "Processed item 14\n",
            "Processed item 15\n",
            "Processed item 16\n",
            "Processed item 17\n",
            "Processed item 18\n",
            "Processed item 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (540 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed item 20\n",
            "Skipping item 21 with empty texts: {'question': 'Secondary retention for a removable partial denture is provided by', 'exp': None}\n",
            "Processed item 22\n",
            "Processed item 23\n",
            "Skipping item 24 with empty texts: {'question': 'Egg shell calcification is seen in all except –', 'exp': None}\n",
            "Processed item 25\n",
            "Processed item 26\n",
            "Processed item 27\n",
            "Processed item 28\n",
            "Processed item 29\n",
            "Processed item 30\n",
            "Skipping item 31 with empty texts: {'question': 'A labourer involved with repair-work of sewers was admitted with fever, jaundice and renal failure. The most appropriate test to diagnose the infection in this patient is -', 'exp': None}\n",
            "Processed item 32\n",
            "Processed item 33\n",
            "Processed item 34\n",
            "Processed item 35\n",
            "Skipping item 36 with empty texts: {'question': 'A 40-year old diabetic patient presents with proptosis of one eye and black eschar over palate. The likely organism is :', 'exp': None}\n",
            "Processed item 37\n",
            "Skipping item 38 with empty texts: {'question': 'Investigations in a clinically suspected case of tuberculosis -', 'exp': None}\n",
            "Processed item 39\n",
            "Processed item 40\n",
            "Processed item 41\n",
            "Processed item 42\n",
            "Processed item 43\n",
            "Processed item 44\n",
            "Skipping item 45 with empty texts: {'question': 'Dimension stability of hydrocollids may be achieved by', 'exp': None}\n",
            "Processed item 46\n",
            "Processed item 47\n",
            "Processed item 48\n",
            "Processed item 49\n",
            "Evaluating fine-tuned model...\n",
            "Processed item 0\n",
            "Processed item 1\n",
            "Processed item 2\n",
            "Processed item 3\n",
            "Processed item 4\n",
            "Processed item 5\n",
            "Processed item 6\n",
            "Processed item 7\n",
            "Processed item 8\n",
            "Processed item 9\n",
            "Processed item 10\n",
            "Skipping item 11 with empty texts: {'question': 'Relining of complete denture is not indicated when', 'exp': None}\n",
            "Processed item 12\n",
            "Processed item 13\n",
            "Processed item 14\n",
            "Processed item 15\n",
            "Processed item 16\n",
            "Processed item 17\n",
            "Processed item 18\n",
            "Processed item 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (540 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed item 20\n",
            "Skipping item 21 with empty texts: {'question': 'Secondary retention for a removable partial denture is provided by', 'exp': None}\n",
            "Processed item 22\n",
            "Processed item 23\n",
            "Skipping item 24 with empty texts: {'question': 'Egg shell calcification is seen in all except –', 'exp': None}\n",
            "Processed item 25\n",
            "Processed item 26\n",
            "Processed item 27\n",
            "Processed item 28\n",
            "Processed item 29\n",
            "Processed item 30\n",
            "Skipping item 31 with empty texts: {'question': 'A labourer involved with repair-work of sewers was admitted with fever, jaundice and renal failure. The most appropriate test to diagnose the infection in this patient is -', 'exp': None}\n",
            "Processed item 32\n",
            "Processed item 33\n",
            "Processed item 34\n",
            "Processed item 35\n",
            "Skipping item 36 with empty texts: {'question': 'A 40-year old diabetic patient presents with proptosis of one eye and black eschar over palate. The likely organism is :', 'exp': None}\n",
            "Processed item 37\n",
            "Skipping item 38 with empty texts: {'question': 'Investigations in a clinically suspected case of tuberculosis -', 'exp': None}\n",
            "Processed item 39\n",
            "Processed item 40\n",
            "Processed item 41\n",
            "Processed item 42\n",
            "Processed item 43\n",
            "Processed item 44\n",
            "Skipping item 45 with empty texts: {'question': 'Dimension stability of hydrocollids may be achieved by', 'exp': None}\n",
            "Processed item 46\n",
            "Processed item 47\n",
            "Processed item 48\n",
            "Processed item 49\n",
            "Base Model BLEU Score: {'score': 18.007909925233037, 'counts': [1454, 1154, 974, 844], 'totals': [1996, 1953, 1910, 1867], 'precisions': [72.84569138276554, 59.08858166922683, 50.99476439790576, 45.20621317621853], 'bp': 0.3208525692780805, 'sys_len': 1996, 'ref_len': 4265}\n",
            "Base Model ROUGE Score: {'rouge1': AggregateScore(low=Score(precision=0.5909026843816844, recall=0.5088833903235308, fmeasure=0.4818410873371489), mid=Score(precision=0.6665892417414951, recall=0.5945729981163465, fmeasure=0.5434069663318433), high=Score(precision=0.7370108330666694, recall=0.6856525270906534, fmeasure=0.6021478886653499)), 'rouge2': AggregateScore(low=Score(precision=0.4718604194468043, recall=0.4102491114424663, fmeasure=0.37853954759255465), mid=Score(precision=0.5432465414882659, recall=0.49635151502070574, fmeasure=0.44329119360113045), high=Score(precision=0.6181246928613301, recall=0.5851354958865159, fmeasure=0.5040514900014308)), 'rougeL': AggregateScore(low=Score(precision=0.5144905911227449, recall=0.44984655527609474, fmeasure=0.42447012274551416), mid=Score(precision=0.5822548163425609, recall=0.5460728178717188, fmeasure=0.48604038117024084), high=Score(precision=0.6495703798736019, recall=0.6422508188034506, fmeasure=0.5462617246759512)), 'rougeLsum': AggregateScore(low=Score(precision=0.5147503896196187, recall=0.4559554988401856, fmeasure=0.42221029990447284), mid=Score(precision=0.5877241814336843, recall=0.5478597953689166, fmeasure=0.4902427732997575), high=Score(precision=0.6546438468252715, recall=0.642047118624853, fmeasure=0.5503527125891127))}\n",
            "Base Model METEOR Score: {'meteor': 0.5224096395104232}\n",
            "Fine-tuned Model BLEU Score: {'score': 13.289453796642325, 'counts': [1220, 707, 480, 358], 'totals': [2738, 2695, 2652, 2609], 'precisions': [44.55807158509861, 26.233766233766232, 18.099547511312217, 13.721732464545804], 'bp': 0.5725207183198072, 'sys_len': 2738, 'ref_len': 4265}\n",
            "Fine-tuned Model ROUGE Score: {'rouge1': AggregateScore(low=Score(precision=0.35990642002699, recall=0.44034935367307093, fmeasure=0.30950314124485423), mid=Score(precision=0.44364163645064714, recall=0.5209272620901519, fmeasure=0.3586956353277011), high=Score(precision=0.5212067978840176, recall=0.6057025632412608, fmeasure=0.404424779741163)), 'rouge2': AggregateScore(low=Score(precision=0.2265498324420146, recall=0.2559925600010141, fmeasure=0.19466251203184873), mid=Score(precision=0.2914265224408312, recall=0.34220027258024366, fmeasure=0.23722200178089686), high=Score(precision=0.3564725096163247, recall=0.4369330216825809, fmeasure=0.2843918689517089)), 'rougeL': AggregateScore(low=Score(precision=0.26740425672398577, recall=0.3446418836174192, fmeasure=0.23784468112280677), mid=Score(precision=0.3265759690207256, recall=0.4343683375332543, fmeasure=0.2739983961776649), high=Score(precision=0.3957789733098239, recall=0.5336228954681174, fmeasure=0.31976981225212014)), 'rougeLsum': AggregateScore(low=Score(precision=0.2673008179600707, recall=0.34803700639543234, fmeasure=0.23508751728098284), mid=Score(precision=0.32902689196405444, recall=0.44105593445796365, fmeasure=0.2787381566583659), high=Score(precision=0.3969791036803389, recall=0.5369832061837438, fmeasure=0.3197906987212989))}\n",
            "Fine-tuned Model METEOR Score: {'meteor': 0.3465162769077006}\n",
            "Base Model Precision: 0.0\n",
            "Base Model Recall: 0.18604651162790697\n",
            "Base Model Accuracy: 0.0\n",
            "Fine-tuned Model Precision: 0.0\n",
            "Fine-tuned Model Recall: 0.09302325581395349\n",
            "Fine-tuned Model Accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from datasets import load_metric\n",
        "\n",
        "# Step 1: Load data from JSON file\n",
        "def load_data(json_file, limit=None):\n",
        "    with open(json_file, 'r', encoding='utf-8') as file:\n",
        "        data = json.load(file)\n",
        "    if limit:\n",
        "        data = data[:limit]\n",
        "    return data\n",
        "\n",
        "# Step 2: Preprocess texts (filter out None values and combine fields)\n",
        "def preprocess_texts(data):\n",
        "    filtered_data = [item for item in data if item['exp'] is not None and item['question'] is not None]\n",
        "    texts = [f\"{item['question']} {item['exp']}\" for item in filtered_data]\n",
        "    return filtered_data, texts\n",
        "\n",
        "# Step 3: Compute TF-IDF for texts and user query\n",
        "def compute_tfidf(texts, query):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(texts + [query])\n",
        "    return tfidf_matrix\n",
        "\n",
        "# Step 4: Compute Cosine Similarity\n",
        "def find_similar_texts(tfidf_matrix):\n",
        "    query_vector = tfidf_matrix[-1]\n",
        "    similarities = cosine_similarity(query_vector, tfidf_matrix[:-1])\n",
        "    return similarities.flatten()\n",
        "\n",
        "# Step 5: Display related texts\n",
        "def get_related_texts(filtered_data, similarities, top_n=3):\n",
        "    sorted_indices = np.argsort(similarities)[-top_n:][::-1]\n",
        "    related_texts = [filtered_data[idx]['exp'] for idx in sorted_indices]\n",
        "    return related_texts\n",
        "\n",
        "# Step 6: Combine question and related texts to create input for the model\n",
        "def create_input(query, related_texts):\n",
        "    input_text = f\"پرسش: {query} زمینه: {' '.join(related_texts)}\"\n",
        "    return input_text\n",
        "\n",
        "# Step 7: Generate answer using the model\n",
        "def generate_answer(model, tokenizer, input_text, max_length=150, num_beams=5, early_stopping=True, temperature=1.0, top_k=None, top_p=None, repetition_penalty=1.0, no_repeat_ngram_size=2, length_penalty=2.0, do_sample=True):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "    outputs = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        num_beams=num_beams,\n",
        "        early_stopping=early_stopping,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "        length_penalty=length_penalty,\n",
        "        do_sample=do_sample\n",
        "    )\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return answer\n",
        "\n",
        "# Load evaluation data (limited to 50 items)\n",
        "eval_file = 'evaluation_data.json'\n",
        "eval_data = load_data(eval_file, limit=50)\n",
        "\n",
        "# Initialize BLEU, ROUGE, and METEOR metrics\n",
        "bleu = load_metric(\"sacrebleu\")\n",
        "rouge = load_metric(\"rouge\")\n",
        "meteor = load_metric(\"meteor\")\n",
        "\n",
        "# Generate answers with the base model\n",
        "base_model_path = \"t5-small\"\n",
        "base_model = T5ForConditionalGeneration.from_pretrained(base_model_path)\n",
        "base_tokenizer = T5Tokenizer.from_pretrained(base_model_path)\n",
        "\n",
        "# Generate answers with the fine-tuned model\n",
        "finetuned_model_path = \"./finetuned_model_11e/\"\n",
        "finetuned_model = T5ForConditionalGeneration.from_pretrained(finetuned_model_path, local_files_only=True)\n",
        "finetuned_tokenizer = T5Tokenizer.from_pretrained(finetuned_model_path, local_files_only=True)\n",
        "\n",
        "# Function to evaluate model\n",
        "def evaluate_model(model, tokenizer, eval_data):\n",
        "    references = []\n",
        "    predictions = []\n",
        "    for idx, item in enumerate(eval_data):\n",
        "        query = item['question']\n",
        "        reference = item['exp']\n",
        "        _, texts = preprocess_texts([item])\n",
        "        if not texts:\n",
        "            print(f\"Skipping item {idx} with empty texts: {item}\")\n",
        "            continue\n",
        "        tfidf_matrix = compute_tfidf(texts, query)\n",
        "        if tfidf_matrix.shape[0] == 0:\n",
        "            print(f\"Empty TF-IDF matrix for query {idx}: {query}\")\n",
        "            continue\n",
        "        similarities = find_similar_texts(tfidf_matrix)\n",
        "        if len(similarities) == 0:\n",
        "            print(f\"No similarities found for query {idx}: {query}\")\n",
        "            continue\n",
        "        related_texts = get_related_texts([item], similarities)\n",
        "        input_text = create_input(query, related_texts)\n",
        "        prediction = generate_answer(model, tokenizer, input_text)\n",
        "        references.append(reference)\n",
        "        predictions.append(prediction)\n",
        "        print(f\"Processed item {idx}\")\n",
        "    return references, predictions\n",
        "\n",
        "# Evaluate base model\n",
        "print(\"Evaluating base model...\")\n",
        "base_references, base_predictions = evaluate_model(base_model, base_tokenizer, eval_data)\n",
        "\n",
        "# Evaluate fine-tuned model\n",
        "print(\"Evaluating fine-tuned model...\")\n",
        "finetuned_references, finetuned_predictions = evaluate_model(finetuned_model, finetuned_tokenizer, eval_data)\n",
        "\n",
        "# Calculate BLEU, ROUGE, and METEOR scores\n",
        "base_bleu_score = bleu.compute(predictions=base_predictions, references=[[ref] for ref in base_references])\n",
        "base_rouge_score = rouge.compute(predictions=base_predictions, references=base_references)\n",
        "base_meteor_score = meteor.compute(predictions=base_predictions, references=base_references)\n",
        "\n",
        "finetuned_bleu_score = bleu.compute(predictions=finetuned_predictions, references=[[ref] for ref in finetuned_references])\n",
        "finetuned_rouge_score = rouge.compute(predictions=finetuned_predictions, references=finetuned_references)\n",
        "finetuned_meteor_score = meteor.compute(predictions=finetuned_predictions, references=finetuned_references)\n",
        "\n",
        "print(\"Base Model BLEU Score:\", base_bleu_score)\n",
        "print(\"Base Model ROUGE Score:\", base_rouge_score)\n",
        "print(\"Base Model METEOR Score:\", base_meteor_score)\n",
        "\n",
        "print(\"Fine-tuned Model BLEU Score:\", finetuned_bleu_score)\n",
        "print(\"Fine-tuned Model ROUGE Score:\", finetuned_rouge_score)\n",
        "print(\"Fine-tuned Model METEOR Score:\", finetuned_meteor_score)\n",
        "\n",
        "# Calculate precision, recall, and accuracy\n",
        "def calculate_precision_recall_accuracy(predictions, references):\n",
        "    precision = np.mean([1 if pred in ref else 0 for pred, ref in zip(predictions, references)])\n",
        "    recall = np.mean([1 if ref in pred else 0 for pred, ref in zip(predictions, references)])\n",
        "    accuracy = np.mean([1 if pred == ref else 0 for pred, ref in zip(predictions, references)])\n",
        "    return precision, recall, accuracy\n",
        "\n",
        "# Base model precision, recall, accuracy\n",
        "base_precision, base_recall, base_accuracy = calculate_precision_recall_accuracy(base_predictions, base_references)\n",
        "print(\"Base Model Precision:\", base_precision)\n",
        "print(\"Base Model Recall:\", base_recall)\n",
        "print(\"Base Model Accuracy:\", base_accuracy)\n",
        "\n",
        "# Fine-tuned model precision, recall, accuracy\n",
        "finetuned_precision, finetuned_recall, finetuned_accuracy = calculate_precision_recall_accuracy(finetuned_predictions, finetuned_references)\n",
        "print(\"Fine-tuned Model Precision:\", finetuned_precision)\n",
        "print(\"Fine-tuned Model Recall:\", finetuned_recall)\n",
        "print(\"Fine-tuned Model Accuracy:\", finetuned_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uypaRtjWn5XH",
        "outputId": "622427c7-953e-4e4e-81ff-9c9dfc471db8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating base model...\n",
            "Skipping item 0 with empty texts: {'question': 'Retinoscopy in 5 year old is best done with:', 'exp': None}\n",
            "Processed item 1\n",
            "Processed item 2\n",
            "Processed item 3\n",
            "Processed item 4\n",
            "Processed item 5\n",
            "Skipping item 6 with empty texts: {'question': 'True about streptococcus:', 'exp': None}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (590 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed item 7\n",
            "Processed item 8\n",
            "Processed item 9\n",
            "Processed item 10\n",
            "Processed item 11\n",
            "Skipping item 12 with empty texts: {'question': 'Patient diagnosed to have malaria, smear shows all stages of schizonts 14-20 merozoites, yellowish - brown pigment. The type of malaria is -', 'exp': None}\n",
            "Processed item 13\n",
            "Processed item 14\n",
            "Processed item 15\n",
            "Skipping item 16 with empty texts: {'question': 'Crude birth rate denominator is -', 'exp': None}\n",
            "Processed item 17\n",
            "Processed item 18\n",
            "Processed item 19\n",
            "Skipping item 20 with empty texts: {'question': 'Intrauterine exposure of diethylstilboestrol is associated with -', 'exp': None}\n",
            "Processed item 21\n",
            "Processed item 22\n",
            "Processed item 23\n",
            "Processed item 24\n",
            "Processed item 25\n",
            "Processed item 26\n",
            "Skipping item 27 with empty texts: {'question': 'Which of the following Antiepileptic acts by opening Potassium channel?', 'exp': None}\n",
            "Processed item 28\n",
            "Processed item 29\n",
            "Processed item 30\n",
            "Processed item 31\n",
            "Processed item 32\n",
            "Processed item 33\n",
            "Processed item 34\n",
            "Processed item 35\n",
            "Processed item 36\n",
            "Processed item 37\n",
            "Processed item 38\n",
            "Processed item 39\n",
            "Processed item 40\n",
            "Processed item 41\n",
            "Processed item 42\n",
            "Skipping item 43 with empty texts: {'question': 'Negative BMR is observed with:', 'exp': None}\n",
            "Processed item 44\n",
            "Skipping item 45 with empty texts: {'question': 'Asepsis means -', 'exp': None}\n",
            "Processed item 46\n",
            "Processed item 47\n",
            "Processed item 48\n",
            "Processed item 49\n",
            "Evaluating fine-tuned model...\n",
            "Skipping item 0 with empty texts: {'question': 'Retinoscopy in 5 year old is best done with:', 'exp': None}\n",
            "Processed item 1\n",
            "Processed item 2\n",
            "Processed item 3\n",
            "Processed item 4\n",
            "Processed item 5\n",
            "Skipping item 6 with empty texts: {'question': 'True about streptococcus:', 'exp': None}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (590 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed item 7\n",
            "Processed item 8\n",
            "Processed item 9\n",
            "Processed item 10\n",
            "Processed item 11\n",
            "Skipping item 12 with empty texts: {'question': 'Patient diagnosed to have malaria, smear shows all stages of schizonts 14-20 merozoites, yellowish - brown pigment. The type of malaria is -', 'exp': None}\n",
            "Processed item 13\n",
            "Processed item 14\n",
            "Processed item 15\n",
            "Skipping item 16 with empty texts: {'question': 'Crude birth rate denominator is -', 'exp': None}\n",
            "Processed item 17\n",
            "Processed item 18\n",
            "Processed item 19\n",
            "Skipping item 20 with empty texts: {'question': 'Intrauterine exposure of diethylstilboestrol is associated with -', 'exp': None}\n",
            "Processed item 21\n",
            "Processed item 22\n",
            "Processed item 23\n",
            "Processed item 24\n",
            "Processed item 25\n",
            "Processed item 26\n",
            "Skipping item 27 with empty texts: {'question': 'Which of the following Antiepileptic acts by opening Potassium channel?', 'exp': None}\n",
            "Processed item 28\n",
            "Processed item 29\n",
            "Processed item 30\n",
            "Processed item 31\n",
            "Processed item 32\n",
            "Processed item 33\n",
            "Processed item 34\n",
            "Processed item 35\n",
            "Processed item 36\n",
            "Processed item 37\n",
            "Processed item 38\n",
            "Processed item 39\n",
            "Processed item 40\n",
            "Processed item 41\n",
            "Processed item 42\n",
            "Skipping item 43 with empty texts: {'question': 'Negative BMR is observed with:', 'exp': None}\n",
            "Processed item 44\n",
            "Skipping item 45 with empty texts: {'question': 'Asepsis means -', 'exp': None}\n",
            "Processed item 46\n",
            "Processed item 47\n",
            "Processed item 48\n",
            "Processed item 49\n",
            "Base Model BLEU Score: {'score': 19.357007784786475, 'counts': [1522, 1281, 1124, 990], 'totals': [1968, 1926, 1884, 1842], 'precisions': [77.33739837398375, 66.51090342679127, 59.660297239915074, 53.74592833876221], 'bp': 0.30374551779995085, 'sys_len': 1968, 'ref_len': 4313}\n",
            "Base Model ROUGE Score: {'rouge1': AggregateScore(low=Score(precision=0.6756700964477128, recall=0.47747067801836346, fmeasure=0.5142322309412799), mid=Score(precision=0.7513585681809407, recall=0.563482475376737, fmeasure=0.5833940970210545), high=Score(precision=0.8206851091331037, recall=0.6481202395931288, fmeasure=0.6484060685905899)), 'rouge2': AggregateScore(low=Score(precision=0.564358173517652, recall=0.37796341532314104, fmeasure=0.4217229162372757), mid=Score(precision=0.6426907378993738, recall=0.46835600292363777, fmeasure=0.49748703711206693), high=Score(precision=0.7160430611920146, recall=0.5641287868549784, fmeasure=0.5759600127215991)), 'rougeL': AggregateScore(low=Score(precision=0.612595751670179, recall=0.4300663296887836, fmeasure=0.46261525091063777), mid=Score(precision=0.6857673019425964, recall=0.5235606895662842, fmeasure=0.5395608766162366), high=Score(precision=0.7548568232244768, recall=0.6132521257047299, fmeasure=0.6122197394705392)), 'rougeLsum': AggregateScore(low=Score(precision=0.6106229640581887, recall=0.4400629326450763, fmeasure=0.46871459422198486), mid=Score(precision=0.6892478730834113, recall=0.5236697285424504, fmeasure=0.5404213103343896), high=Score(precision=0.7562063694711155, recall=0.606129841699444, fmeasure=0.6079078928930293))}\n",
            "Base Model METEOR Score: {'meteor': 0.5026893198195952}\n",
            "Fine-tuned Model BLEU Score: {'score': 15.236607393118662, 'counts': [1360, 889, 657, 510], 'totals': [2370, 2328, 2286, 2244], 'precisions': [57.38396624472574, 38.187285223367695, 28.74015748031496, 22.727272727272727], 'bp': 0.4405059952357588, 'sys_len': 2370, 'ref_len': 4313}\n",
            "Fine-tuned Model ROUGE Score: {'rouge1': AggregateScore(low=Score(precision=0.5061043969619826, recall=0.39706514116105773, fmeasure=0.3798458731288567), mid=Score(precision=0.5899004456560585, recall=0.4714688307213181, fmeasure=0.4387542881451228), high=Score(precision=0.6753762111396324, recall=0.546294433894917, fmeasure=0.4978213173711962)), 'rouge2': AggregateScore(low=Score(precision=0.34716330706617066, recall=0.24875211816937287, fmeasure=0.25426277908021666), mid=Score(precision=0.4324388468310202, recall=0.31380811471810616, fmeasure=0.31313641508809065), high=Score(precision=0.5189095502051017, recall=0.3926921331037391, fmeasure=0.37049689943604486)), 'rougeL': AggregateScore(low=Score(precision=0.37981637205576047, recall=0.3052454606978742, fmeasure=0.2845229004788009), mid=Score(precision=0.4727105229817361, recall=0.372544768035183, fmeasure=0.3420174001685146), high=Score(precision=0.5618033616997093, recall=0.4499358973639025, fmeasure=0.40361163029055613)), 'rougeLsum': AggregateScore(low=Score(precision=0.3965093253337179, recall=0.3108690445346892, fmeasure=0.2943466843901862), mid=Score(precision=0.4767931412821358, recall=0.37921880034166094, fmeasure=0.3491414225664365), high=Score(precision=0.567409983430322, recall=0.45292393505222994, fmeasure=0.40519920576473745))}\n",
            "Fine-tuned Model METEOR Score: {'meteor': 0.37485081060888453}\n",
            "Base Model Precision: 0.023809523809523808\n",
            "Base Model Recall: 0.023809523809523808\n",
            "Base Model Accuracy: 0.0\n",
            "Fine-tuned Model Precision: 0.023809523809523808\n",
            "Fine-tuned Model Recall: 0.023809523809523808\n",
            "Fine-tuned Model Accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fPyimZEbn5OR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}